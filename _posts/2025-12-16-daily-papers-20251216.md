---
layout: post
title: "Daily Papers — 2025-12-16"
date: 2025-12-16 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.13604)

## Introduction
- Goal: 본 연구는 선행된 비디오 확산 모델을 확장하여 조작 가능하고 장시간에 걸쳐 시공간적 일관성을 유지하는 비디오 월드 모델을 구축하는 것이다.  
- Motivation: 기존 비디오 월드 모델은 환경에 대한 전역적이고 의미론적 조작 능력이 제한적이며, 장시간 생성 시 시각 품질 저하와 시간적 불연속성 문제를 겪고 있기 때문이다.  
- Contribution: 제안된 LongVie 2는 다중 모달 제어 신호 통합, 열화 인지 학습, 그리고 이력 맥락 가이던스의 3단계 점진적 학습 방식으로 조작성, 시각 품질 및 시간적 일관성을 강화하는 자기회귀적 장기 비디오 생성 프레임워크를 제안한다.  

## Method  
LongVie 2는 깊이 맵과 3D 포인트맵을 활용한 다중 모달 제어를 통해 세계 수준의 가이던스를 제공하며, 첫 프레임에 인위적 열화를 적용하여 장시간 생성 시 시각 저하를 완화한다. 아울러 이전 클립의 프레임을 맥락으로 활용하여 클립 간 시간적 연속성을 유지하도록 학습한다. 학습 시에는 고정된 확산 모델 백본 위에 제어 모듈을 추가하고, 점진적 단계별 학습으로 각 요소를 순차적으로 최적화한다.  

## Results  
LongVGenBench 벤치마크(100편 이상의 1분 이상 고해상도 비디오 집합)에서 LongVie 2는 조작성, 시간적 일관성 및 시각 품질 부문에서 최첨단 성능을 보이며 최대 3~5분 길이의 영상을 안정적으로 생성하였다.  

## Limitations  
모델은 장시간 생성 시 여전히 클립 간 경계 프레임에서 일부 불연속성이 발생할 수 있으며, 완벽한 무결점 시간적 일관성을 위해서는 추가 연구가 필요하다.  

## Conclusion  
LongVie 2는 선행 비디오 확산 모델을 기반으로 조작 가능하고 시공간적 일관성을 갖춘 장기 비디오 월드 모델 생성에 성공하였으며, 이는 통합형 비디오 월드 모델링을 향한 중요한 진전으로 평가된다.

# 2. [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.10655)

## Introduction
- Goal: 본 연구의 목표는 텍스트-이미지 확산 모델에서 훈련 데이터의 과도한 암기를 완화하는 새로운 추론 시간 기법을 제안하는 것이다.  
- Motivation: 확산 모델이 훈련 데이터의 이미지를 무단 복제하는 현상이 발견되어 저작권 및 개인정보 침해 우려를 해소할 필요가 있다.  
- Contribution: CAPTAIN이라는 훈련 없이 잠재 특징공간에서 의미론적 특징 주입을 통해 암기를 감소시키면서 입력 프롬프트와의 정렬성을 유지하는 방법을 제안하였다.  

## Method  
CAPTAIN은 저주파수 구성을 가진 참조 이미지와 고주파수 노이즈를 결합하는 주파수 분해 초기화로 생성 초기 단계의 암기 의존도를 낮춘다.  
모델 생성 과정에서 최적의 시점과 암기 영역을 밝히기 위해 CLIP 점수 변화와 밝은 종료(Bright Ending) 주의를 활용한 시공간적 암기 위치 선정 전략을 적용한다.  
그리고 추론 중 해당 지역에 참조 이미지로부터 뽑은 의미론적으로 정렬된 특징을 국소적으로 주입하여 암기를 억제함과 동시에 시각 품질과 프롬프트 일치를 유지한다.  

## Results  
Stable Diffusion v1.4와 2.0에서 CAPTAIN은 기존의 CFG 조작 및 프롬프트 변형 방식 대비 SSCD 점수를 38%까지 줄이면서 CLIP 점수를 28% 이상 개선하는 등 뛰어난 암기 완화와 의미론적 정합성 균형을 달성하였다.  

## Limitations  
본 방법은 외부 참조 이미지 검색 성능과 BE+개념별 주의 기반 암기 영역 추출의 신뢰성에 의존하며, 데이터셋 변경 시 새로운 유사도 색인 구축이 필요하다.  

## Conclusion  
CAPTAIN은 훈련 없이 잠재 공간에서 의미론적 특징을 주사입하여 안정적이고 효과적인 암기 완화와 프롬프트 정합성을 동시에 구현하는 추론 시간 기반 텍스트-이미지 확산 모델 암기 완화 프레임워크이다.

# 3. [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.13672)

## Introduction
- Goal: 본 연구는 텍스트-이미지 생성 개인화에서 텍스트토큰 임베딩의 크기 문제를 해결하여 텍스트 충실도를 향상시키는 방법을 제안하는 데 목적이 있다.  
- Motivation: 기존의 Textual Inversion(TI)은 복잡한 프롬프트에서 임베딩 크기 과대화로 인해 텍스트-이미지 정렬 성능이 저하되는 한계가 존재하였다.  
- Contribution: 임베딩의 방향만을 단위 구면에서 최적화하고 크기는 사전훈련 분포 내로 고정하는 Directional Textual Inversion(DTI) 기법을 제안하여 텍스트 충실도와 개인화 효과를 동시에 개선하였다.  

## Method  
DTI는 임베딩을 크기와 방향으로 분리하여 크기는 고정하고 방향만 Riemannian SGD를 통해 업데이트하며, 방향 최적화를 von Mises-Fisher 분포를 이용한 MAP 추정으로 정규화한다. 이로써 임베딩의 방향 정보가 의미를 주로 담고 크기 과대화를 억제함으로써 프롬프트에 대한 해석력을 유지한다. 또한, 방향 공간에서 부드럽고 의미론적으로 일관된 개념 간 보간이 가능해져 창의적 응용에 용이하다.  

## Results  
DTI는 Stable Diffusion XL 및 SANA 모델에서 기존 TI 및 변형 기법 대비 텍스트-이미지 정렬 성능과 주제 일관성 모두에서 우수한 결과를 보였으며, 사용자 평가에서도 선호도가 높았다.  

## Limitations  
DTI는 주제 충실도에 직접 최적화를 수행하지 않아 최고 수준의 주제 일치가 필요한 경우 경량 미세조정과 병행할 필요가 존재한다.  

## Conclusion  
본 연구는 임베딩 방향 최적화와 크기 제어를 통해 텍스트-이미지 생성 개인화의 텍스트 충실도를 향상시키는 효과적인 방법을 제시함으로써 실용성과 창의적 활용 가능성을 크게 확장하였다.

# 4. [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.13006)

## Introduction
- 본 논문은 텍스트-이미지 생성에서 확산모델 기반 증류 기법의 적응과 비교를 통해 효율적이고 고품질의 생성기를 구축하는 방법을 제시하는 데 목표가 있다.  
- 대규모 확산 모델이 높은 품질을 달성하나 반복적 샘플링 과정으로 인해 계산 비용과 지연 시간이 커서 실시간 및 자원 제한 환경에서의 활용이 어렵다는 문제가 동기이다.  
- 최신 증류 기법들을 통합 프레임워크에서 분석하고 텍스트-이미지 생성 특성에 맞춘 실용적 가이드라인과 공개 코드, 사전학습된 모델을 제공한 점이 주요 공헌이다.  

## Method  
- 대표적 증류 방법론인 sCM(연속시간 일관성 모델)과 MeanFlow(평균 흐름 추정)를 FLUX.1-lite 교사 모델에 적용하여 텍스트 조건 생성에 최적화하였다.  
- sCM은 시간 스케일 정규화와 교사 속도 예측 신호를 통한 안정적 훈련을 수행하였으며, MeanFlow는 이중 시간 입력과 고차 손실 함수를 도입하여 궤적 일관성을 강화하였다.  
- 또한, 두 기법 간 이론적 관계를 밝히고, FLUX.1-lite 모델 구조를 바탕으로 텍스트-이미지 생성에 특화된 맞춤형 실험 환경을 구축하였다.  

## Results  
- GenEval 및 DPG-Bench 벤치마크에서 sCM은 1~2단계 함수 평가 시에도 교사 모델과 유사한 품질을 유지하며 빠른 응용에 적합하고, MeanFlow는 4단계 평가 시 교사와 거의 동등한 고해상도 이미지를 생성하였다.  

## Limitations  
- MeanFlow는 매우 적은 단계에서 성능이 급격히 저하되어 초고속 생성에는 부적합한 반면, sCM은 높은 단계에서 세밀한 표현력에서 상대적으로 부족한 한계가 존재한다.  

## Conclusion  
- 본 연구는 텍스트-이미지 생성에서 증류 기법의 체계적 분석과 실용적 적용 방안을 제시하여 차세대 고속·고품질 확산 모델 개발에 기초를 마련하였다.

# 5. [Flowception: Temporally Expansive Flow Matching for Video Generation](https://arxiv.org/abs/2512.11438)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.11438)

## Introduction
- Goal: 본 연구는 비디오 생성에서 연속적인 프레임 디노이징과 이산적인 프레임 삽입을 결합하여 비가변 길이의 비자기회귀적 비디오 생성 프레임워크인 Flowception을 제안하는 것이다.  
- Motivation: 기존의 자기회귀적 방법은 오류 누적과 주의 메커니즘 제한으로 장기적 문맥 처리가 어렵고, 전체 시퀀스 방법은 고비용과 고정 길이 생성의 한계가 존재하였다.  
- Contribution: Flowception은 프레임 삽입과 디노이징을 교차하여 처리하며, 학습 효율성 향상과 다양한 영상 생성 작업에서 우수한 성능을 보이는 새로운 동영상 생성 방식을 제안하였다.  

## Method  
Flowception은 각 프레임에 대해 삽입률과 연속적인 속도장(velocity field)을 예측하여 새로운 프레임을 확률적으로 삽입하고 동시에 기존 프레임을 디노이징한다. 삽입과 디노이징 과정을 반복하며 변수 길이 시퀀스를 처리하여 전체 시퀀스를 동시에 다루는 풀 시퀀스 방식보다 FLOPs를 약 3배 절감한다. 또한 프레임 순서에 따른 다중 조건을 이용해 이미지-비디오 변환, 비디오 보간 등의 다양한 작업이 가능하다.  

## Results  
Flowception은 Tai-Chi-HD, RealEstate10K, Kinetics 600 데이터셋에서 기존 자기회귀 및 풀 시퀀스 영상 생성 모델 대비 FVD 및 VBench 지표에서 일관되게 우수한 성능을 기록하였다.  

## Limitations  
부분 시퀀스 학습 방식으로 인해 모든 프레임이 완전히 디노이징될 때까지 반복 횟수가 증가하여 학습 및 샘플링 효율성 개선 여지가 존재한다.  

## Conclusion  
Flowception은 삽입과 디노이징을 연계한 유연한 비디오 생성 프레임워크로서 기존 방법 대비 높은 효율성과 생성 품질을 달성하며, 장기 생성과 다양한 편집 작업에 효과적인 대안이 된다.

# 6. [Inferring Compositional 4D Scenes without Ever Seeing One](https://arxiv.org/abs/2512.05272)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.05272)

## Introduction
- Goal: 본 연구는 단일 모노큘러 비디오로부터 복합적인 정적 및 동적 객체가 포함된 4D 장면의 구조와 시공간적 구성을 일관성 있게 재구성하는 방법을 제안하는 것이다.  
- Motivation: 실제 환경에서 다수의 정적 및 동적 객체가 상호작용하는 4D 장면을 직접 관측하거나 대규모 4D 합성 데이터를 확보하기 어렵기 때문에, 기존 연구들이 하나의 객체만 또는 특정 범주에 한정하여 접근하는 한계가 존재한다.  
- Contribution: COM4D라 명명한 본 방법은 공간적 및 시간적 주의를 독립적 데이터에서 학습하여 이를 통합하는 Attention Parsing과 Attention Mixing 기법을 통해, 4D 구성 데이터를 전혀 보지 않고도 다중 객체의 지속적이고 일관된 4D 장면 재구성을 가능하게 하였다.  

## Method  
COM4D는 3D-FRONT 데이터셋의 정적 다중 객체 구성과 DeformingThings 데이터셋의 단일 객체 동적 시퀀스로부터 공간 및 시간적 주의를 각각 학습하는 Attention Parsing 훈련 전략을 사용한다. 학습된 공간 및 시간 주의는 추론 시 Attention Mixing 메커니즘을 통해 단일 트랜스포머 모델로 결합되어 다중 정적 및 동적 객체의 4D 장면을 재구성한다. 또한, Diffusion Forcing 기법을 도입해 시간적 일관성과 동적 객체의 연속적인 움직임 모델링을 강화하였다.  

## Results  
COM4D는 3D-FRONT, DeformingThings, Objaverse 등에서 기존 최첨단 방법들을 능가하는 3D 및 4D 재구성의 정밀도와 시간적 일관성을 달성하였으며, CMU Panoptic 데이터셋 기반 사용자 평가에서도 기존 모델 대비 현저히 선호되는 결과를 나타냈다.  

## Limitations  
본 방법은 고정된 카메라 영상에만 적용 가능하고, 객관의 가리는 상황에서 물리적 인과관계를 명시적으로 모델링하지 않아 occlusion 발생 시 객체의 물리적 연속성 추론에 제한이 있다.  

## Conclusion  
COM4D는 희소한 4D 구성 데이터를 요구하지 않으면서도 단일 모노큘러 영상으로부터 복합적인 4D 신(scene) 재구성을 수행하는 혁신적 방법으로, 공간 및 시간적 주의를 효과적으로 분리·통합하여 다중 상호작용 객체의 지속적인 4D 복원을 실현하였다.
