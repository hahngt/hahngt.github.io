---
layout: post
title: "Arxiv - 2025-09-23"
date: 2025-09-23 08:15:00
tags: [arxiv, multimodal, diffusion, generative AI, vision]
categories: []
---


# 1. [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers](http://arxiv.org/abs/2509.18096v1)

## Introduction
- Goal: 본 연구는 텍스트-이미지 확산 변환기에서 다중모달 확산 변환기(MM-DiT)의 주의(attention) 메커니즘을 분석하여 텍스트와 이미지의 의미적 정합(semantic grounding) 및 개방어휘 기반 분할 능력을 밝혀내는 것이다.  
- Motivation: 기존 U-Net 기반 확산 모델에 비해 MM-DiT 모델은 텍스트와 이미지 토큰의 결합된 자기주의를 통해 보다 풍부하고 확장 가능한 의미 정렬을 가능하게 하지만, 이러한 주의 맵이 이미지 생성에 어떻게 기여하는지에 대한 상세한 이해가 부족하다.  
- Contribution: 본 논문에서는 의미 정합에 중요한 역할을 하는 MM-DiT 내부의 특정 레이어를 ‘semantic grounding expert layer’로 규명하고, 이를 활용한 개방어휘 분할 기법과 경량 마스크 정렬 미세조정 방법(MAGNET)을 제안하여 분할 성능과 생성 품질을 동시에 향상시켰다.  

## Method  
다중모달 주의 메커니즘을 이용해 텍스트와 이미지 토큰 간 의미적 상호작용을 심층 분석하였다. 의미 정합 전문 레이어의 I2T 주의 맵을 활용하여 개방어휘 분할 마스크를 추출하는 무감독 분할 방식을 설계하였다. 또한 마스크 주석 이미지를 활용해 해당 레이어의 의미적 그룹화 강화를 위한 경량 미세조정 방식을 도입하였다.  

## Results  
Seg4Diff는 PascalVOC, COCO 등 다양한 데이터셋에서 경쟁력 있는 개방어휘 및 무감독 분할 성능을 보였으며, MAGNET 미세조정을 통해 텍스트-이미지 정합도와 이미지 생성 품질에서도 유의미한 향상을 달성하였다.  

## Limitations  
본 방법은 객체 간 관계 추론 및 복잡한 구성 이해에서는 일부 한계가 존재하며, 이러한 부분은 추가 연구가 필요하다.  

## Conclusion  
본 연구는 다중모달 확산 변환기의 내재된 의미 정렬 특성을 규명하고 이를 증폭할 수 있는 실용적 방법을 제시함으로써 생성과 인지를 통합하는 통합 모델 개발의 가능성을 제시하였다.

# 2. [UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning](http://arxiv.org/abs/2509.18094v1)

## Introduction
- Goal: 본 연구의 목적은 시각-언어 대형 멀티모달 모델(LMM)의 픽셀 단위 세분화 및 객체 지칭 능력을 통합하여 시각적 추론 능력을 향상하는 것이다.  
- Motivation: 기존 LMM들은 전반적인 이미지 및 영상 이해에는 강점을 보이나, 픽셀 단위의 정밀한 객체 지칭과 마스크 생성, 그리고 이를 활용한 시각적 추론을 동시에 수행하는 데 한계가 존재하였다.  
- Contribution: 본 연구는 객체 메모리 뱅크(object memory bank)를 도입해 지칭 및 분할 정보를 통합하여 유연한 비주얼 프롬프트 처리와 마스크 기반 응답 생성을 지원하는 UniPixel 모델을 제안하였다.  

## Method  
UniPixel은 점, 박스, 마스크 형태의 다양한 시각적 프롬프트를 인코딩하는 프롬프트 인코더, 객체 정보를 저장·활용하는 객체 메모리 뱅크, SAM 2.1 기반 마스크 디코더로 구성된다.  
입력된 영상과 텍스트 질의에 대해 메모리 뱅크를 초기화·갱신하며 객체 중심의 정보를 통합하고, 이를 바탕으로 공간-시간적 마스크를 예측하고 텍스트 답변을 생성한다.  
모델 훈련은 대규모 데이터셋을 활용한 세 단계의 점진적 정렬 방식을 통해 수행되었으며, 언어 모델과 마스크 디코더를 공동 최적화하였다.  

## Results  
UniPixel은 영상 객체 지칭 및 분할, 시각적 추론, 질문응답 등 다양한 픽셀 단위 시각 태스크 10개 벤치마크에서 최첨단 성능을 달성하였으며, 특히 PixelQA와 같이 지칭, 분할, 질문응답을 통합 수행하는 신규 과제에 대해 강력한 기준선을 제시하였다.  

## Limitations  
본 연구에서는 ReasonSeg과 같이 데이터 샘플이 적은 추론 분할 태스크에서 데이터 불균형으로 인한 성능 한계가 일부 관찰되었다.  

## Conclusion  
UniPixel은 객체 지칭과 분할 내부 표현을 통합해 픽셀 수준의 시각적 추론 능력을 개선하며, 향후 세밀한 시각-언어 이해 연구에 기여할 것으로 기대된다.

# 3. [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation](http://arxiv.org/abs/2509.18092v1)

## Introduction
- 본 논문은 얼굴, 헤어스타일, 의복 등 인간 이미지의 세부 속성을 개별적으로 제어하여 고품질의 맞춤형 인물 이미지를 생성하는 방법을 제안한다.  
- 기존 방법들은 정체성 보존에 집중하였으나, 시각적 속성의 독립적이고 모듈화된 제어가 불가능하였다는 한계가 존재하였다.  
- 제안하는 ComposeMe는 속성별 이미지 프롬프트와 멀티-속성 교차참조 훈련을 통해 서로 다른 이미지에서 추출한 속성들을 조합하여 자연스러운 인간 이미지를 생성한다.  

## Method  
ComposeMe는 각 속성별 전용 토크나이저를 이용하여 얼굴, 헤어스타일, 의복에 대한 입력 이미지를 개별적으로 토큰화하고, 이 토큰들을 결합하여 사전학습된 텍스트-투-이미지 확산 모델에 주입한다.  
이 과정에서 각 속성 간의 연관성을 해소하고 자연스러운 조합을 가능하게 하는 멀티-속성 교차참조 훈련 방식을 도입하였다.  
이를 통해 서로 다르게 정렬된 속성 입력들에서도 텍스트 조건에 충실하며 고품질 이미지를 생성할 수 있다.  

## Results  
ComposeMe는 다양한 실험에서 정체성 보존과 속성별 충실도 측면에서 최첨단 성능을 달성하였으며, 특히 복수의 인물과 복수 속성 조합 상황에서도 우수한 결과를 보였다.  

## Limitations  
ComposeMe는 최대 두 명의 인물과 세 가지 속성에 한정된 폐쇄형 설정에서 동작하며, 작은 얼굴 이미지에 대한 생성 오류가 발생하는 한계가 존재한다.  

## Conclusion  
ComposeMe는 속성별 이미지 프롬프트와 멀티-속성 교차참조 훈련을 결합하여 세밀하고 모듈화된 인간 이미지 생성을 가능케 하여 개인화 텍스트-투-이미지 합성 분야에 새로운 방향을 제시하였다.

# 4. [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative   Decoding](http://arxiv.org/abs/2509.18085v1)

## Introduction
- 본 논문은 손실 없이 확산 기반 대규모 언어 모델(diffusion LLM, dLLM)의 생성 속도를 대폭 향상시키는 Spiffy라는 추측적 디코딩(speculative decoding) 알고리즘을 제안하는 것을 목표로 한다.  
- 기존의 dLLM은 출력 품질 유지를 위해 각 디노이징 단계마다 한 토큰씩만 생성하여 속도가 낮은 문제를 가지고 있다.  
- Spiffy는 dLLM의 출력 분포를 유지하면서 최대 3배 이상의 추론 가속을 가능하게 하고, 다른 병렬 디코딩 기법과 결합하여 최대 7.9배의 속도 향상을 시현한다.  

## Method  
Spiffy는 dLLM의 자기 분포를 활용하여 다중 토큰 블록 형태의 초안 상태(draft states)를 생성하고, 이를 방향성 초안 그래프(directed draft graph)로 구조화하여 병렬로 검증한다.  
방향성 초안 그래프는 dLLM의 양방향 및 블록 단위 생성 특성을 반영하여 여러 부모 노드를 허용함으로써 다중 경로를 통한 상태 수용 가능성을 높인다.  
또한, 오프라인 보정 알고리즘을 통해 초안 그래프의 구조를 최적화하여 수용률을 증가시키고 전체 추론 속도를 극대화한다.  

## Results  
LLaDA-Base-8B, LLaDA-Instruct-8B, LLaDA-1.5-8B 등 다양한 공개 dLLM에서 GSM8K, HumanEval, MATH, MBPP 벤치마크를 기준으로 최대 3배의 단독 가속과 최대 7.9배의 병렬 디코딩 가속 효과를 보이며, 출력 품질 저하 없이 성능이 유지되었다.  

## Limitations  
본 연구에서 제안한 Spiffy의 효율성은 오프라인에서 수행하는 초안 그래프 보정에 의존하기 때문에, 보정 과정에 일정 수준의 계산 자원과 데이터가 요구된다.  

## Conclusion  
Spiffy는 손실 없는 추측적 디코딩을 통해 dLLM의 생성 속도를 획기적으로 개선하는 범용적이고 확장 가능한 가속화 프레임워크로, 미래의 고속 dLLM 추론 구현에 핵심적인 역할을 할 것으로 기대된다.

# 5. [Improving Large Language Models Function Calling and Interpretability   via Guided-Structured Templates](http://arxiv.org/abs/2509.18076v1)

## Introduction
- Goal: 대형 언어 모델(LLM)의 함수 호출 기능과 해석 가능성을 향상시키기 위한 구조화된 가이드 템플릿을 제안하는 것이다.  
- Motivation: 기존의 자유 형식 추론(CoT) 방식은 구조화된 함수 호출 작업에 적합하지 않고 성능 저하 및 해석력 부족 문제를 야기한다.  
- Contribution: 구조화된 추론 템플릿과 합성 데이터셋 구축 기법을 도입하여 함수 호출 정확도와 설명 가능성을 동시에 개선하였다.  

## Method  
LLM이 함수 호출을 단계별로 체계적으로 수행하도록 명확한 템플릿 기반 커리큘럼을 프롬프트 내에 포함시켜 안내한다.  
고품질 구조화된 추론 사슬을 생성하는 합성 데이터셋(ToolGT)을 구축하여 템플릿 기반의 지도학습을 수행한다.  
다양한 오픈소스 및 폐쇄형 모델을 대상으로 템플릿 기반 프롬프트 및 미세조정 전략을 평가하였다.  

## Results  
BFCLv2와 Nexus 벤치마크에서 템플릿 기반 프롬프트와 미세조정 모두 기존 CoT 및 무사고(No Thought) 방식 대비 3~12% 상대적 성능 향상을 보였다.  

## Limitations  
제한된 공개 데이터셋과 단일 턴 함수 호출 시나리오에 국한되어 복잡한 다중 턴 및 중첩 함수 호출 문제에 대한 검증은 이루어지지 않았다.  

## Conclusion  
구조화된 템플릿 기반 추론과 데이터 구축 방식을 통해 LLM의 함수 호출 능력과 투명성을 향상시킬 수 있음을 실험적으로 입증하였다.

# 6. [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation](http://arxiv.org/abs/2509.18060v1)

## Introduction
- Goal: 본 연구는 티베트어의 세 주요 방언(Ü-Tsang, Amdo, Kham)을 통합하여 다방언 텍스트-투-스피치(TTS) 합성을 수행하는 TMD-TTS 프레임워크를 제안하는 것이다.  
- Motivation: 티베트어가 자원 부족 언어이며, 세 방언 간 음운, 어휘, 구문 차이로 인해 대량의 고품질 병렬 말뭉치 확보 및 방언 간 상호이해가 어렵다.  
- Contribution: 본 연구는 방언 융합 모듈과 방언 특화 동적 라우팅 네트워크(DSDR-Net)를 도입해 미세한 음향·언어 차이를 포착하고, 대규모 다방언 합성 음성 데이터셋(TMDD)을 구축 및 공개하였다.  

## Method  
TMD-TTS는 텍스트 인코더에 방언 임베딩을 융합하고, 변형기(Transformer)의 FFN을 방언별 특화된 DSDR-Net으로 대체하여 각각의 방언 특성을 세밀하게 반영한다. 또한, 다중 방언 음성 합성을 위해 텍스트 기반 문장 샘플링과 음성 합성, 품질 검수 과정을 포함한 데이터셋 생성 파이프라인을 설계하였다.  

## Results  
TMD-TTS는 객관적(예: STOI 94.52%, PESQ 3.03, DECS 88.09%) 및 주관적 평가에서 기존 SC-CNN, VITS2, Matcha-TTS 대비 전 반향에서 뛰어난 음질과 방언 일관성을 보였다.  

## Limitations  
본 모델은 VITS2보다 다소 낮은 실시간 생성 속도를 보이며(실시간 계수 약 0.031~0.032), 최적화가 추가로 요구된다.  

## Conclusion  
통합 다방언 TTS 모델 TMD-TTS와 대규모 티베트어 다방언 합성 음성 데이터셋(TMDD)을 제안하여 티베트어 음성 합성 및 방언 변환 연구 발전을 촉진하였다.

# 7. [Reinforced Generation of Combinatorial Structures: Applications to   Complexity Theory](http://arxiv.org/abs/2509.18057v1)

## Introduction
- 본 연구의 목표는 인공지능 기술을 활용하여 복잡도 이론 내 새로운 조합론적 구조를 발견하고, 효율적 알고리즘의 증명 가능한 한계를 개선하는 것이다.  
- 이는 평균 케이스 난이도와 최악 케이스 근사 어려움 문제에서 AI 기반 기법의 활용 가능성을 탐구하기 위한 동기에서 출발하였다.  
- 본 논문은 AlphaEvolve라는 대규모 언어 모델 기반 코딩 에이전트를 활용하여 희소 랜덤 그래프의 MAX-CUT 및 MAX-Independent Set 인증 문제와 MAX-k-CUT 문제에 대해 기존 한계를 갱신한 결과를 제시하였다.

## Method
- AlphaEvolve는 제안-검증-개선(Ptr, propose-test-refine) 패러다임에 따라 조합 구조를 생성하는 코드 스니펫을 반복적으로 진화시키며 최적화 문제를 해결한다.  
- 생성된 구조의 타당성과 적합도를 검증하는 검사자(verifier)를 포함하며, 이 과정에서 검사 시간 단축을 위해 AlphaEvolve 자체를 이용하여 검사자 성능을 최대 10,000배 개선하였다.  
- 본 연구에서는 Ramanujan 그래프 탐색과 CSP 기반 기어짓기 감지(gadget reduction)를 위한 자동 탐색 및 검증 프레임워크를 구축하였다.

## Results
- AlphaEvolve를 통해 163개 노드 규모의 근사 최적 Ramanujan 그래프를 발견하여 MAX-CUT과 MAX-Independent Set에 대한 하한을 강건하게 개선하였고, MAX-3-CUT과 MAX-4-CUT에 대한 NP-근사 난이도 결과를 각각 0.9649와 0.987로 갱신하는 새로운 기어짓기 환원을 도출하였다.

## Limitations
- 기어짓기 검증에 소요되는 연산 비용이 지수적으로 증가하여 효율적인 검증 알고리즘 개발 없이는 대규모 구조 탐색에 한계가 존재한다.

## Conclusion
- AlphaEvolve 기반의 인공지능 활용은 복잡도 이론 내 조합 구조 탐색과 난이도 문제 해결에 유의미한 진전을 가능하게 하였으며, AI 보조 수학 연구의 새로운 가능성을 제시한다.

# 8. [NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and   Neuro-Symbolic Reasoning](http://arxiv.org/abs/2509.18041v1)

## Introduction
- Goal: 본 연구의 목표는 자연어 질문을 시간 논리(temporal logic)와 신경기호적 추론(neuro-symbolic reasoning)을 통해 긴 영상에서 복잡한 시계열적 질문에 정확히 답하는 것이다.  
- Motivation: 기존 비전-언어 모델은 짧은 영상이나 정적 이미지에서는 좋은 성능을 보이나, 긴 영상 내 복잡한 시간적 추론과 인과 관계를 요구하는 질문에서는 성능이 저하된다.  
- Contribution: 본 논문은 자연어 질문을 시간 논리 명세로 변환하고, 영상 자동자를 구성 및 검증하여 논리적으로 타당한 영상 부분만을 선택해 VLM에 입력하는 NeuS-QA라는 학습이 불필요한 신경기호적 LVQA 파이프라인을 제안하였다.  

## Method
NeuS-QA는 자연어 질문을 시간 논리 공식으로 변환하고, 프레임별 의미 명제들을 기반으로 영상 자동자를 구성하여 모델 체크를 수행한다. 이때 논리적으로 검증된 영상 구간만 후속 VLM에 입력하여 정확한 답변을 유도한다. 또한, 신경기호적 기법을 통해 복잡한 시간적 질문에 적합한 유의미한 프레임 집합을 자동으로 추출한다.  

## Results
NeuS-QA는 LongVideoBench와 CinePile 벤치마크에서 기존 VLM 및 구조화 추론 기법 대비 10% 이상 정확도를 향상시켰으며, 특히 사건 순서, 인과 관계, 다단계 합성 추론 질문에서 크게 성능이 개선되었다.  

## Limitations
NeuS-QA의 성능은 영상 내 의미 명제 추출의 정확도에 크게 의존하며, 시각 정보가 모호하거나 단서가 미미한 경우에는 논리 명세 검증에서 실패할 수 있다.  

## Conclusion
NeuS-QA는 영상 내용과 질의 구성을 신경기호적으로 정렬하여 긴 영상의 복잡한 시간적 질문에 체계적이고 해석 가능한 답변을 제공하는 범용 LVQA 프레임워크임을 입증하였다.

# 9. [RadEval: A framework for radiology text evaluation](http://arxiv.org/abs/2509.18030v1)

## Introduction
- Goal: 본 연구의 목표는 방사선 텍스트 평가를 위한 통합되고 오픈소스인 평가 프레임워크 RadEval을 제안하는 것이다.  
- Motivation: 방사선 보고서 생성의 평가가 임상적 정확성, 전문 용어, 진단 관련성 등을 포괄적으로 반영하지 못하고 평가 지표별 구현 차이로 재현성이 떨어지는 문제점이 존재하기 때문이다.  
- Contribution: RadEval은 다양한 기존 평가 지표를 표준화 및 개선하여 통합 구현하고, 전문의가 주석한 데이터셋을 공개하며, 다중 공개 데이터셋에서 모델 평가와 통계적 유의성 검정을 지원한다.  

## Method  
본 프레임워크는 고전적인 n-그램 중첩 기반 지표부터 임상 개념 추출, 의미 관계 그래프, 최첨단 대형언어모델 기반 평가자를 포함하는 다양한 평가 지표를 하나의 모듈형 오픈소스 코드베이스로 제공한다.  
특히 방사선 문서 특화 문장 임베딩을 위한 RadEvalBERTScore를 개발하여 제로샷 검색 성능을 입증하였다.  
또한 최신 LLM평가자 GREEN의 경량화 버전을 도입하여 다중 영상 모달리티에 대응하고 프라이버시 보호 및 재현 가능 평가를 가능하게 하였다.  

## Results  
RadEval은 공개 전문가 주석 데이터에서 임상적 중요 오류와 평가 지표 간 상관관계를 분석하여 GREEN, SRR-BERT와 RadCliQ, RadEvalBERTScore가 임상 오류 검출에 가장 높은 일치도를 보였고, MIMIC-CXR, CheXpert-Plus, ReXGradient-160K 등 다양한 공개 데이터셋에서 여러 최신 보고서 생성기들의 평가를 구현하였다.  

## Limitations  
본 프레임워크는 최근 제안된 일부 LLM 기반 평가 지표들을 포함하지 않으며, 주로 미국 영어 흉부 X선 보고서에 한정된 연구로 다국어 및 공간적 확장성에는 제약이 존재한다.  

## Conclusion  
RadEval은 방사선 보고서 생성 평가의 재현성, 표준화, 임상 기반 신뢰성을 크게 향상시키며, 다방면의 평가 지표 통합과 경량 LLM 지원을 통해 방사선 인공지능 시스템의 안전하고 효과적인 도입을 가속화할 수 있음을 보였다.

# 10. [Variation in Verification: Understanding Verification Dynamics in Large   Language Models](http://arxiv.org/abs/2509.17995v1)

## Introduction
- Goal: 본 연구는 문제 난이도, 생성기 및 검증기의 생성 능력이 대형 언어 모델의 검증 성능에 미치는 영향을 체계적으로 분석하는 데 목적이 있다.  
- Motivation: 대형 언어 모델의 출력에는 여전히 신뢰할 수 없는 오류가 존재하며, 검증 메커니즘을 통해 이를 효과적으로 식별하는 것이 필요하다.  
- Contribution: 본 논문에서는 12개 벤치마크와 14개 공개 모델 및 GPT-4o를 대상으로 검증 역학을 실험적으로 분석하여 검증 성공에 영향을 주는 세 가지 주요 요인을 규명하였다.  

## Method  
- 생성적 검증기는 사유 연쇄(chain-of-thought) 추론 후 이진 판단을 생성하는 방식으로 검증을 수행한다.  
- 문제 난이도는 다양한 생성기들의 평균 통과율로 정의하였고, 생성기 및 검증기의 생성 능력은 각 모델의 정답 생성률(pass rate)로 측정하였다.  
- 검증 성능 평가는 참 정답 인식 확률(true positive rate)과 오답 탐지 확률(true negative rate)을 중심으로 수행하였다.  

## Results  
- 문제 난이도가 낮을수록 검증기가 올바른 답변을 인식하는 능력이 향상되고, 생성 능력이 약한 생성기가 만든 오류일수록 검증이 용이하며, 검증기의 생성 능력과 검증 성능은 문제 난이도에 따라 비선형적으로 상관관계가 나타났다.  

## Limitations  
- 본 연구는 수학적 추론, 지식 기반 질의응답, 자연어 추론 세 영역에 기반해 분석했으나, 다른 도메인에 대한 일반화 가능성은 추가 검증이 필요하다.  

## Conclusion  
- 문제 난이도, 생성기 및 검증기의 생성 능력은 모두 검증 성능에 중요한 영향을 미치며, 이들의 상호작용을 고려한 검증 전략이 테스트 시 성능 향상과 비용 효율화에 기여한다는 점이 확인되었다.
