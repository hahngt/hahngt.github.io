---
layout: post
title: "Arxiv - 2025-09-02"
date: 2025-09-02 08:15:00
tags: [papers, arxiv, ai]
categories: []
---

# 1. [DriveQA: Passing the Driving Knowledge Test](http://arxiv.org/abs/2508.21824v1)

## Introduction

- Goal: 본 연구는 대형 언어 모델(LLM)이 운전 지식 시험에서 통과할 수 있는지를 평가하는 DriveQA라는 종합적인 텍스트 및 시각 기반 벤치마크를 제안하는 데 목적이 있다.
- Motivation: 기존 자율주행 벤치마크들은 주로 공간 인지 및 시각적 질문응답에 집중하는 반면, 실제 운전 지식 시험은 교통 규칙, 교통 표지, 우선통행 원칙 등 복잡하고 다양한 교통 상황에 대한 완전한 이해를 요구한다.
- Contribution: DriveQA는 19개 질문 카테고리와 22만 개 이상의 트래픽 표지를 포함하며, 최신 LLM 및 다중모달 LLM(MLLM)의 교통 규칙 이해 능력 평가, 모델의 미세 조정 효과 분석, 환경 변화에 대한 모델 민감도 고찰 및 실제 주행 데이터셋으로의 전이 가능성을 실험적으로 입증하였다.

## Method

DriveQA는 텍스트 기반 DriveQA-T와 이미지-텍스트 기반 DriveQA-V로 구성되며, 미국 50개 주의 운전자 매뉴얼에 기반해 GPT-4o로 질문을 자동 생성 후 수동 검수하였다. DriveQA-V는 CARLA 시뮬레이터를 활용해 다양한 시점, 기상, 조명 조건에서 220가지 이상의 미니 모델 교통 표지를 포함한 합성 이미지 질문을 생성하였다. 모델 평가는 체인 오브 띵킹(CoT), 검색 기반 생성을 통한 외부 문서 참조(RAG), 그리고 미세 조정 방식을 복합적으로 사용하였다.

## Results

최신 LLM과 MLLM은 기본 교통 규칙에는 강점을 보이나, 수치 추론과 복잡한 우선통행 상황, 교통 표지 변이, 공간 배치 인식에 약점을 드러냈으며, DriveQA로의 미세 조정 시 정확도가 전반적으로 향상되고 실제 자율주행용 데이터셋(nuScenes, BDD 등)과의 전이성도 개선되었다.

## Limitations

본 벤치마크는 정적 교통 규칙 이해에 초점을 맞추어 다이나믹한 비디오 기반 상황 인식 및 공간 추론 능력 강화가 미흡하며, 합성 데이터 의존으로 인한 도메인 적응 문제도 존재한다.

## Conclusion

DriveQA는 대형 언어 및 다중모달 모델의 교통 규칙 및 복잡한 운전 지식 평가를 위해 설계된 포괄적이고 실용적인 벤치마크로, 모델의 실제 주행 상황 적용 가능성 제고와 교통 규칙 이해 능력 향상을 위한 기반을 제공한다.

# 2. [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](http://arxiv.org/abs/2508.21815v1)

## Introduction

- Goal: 본 연구는 Rényi 차등개인정보보호 하에서 힐베르트-슈미트 독립성을 달성하여 공평하고 개인정보 보호가 보장된 합성 데이터 생성을 목표로 한다.
- Motivation: GDPR, HIPAA, AI 법률 등 개인정보 및 윤리적 데이터 활용 규제가 강화되면서 민감 영역의 탭형 데이터 공유와 모델 개발에 안전한 대안이 필요하다.
- Contribution: FLIP(Fair Latent Intervention under Privacy guarantees)라는 변분 오토인코더 기반 모델을 제안하여 태스크 비종속적 조건에서 균형 샘플링과 잠재 공간 내 힐베르트-슈미트 독립성 기준 정렬을 통해 공정성과 개인정보 보호를 동시에 달성한다.

## Method

FLIP는 변분 오토인코더(VAE)와 잠재 확산 모델을 결합하여 이질적 탭형 데이터를 생성하며, 두 단계 학습 절차로 고품질 표현 학습 후 보호속성 분리를 수행한다.  
학습 중에는 그룹별 노이즈 조절이 가능한 Rényi 차등개인정보보호 제약과 균형 샘플링을 적용하며, 잠재 공간에서는 Centered Kernel Alignment(CKA)를 통한 활성화 패턴 정렬로 보호속성과 학습 표현의 통계적 독립성을 유도한다.  
또한 그룹별 샘플 비율이 다른 균형 샘플링에 대응하는 RDP 호환 기법을 개발하여 다양한 보호 그룹을 공평하게 처리한다.

## Results

FLIP는 다양한 공개 데이터셋에서 태스크 비의존적 상황과 여러 다운스트림 태스크 전반에 걸쳐 차등개인정보보호 조건 하에서 유의미한 공정성 개선과 양질의 합성 데이터 생성을 달성함을 입증하였다.

## Limitations

공정성 강화 시 표현의 분산 및 충실도 저하가 불가피하여 공정성-품질 간의 절충 문제가 남아있다.

## Conclusion

FLIP는 태스크 비종속적 접근으로 공정성과 개인정보 보호를 포괄적으로 관리하는 혁신적 합성 데이터 생성 기법임이 입증되었다.

# 3. [VoCap: Video Object Captioning and Segmentation from Any Prompt](http://arxiv.org/abs/2508.21809v1)

## Introduction

- Goal: 본 논문은 영상 내 객체를 다양한 형태의 입력 프롬프트(텍스트, 박스, 마스크)로 받아 시공간적 분할 마스크와 객체 중심 캡션을 동시에 생성하는 영상 객체 캡셔닝 및 분할 모델 VoCap을 제안하는 데 목적이 있다.
- Motivation: 기존 컴퓨터 비전 시스템은 영상에서 객체의 세밀한 분할과 자연어 기반 의미 이해를 동시에 수행하지 못하는 한계가 존재한다.
- Contribution: 본 연구는 대규모 분할 데이터셋인 SAV에 고품질의 의사 캡션을 자동 생성하고, 이를 활용해 VoCap 모델을 공동학습하여 영상 객체 참조 분할과 캡셔닝에서 최첨단 성능을 달성하였다.

## Method

VoCap 모델은 각 프레임을 개별적으로 처리하는 이미지 인코더, 시간적 정보를 통합하는 메모리 모듈, 위치 및 텍스트 프롬프트 인코더, 마스크 디코더와 언어 생성 모듈로 구성된다.  
대규모 비전-언어 모델을 활용해 기존 영상 분할 데이터에 객체 중심 캡션을 의사 라벨링하여 SAV-Caption 데이터셋을 구축하고, 본 모델은 다양한 분할·캡션 데이터셋과 함께 멀티태스크 학습된다.  
추론 시 VoCap은 온라인 방식으로 시퀀스 상 모든 프레임에서 분할 마스크와 캡션을 생성하며, 객체의 존재 여부에 따라 최종 캡션을 결정한다.

## Results

VoCap은 참조 표현 영상 객체 분할(RefVOS)에서 최첨단 성능을 달성하였고, 반감독 영상 객체 분할 및 객체 캡셔닝에서도 경쟁력 있는 결과를 보였다.

## Limitations

객체가 첫 프레임에 명확히 나타나지 않거나 애매한 경우에는 추적 및 캡션 생성 정확도가 저하될 수 있다.

## Conclusion

VoCap과 대규모 SAV-Caption 데이터셋은 유연한 입력 프롬프트를 통해 상세한 시공간적 영상 객체 이해를 실현하며 관련 분야 연구의 기초를 마련하였다.

# 4. [Tree-Guided Diffusion Planner](http://arxiv.org/abs/2508.21800v1)

## Introduction

- Goal: 본 논문은 비지도 학습된 확산 모델을 활용하여 복잡한 테스트 시 제어 문제에 대응 가능한 제로샷 계획 기법을 제안하는 것을 목표로 한다.
- Motivation: 기존의 그래디언트 기반 안내 기법은 볼록 함수와 미분 가능 목표 하에서만 효과적이며, 실제 복잡한 문제에서 나타나는 비볼록성과 비미분성 제약 조건에는 한계가 존재한다.
- Contribution: 탐색과 활용의 균형을 이루는 트리 구조를 통해 다양한 궤적을 생성하고, 사전학습 모델과 테스트 시 보상 신호만으로 최적 해를 찾는 Tree-guided Diffusion Planner (TDP)를 제시하였다.

## Method

TDP는 두 단계의 샘플링을 수행한다: 첫째, 입자 안내 방식을 통해 다양한 부모 궤적을 탐색하고, 둘째, 해당 궤적 위에서 빠른 조건부 복원을 통해 서브 트리를 확장하며 과제 목적에 부합하도록 정교화한다.  
상태를 관측 상태와 제어 상태로 자동 분해하고, 제어 상태 공간에서 탐색을 촉진하는 입자 안내와 관측 상태에 대한 그래디언트 안내를 결합하여 넓은 해 공간을 효과적으로 탐색한다.  
이러한 이중 수준의 트리 탐색 구조는 다목적, 비볼록, 비미분성 제약 문제에서 기존 방법 대비 우수한 성능을 보장한다.

## Results

미로의 금 획득, 로봇 팔 블록 조작, AntMaze 다목표 탐험 등 다양한 실험에서 TDP는 기존 최첨단 제로샷 계획 방법들 대비 일관되게 향상된 성과를 나타내었다.

## Limitations

비록 TDP가 다양한 복잡한 제약 조건을 처리하지만, 입자 안내 과정에서의 계산 비용 증가와 하이퍼파라미터 조정에 관한 제한점이 존재한다.

## Conclusion

TDP는 사전학습된 확산 모델과 테스트 시 보상 신호만을 활용하여 복잡한 테스트 시 환경에서도 효율적이고 유연한 제로샷 계획이 가능함을 입증하였다.

# 5. [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](http://arxiv.org/abs/2508.21793v1)

## Introduction

- Goal: 본 연구는 불완전하고 이질적인 다중모달 의료 데이터를 효과적으로 융합하여 임상 예측의 견고성을 높이기 위한 MoE-Health 프레임워크를 제안하는 것이다.
- Motivation: 실제 임상 현장에서는 데이터 모달리티 간 가용성이 환자 및 기관별로 다양하게 나타나며, 기존 모델은 완전한 데이터만을 전제로 하여 적용에 한계가 존재한다.
- Contribution: MoE-Health는 모달리티별 특화된 전문가 네트워크와 동적 게이팅 메커니즘을 통해 다양한 결측 상황에 적응하며 우수한 예측 성능과 견고성을 동시에 달성한다.

## Method

MoE-Health는 구조화된 전자의무기록(EHR), 임상 서술문, 흉부 X선 영상 세 가지 모달리티를 각각 인코딩하고, 결측된 모달리티에 대해 학습 가능한 결측 인디케이터 임베딩을 활용한다. 모달리티 조합별 전용 전문가 MLP들을 사전학습하여 특화한 뒤, 동적 게이팅 네트워크가 입력 데이터에 적합한 전문가를 선택하여 가중합함으로써 최종 예측을 수행한다. 학습 시에는 예측 손실과 전문가 부하 균형 손실을 함께 최적화한다.

## Results

MIMIC-IV 데이터셋에서 시행한 세 가지 임상 예측 과제(입원 중 사망, 장기 입원, 재입원) 실험에서 MoE-Health는 기존 다중모달 융합 방법 대비 AUROC 및 F1 점수에서 일관되게 우수한 성능을 보였다.

## Limitations

본 연구는 단일 기관 데이터에 기반하여 제안한 모델의 일반화 능력 및 타 기관 데이터에 대한 적용 가능성 검증이 필요하다.

## Conclusion

MoE-Health 프레임워크는 결측과 이질성이 존재하는 다중모달 의료 데이터를 효과적으로 융합하여 현실 임상 환경에 적합한 견고하고 성능이 뛰어난 예측 모델을 제시한다.

# 6. [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](http://arxiv.org/abs/2508.21732v1)

## Introduction

- Goal: 본 연구는 대형 비전-언어 모델(LVLM)의 디지털 측정기기(DMD) 판독 능력 향상을 위한 합성 데이터 생성 도구 CAD2DMD-SET를 개발하는 데 목적이 있다.
- Motivation: 현재 LVLM들은 실제 환경에서 측정기기 값을 정확히 읽지 못하는 한계가 존재하며, 특히 혼잡한 배경, 장애물, 다양한 시점과 모션 블러가 있는 조건에서 성능 저하가 심하다.
- Contribution: 본 연구는 (1) DMD용 합성 데이터 생성 도구인 CAD2DMD-SET 제공, (2) 실제 이미지 기반 검증 세트 DMDBench 구축, (3) CAD2DMD-SET 데이터로 LVLM 미세조정 및 (4) 모델 성능 개선을 입증하는 실험적 결과를 제시한다.

## Method

CAD2DMD-SET은 3D CAD 모델과 Blender 렌더링, 이미지 합성(libcom 라이브러리 활용)을 통해 다양한 디지털 표시 이미지와 고품질 합성 데이터를 생성한다.  
생성된 합성 이미지는 VQA(Visual Question Answering) 라벨이 붙어 있으며, 실제 환경을 반영한 배경과 전경 객체 합성을 거쳐 현실감 높은 데이터셋을 구성한다.  
이 도구는 UV 매핑과 렌더링 매개변수 조절을 통해 다양한 카메라 거리 및 조명 상태를 모사하며, 최종 합성 이미지에 모션 블러 효과도 적용한다.

## Results

세 가지 최첨단 LVLM(InternVL, LLaVA, Pixtral-12B)을 DMDBench로 벤치마킹한 결과, CAD2DMD-SET으로 미세 조정한 InternVL 모델은 ANLS 점수가 32.92%에서 96.04%로 약 200% 향상되었으며, 다른 작업 성능 저하 없이 개선되었다.

## Limitations

합성 데이터의 현실감 향상(예: 마모와 흠집 표현) 및 더욱 다양한 주석 타입과 평가 지표의 확장 필요성 등이 한계로 남아 있다.

## Conclusion

CAD2DMD-SET은 디지털 측정기기 판독 정확도 향상에 효과적이며, 실제 환경에서 LVLM의 성능 강화를 위한 중요한 기초 자료를 제공한다.

# 7. [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](http://arxiv.org/abs/2508.21712v1)

## Introduction

- 본 연구의 목표는 저데이터 환경에서 객체 검출을 위한 합성 데이터를 효율적으로 생성하는 경량화된 파이프라인 FLORA를 제안하는 것이다.
- 기존 확산기반 생성 모델은 대규모 전면 미세조정이 요구되어 고가의 GPU와 대량의 합성 데이터가 필요하지만, 이는 자원 접근성을 저해하는 문제점이 존재한다.
- 본 연구는 Flux 1.1Dev 모델과 LoRA 기반 미세조정으로 연산 및 데이터 요구량을 획기적으로 줄여, 소비자 등급 GPU에서도 고성능 객체 검출 향상이 가능함을 입증하였다.

## Method

FLORA는 두 단계로 구성되는데, 첫째 각 객체 카테고리에 대해 30개의 객체 크롭 이미지를 사용하여 LoRA 기반 미세조정을 수행한다.  
둘째, 원본 이미지의 영역을 마스크 처리한 후 LoRA 모듈을 적용한 확산 모델을 통해 문맥적 일관성과 위치를 유지한 합성 이미지를 생성한다.  
이 과정은 단일 소비자 GPU에서 하루 이내에 완성되며, 추가 후처리 없이 원래의 라벨을 그대로 사용한다.

## Results

FLORA는 6개 도메인 데이터셋 대상으로 500개의 합성 이미지만으로 ODGEN 기반 5,000개 합성 이미지 대비 최대 21.3% mAP 향상을 달성하며, 5개 데이터셋에서 최고 성능을 기록하였다.

## Limitations

MRI 데이터셋에서 ODGEN에 비해 근소하게 낮은 객체 검출 성능을 보였으며, LoRA 미세조정 파라미터 탐색이 제한적으로 진행되었다.

## Conclusion

FLORA는 적은 양의 고품질 합성 데이터로 강력한 객체 검출 성능을 가능하게 하여, 고비용 대규모 미세조정 방식을 대체할 실용적이고 효율적인 합성 데이터 생성 방법임을 증명하였다.

# 8. [Is this chart lying to me? Automating the detection of misleading visualizations](http://arxiv.org/abs/2508.21675v1)

## Introduction

- Goal: 본 연구의 목적은 오해를 유발하는 시각화를 자동으로 탐지하고 이들이 위반하는 구체적 디자인 규칙을 식별하는 것이다.
- Motivation: 시각적 왜곡은 사회관계망서비스와 웹에서 잘못된 정보 확산의 주요 원인이나, 이를 탐지하기 위한 AI 모델 개발은 대규모, 다양하며 공개된 데이터셋 부재로 제한되어 왔다.
- Contribution: 본 연구는 12가지 유형의 오해 유발 요소가 주석된 2,604개의 실제 시각화 데이터셋 Misviz와 81,814개의 합성 시각화 데이터셋 Misviz-synth를 공개하고, 이를 활용한 종합적 평가 결과를 제시하였다.

## Method

본 연구는 12종 오해 유발 유형을 엄선하여 실제 데이터셋 Misviz를 구성하고, 실제 표 기반의 합성 시각화 Misviz-synth를 개발하였다. Matplotlib를 활용하여 합성 데이터를 생성하고, 축 정보 메타데이터를 포함해 표와 시각화를 연결함으로써 축 추출기(DePlot)와 결합한 탐지 모델을 학습하였다. 세 가지 접근법인 다중 모달 대형 언어 모델(MLLM), 룰 기반 린터, 미세 조정된 분류기를 대상으로 정확도와 정밀도 등을 평가하였다.

## Results

실험 결과, MLLM은 실제 시각화에서 가장 우수한 성능을 보였으나, 합성 데이터셋에서는 축 메타데이터를 활용한 린터 및 미세 조정 분류기가 우위를 점하였고, 축 추출 모듈의 실세계 일반화가 미흡함을 확인하였다.

## Limitations

합성 데이터셋 Misviz-synth는 차트 유형 다양성이 제한되고, 실제 현장에 존재하는 일부 오해 유발 유형과 복잡한 다중 차트 구성을 반영하지 못하며, 축 추출의 실세계 일반화에도 한계가 존재한다.

## Conclusion

본 연구는 오해를 부르는 시각화 탐지 분야에서 대규모 공개 데이터셋과 새로운 모델 평가 기준을 확립함으로써 향후 연구와 실제 활용에 기여하는 기반을 마련하였다.

# 9. [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](http://arxiv.org/abs/2508.21581v1)

## Introduction

- Goal: 본 연구는 명확세포 신세포암(ccRCC)의 개인 맞춤형 재발 위험 예측을 위해 병리학적 전후영상(CT 및 WSI)을 통합하는 다중모달 예측 모델을 개발하고 평가하는 데 목적이 있다.
- Motivation: 기존의 Leibovich 점수는 환자 수준의 세분화가 미흡하고 영상 정보를 포함하지 않아 개선된 위험 평가 방법이 요구된다.
- Contribution: 기반 모델을 활용한 모듈화된 딥러닝 프레임워크를 통해 CT 영상과 병리학적 전자슬라이드(WIS)를 중간 융합 방식으로 통합하여 재발 위험 예측 성능을 향상시킨 점이다.

## Method

본 연구는 TCGA-KIRC 다중모달 데이터를 이용해 CT와 WSI로부터 특징 벡터를 추출하며, 각각 MedicalNet, SwinUNETR, TITAN–CONCH 등의 사전학습된 인코더를 사용하였다. 추출된 특징은 코프스(Cox) 기반 생존분석과 다층 퍼셉트론(MLP) 모델로 각각의 모달리티 또는 융합 모델에 적용되어 재발 위험 점수를 예측하였다. 모델 학습은 5겹 교차검증과 Optuna 하이퍼파라미터 최적화를 통해 진행하였다.

## Results

병리학적 WSI 기반 모델이 CT 단독 모델보다 일관되게 우수했으며, 중간 융합 방식으로 통합한 TITAN–CONCH와 ResNet-18 조합이 임상적 Leibovich 점수에 근접하는 성능(C-index 0.775±0.044)을 보였다.

## Limitations

CT 영상에 대한 사전학습 모델의 한계와 데이터셋 크기 제한으로 인해 CT 단독 모델의 예측 안정성과 성능이 상대적으로 낮았다.

## Conclusion

CT 영상과 병리학적 전자슬라이드의 다중모달 융합은 명확세포 신세포암 재발 위험 예측의 개인화 정확도를 향상시키며, 보다 발전된 융합 기법과 대규모 데이터셋 개발이 필요하다.

# 10. [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](http://arxiv.org/abs/2508.21580v1)

## Introduction

- Goal: 본 연구는 4D 종적 의료 영상에서 시공간적 궤적을 학습하기 위한 통합 생성 모델인 Temporal Flow Matching (TFM)을 제안하는 데 목적이 있다.
- Motivation: 기존 심층학습 기법들은 단일 시점 중심 분석, 분류나 회귀에 치중하여 세밀한 공간적 예측에 제한적이며, 시공간적 변화를 효과적으로 모델링하지 못하는 한계가 존재한다.
- Contribution: TFM은 여러 시점 영상과 불규칙 시계열을 지원하며 차별화된 변화만을 모델링하는 차분 모델링(Difference Modeling)을 도입하여 4D 의료 영상 예측에서 최첨단 성능과 안정적 기준선을 수립하였다.

## Method

TFM은 플로우 매칭(flow matching) 기법을 확장하여 불규칙하고 희소한 종적 시계열에서 각 시점 영상으로부터 목표 시점 영상으로의 변환을 학습한다. 모델은 결손된 시점 영상을 가장 최근의 이미지로 채우는 희소성 보충(sparsity filling) 전략을 적용하며, 전체 시계열을 통합적으로 처리하여 시공간 의존성을 효과적으로 활용한다. 학습은 수치적 미분방정식(ODE) 적분을 통해 중간 상태에서의 속도장 예측을 최적화한다.

## Results

세 개의 공개 종적 의료 영상 데이터셋(심장 MRI, 뇌졸중 CT, 뇌종양 MRI)에서 실험한 결과, TFM은 모든 성능 지표(정규화된 RMSE, SSIM, PSNR)에서 기존 자연 영상 및 의료 영상 기반 시공간적 예측 기법과 LCI(마지막 맥락 영상) 기준선을 일관되게 능가하였다.

## Limitations

본 연구는 대규모 고품질 종적 영상 코호트의 희소성과 고차원 전역 변환 모델의 최적화 어려움 등도존재하며, 패치 기반 국소화 모델과 대규모 사전학습 모델 도입이 향후 과제로 남는다.

## Conclusion

TFM은 불규칙 다시점 의료 영상에서 변화만을 효율적으로 모델링하는 차분 생성 모델로서 4D 의료 영상 예측에 새로운 표준을 제시하며, 후속 임상 적용과 대규모 데이터 확보의 필요성을 강조한다.

# 11. [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](http://arxiv.org/abs/2508.21570v1)

## Introduction

- Goal: 본 연구의 목표는 희소한 드리프터 궤적을 활용하여 해양 염분 값을 정확히 보간하는 새로운 확산 적대 네트워크 기반의 해양 염분 보간 시스템(OASIS)을 제안하는 것이다.
- Motivation: 기존 원격 탐사 및 최적 보간 기법은 선형성과 정상성 가정 및 위성 재방문 주기의 제한으로 인해 복잡한 해양 염분 변동을 효율적으로 포착하지 못한다는 문제에서 연구가 출발하였다.
- Contribution: 본 연구는 전역적 시공간 의존성 학습을 위한 트랜스포머 모듈, 조수 고도 조건부 생성 모델, 확산 스케줄러를 통합하여 기존 대비 최대 52.5% MAE 감소 효과를 보이는 완전 자동화된 염분 보간 프레임워크를 개발하였다.

## Method

OASIS는 드리프터 센서 간 분포 불일치를 해소하기 위한 정규화, 장거리 시공간 의존성을 포착하는 트랜스포머 기반 모듈, 코사인 스케줄러 확산과 적대적 손실을 결합한 생성 모델로 구성된다.  
시간, 공간 좌표, 조수 고도를 입력으로 하여 희소 데이터에서 물리적 조수 신호의 주기성을 활용함으로써 정확한 염분 값을 점진적으로 보정한다.  
정규화과정을 통한 안정적 학습과 어텐션 메커니즘을 통한 특성 추출을 통해 드리프터 궤적의 불규칙성과 데이터 부족 문제를 극복한다.

## Results

네 개의 벤치마크 데이터셋과 실제 플로리다 해역 관측 데이터를 대상으로 한 실험에서, OASIS는 Kriging 등 전통적 및 최신 신경망 모델 대비 RMSE 21.3%, MAPE 18.5%까지 감소시켜 일관된 성능 향상을 입증하였다.

## Limitations

해양 환경의 복잡성과 센서 특성 변이에 따른 데이터 이질성을 완전히 해소하기 위해 추가 연구와 대규모 현장 테스트가 요구된다.

## Conclusion

본 연구에서 제안한 OASIS는 조수 고도와 시공간 의존성을 효과적으로 통합한 확산 적대 네트워크 기반 모델로서 희소한 드리프터 데이터를 활용하여 해양 염분 보간의 정확도와 견고성을 크게 향상시켰다.

# 12. [ECHO: Ego-Centric modeling of Human-Object interactions](http://arxiv.org/abs/2508.21556v1)

## Introduction

- Goal: 본 연구는 간결한 3점(head 및 손목) 추적 정보만으로 에고센트릭 시점에서 인간-물체 상호작용(HOI)을 모델링하는 방법을 제안하는 데 목적이 있다.
- Motivation: 웨어러블 기기의 보급 증가에 따라 사용자 활동 및 상호작용을 최소한의 센서 정보로 정확히 복원하는 연구가 필요하다.
- Contribution: 본 논문은 인간 자세, 물체 동작, 접촉 시퀀스를 통합적으로 복원하는 최초의 통합 프레임워크인 ECHO를 제안하였다.

## Method

ECHO는 변형된 확산 변환기(Diffusion Transformer) 아키텍처와 세 변량 확산 과정을 활용하여 인간, 물체, 접촉의 상호의존성을 동시에 모델링한다.  
모델은 머리 중심 좌표계에서 작동하며, 적은 정보에도 견고하게 대응할 수 있는 컨베이어 기반 추론 방식을 도입하였다.  
추가로 부분적으로 관측될 수 있는 인간 또는 물체의 일부 정보도 효과적으로 활용할 수 있도록 설계되었다.

## Results

ECHO는 BEHAVE, OMOMO, AMASS 데이터셋에서 기존 방법 대비 인간-물체 상호작용 복원 및 인간 동작 재현 성능에서 우수한 성능을 보였다.

## Limitations

본 연구는 다층 환경이나 미세한 손동작과 같이 보다 복잡한 조건을 포함하는 상호작용을 다루기 위해 추가 센서 연동 및 공간 정보 통합 연구가 요구된다.

## Conclusion

ECHO는 에고센트릭 3점 추적으로부터 인간-물체 상호작용을 효과적으로 복원하는 최초의 모델로서, 실시간 응용과 후속 연구에 유용한 기반을 제공한다.

# 13. [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](http://arxiv.org/abs/2508.21542v1)

## Introduction

- Goal: 단일 이미지로부터 완전한 3D 장면을 Gaussian splats로 재구성하는 방법을 제안하는 것이다.
- Motivation: 기존의 Gaussian splatting은 장면의 밀집 관측이 필요하며, 가려지거나 관측되지 않은 영역의 재구성이 불가능하거나 부정확한 문제가 있었다.
- Contribution: 별도의 3D 정답 데이터 없이 2D 이미지만으로 학습 가능한 Variational AutoReconstructor와 latent diffusion 모델을 활용하여 고품질이며 다양성을 지닌 완전한 3D 장면 재구성을 가능하게 하였다.

## Method

Variational AutoReconstructor는 2D 이미지만으로부터 3D Gaussian splats의 잠재 공간을 자기지도학습으로 학습한다. 이와 더불어, latent diffusion 모델을 통해 단일 입력 이미지 조건 아래에서 다양한 3D 장면 샘플링을 수행한다. 고주파 세부 묘사 보존을 위한 skip connection과 모델의 신뢰도 및 다양성 조절을 위한 classifier-free guidance 기법도 적용되었다.

## Results

CO3D 및 RealEstate10K 데이터셋에서 기존 최첨단 방법들 대비 더 선명하고 완전한 재구성 성능과 함께 가려진 영역의 복원 및 다양한 복원 결과 샘플링이 가능함을 실험적으로 입증하였다.

## Limitations

복잡한 장면이나 다중 뷰 기반 3D 정답 부재로 인한 일부 극한 상황에서의 재구성 정확도 제한 가능성이 존재한다.

## Conclusion

본 연구는 단일 이미지 입력만으로도 고품질 3D Gaussian splats 완전 장면 재구성이 가능하며, 자기지도 학습과 생성 모델을 결합한 효과적인 학습 파이프라인을 제안하였다.

# 14. [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](http://arxiv.org/abs/2508.21496v1)

## Introduction

- 본 연구는 긴 영상 이해 과정에서 발생하는 의미 집계 환각(Semantic Aggregation Hallucination, SAH)을 체계적으로 평가하기 위한 벤치마크 ELV-Halluc를 제안하는 것이다.
- 기존 연구들이 주로 단편 영상에서의 환각 현상에 집중한 반면, 긴 영상에서는 다중 이벤트 간 의미 집계 오류인 SAH가 중요한 문제임에도 불구하고 충분히 탐구되지 않았다.
- 본 연구는 SAH를 정량화하고 완화하는 방안을 제시하며, 8,630개의 적대적 질의응답 쌍으로 구성된 데이터셋을 공개한다.

## Method

ELV-Halluc는 이벤트별로 명확히 구분된 긴 영상에서 다양한 환각 유형을 평가하도록 설계되었고, GPT-4o를 활용해 영상 내외부 의미 변형 질문을 생성하는 반대 질문쌍(adversarial triplet QA pairs) 방식을 도입하였다.  
환각 민감도를 측정하기 위해 영상 내 환각 질문에 대한 정확도와 영상 외 환각 질문에 대한 정확도 차이로 SAH 비율을 계산하며, 이를 통해 모델이 의미를 잘못 집계하는 경향성을 분석하였다.  
또한, 위치 인코딩 강화와 Direct Preference Optimization(DPO) 학습 전략을 적용하여 SAH 완화 효과를 검증하였다.

## Results

ELV-Halluc 평가 결과, 의미 복잡도가 증가할수록 SAH가 심화되며, 빠르게 변하는 시각적 세부정보에서 SAH 빈도가 높았고, 제안한 위치 인코딩과 DPO 전략을 활용해 최대 27.7%의 SAH 비율 감소와 전체 영상 이해 성능 향상을 달성하였다.

## Limitations

본 연구의 한계는 Gemini-2.5 Flash가 캡션 생성 과정에 사용되어 일부 결과에 편향을 유발할 수 있고, 이벤트 중심 데이터셋이 현실 긴 영상과 차이가 있으며, 고품질 주석 비용으로 인해 데이터셋 규모가 제한적이라는 점이다.

## Conclusion

본 연구는 긴 영상 내 의미 집계 환각 문제를 분석하고 이를 평가·완화할 수 있는 최초의 장영상 환각 벤치마크와 실험적 근거를 제시하여 신뢰성 높은 영상 이해 모델 개발에 기여하였다.

# 15. [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](http://arxiv.org/abs/2508.21475v1)

## Introduction

- Goal: 본 연구는 미세한 시각 신호를 활용한 정교한 다중모달 추론과 장기 도구 활용을 요구하는 멀티모달 웹 에이전트를 위한 새로운 벤치마크 MMSearch-Plus를 제안하는 것이다.
- Motivation: 기존 멀티모달 웹 벤치마크는 이미지 검색에 과도하게 의존하여 진정한 시각 추론과 출처 검증, 긴 탐색 과정이 요구되는 복잡한 과제들을 충분히 반영하지 못하였다.
- Contribution: 공간-시간 외삽 기법을 통해 지역적 시각 단서와 시간적 흔적을 종합하여 이미지 외 정보를 유추하는 311개 과제를 구축하고, 다양한 대규모 멀티모달 언어 모델을 평가할 수 있는 통합적 탐색 에이전트 프레임워크를 개발하였다.

## Method

MMSearch-Plus는 미세한, 다중 약한 시각 신호들을 반복적인 이미지·텍스트 검색과 교차 검증을 통해 추론해야 하는 공간-시간 외삽 문제들을 포함한다.  
데이터는 유튜브, 빌리빌리, 아카이브 등 공개 웹 소스에서 실제 이벤트 중심으로 수집되었고, 해답 추론에 필요한 핵심 시각 단서가 의도적으로 모호하거나 희소하도록 선별·가공되었다.  
벤치마크는 검색 에이전트 기반의 평가 체계로 수행되며, 이미지 검색 결과 캐싱과 웹페이지 요약 등의 최적화를 포함한다.

## Results

가장 성능이 우수한 폐쇄형 MLLM인 o3 모델은 검색 도구 없이 15.1% 정확도를, 최대 20회 검색을 수행하는 전체 롤아웃 시 36.0% 정확도를 기록하였으며, 공개형 최강 모델 Qwen-2.5-VL은 0.0%에서 6.9%로 성능 향상을 보였다.

## Limitations

본 벤치마크는 공개 웹 기반 데이터 편향 및 비영어 또는 시각적 밀도 낮은 페이지의 불균형 문제와 영상 또는 동적 인터페이스 활용 부족 등의 한계를 갖는다.

## Conclusion

MMSearch-Plus는 미세한 시각 신호 기반 탐색과 출처 검증이 요구되는 멀티모달 웹 에이전트 연구를 위한 엄격한 평가 지표를 제공하며, 현재 MLLM들이 복잡한 다중모달 추론과 장기적 도구 활용에서 여전히 해결해야 할 과제가 많음을 입증하였다.

# 16. [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](http://arxiv.org/abs/2508.21468v1)

## Introduction

- 구조 기반 신약 설계(SBDD)에서 표적 단백질에 결합하는 3D 분자 생성을 위한 생성모델 개발이 목표이다.
- 기존 확산 기반 모델들이 결합 친화도 위주 평가에 치중해 합성 가능성과 선택성 같은 중요한 약리학적 특성을 간과하는 한계가 있다.
- 이에, 베이지안 플로우 네트워크(BFN)를 확장하고 그래디언트 기반 조건 생성 기법을 도입한 CBYG 프레임워크와 실용적 평가 지표를 제안하였다.

## Method

제안하는 CBYG는 베이지안 업데이트를 통해 계속 파라미터를 갱신하며, 외부 예측기와 연동하여 목표 특성에 대한 그래디언트 지도를 통합한다. 이 방법은 연속 변수와 범주형 변수를 모두 파라미터 공간 내에서 안정적으로 처리하며, 확산 모델 대비 화학적 유효성과 특성 제어의 안정성을 높인다. 또한, 합성 난이도와 선택성 등 현실적 제약을 반영한 평가체계를 함께 도입하였다.

## Results

CBYG는 AutoDock Vina, SMINA, GNINA 등 다양한 도킹 도구와 신규 평가 지표를 활용한 실험에서 기존 모델 대비 전반적인 결합 친화도, 합성 가능성, 선택성 지표에서 우수한 성능을 보였다.

## Limitations

고도화된 모델임에도 불구하고 생성된 분자의 약 절반은 실제 합성에 어려움이 있어, 합성 난이도 평가의 정교화와 향상 필요성이 남아 있다.

## Conclusion

본 연구는 BFN 기반의 그래디언트 지도 통합 기법을 통해 실용적이며 통제 가능한 3D 분자 생성 모델을 제시하여 SBDD 분야에서 실질적 약물 후보 발굴에 기여하는 성과를 달성하였다.

# 17. [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](http://arxiv.org/abs/2508.21451v1)

## Introduction

- Goal: 본 논문은 경량 이미지 캡셔닝 모델을 개발하여 시각 정보 해석을 효율적으로 수행하는 실용적인 시각 전문가를 구현하는 것을 목표로 한다.
- Motivation: 기존의 대규모 멀티모달 언어 모델(MLLM)은 높은 연산량 때문에 엣지 디바이스에서의 실시간 적용이 어려워, 경량화된 캡셔닝 모델이 필요하다.
- Contribution: 125M 파라미터의 경량 언어 모델을 기반으로 한 캡셔닝 전문가를 제안하고, 시각적 집착 문제를 개선하는 Sharp-Eyed Refinement 프레임워크와 DeepLens 모듈을 도입하였다.

## Method

경량 모델은 OPT-125M으로 LLaVA-7B의 56배 작은 모델을 사용하여 캡션을 생성하는 방식을 채택하였다.  
Sharp-Eyed Refinement는 인간의 두 번째 자세한 응시 과정을 모방하여 초기 캡션을 생성 후 DeepLens를 활용해 시각적 근거를 강화하며 세밀하게 수정한다.  
DeepLens는 초기 캡션과 비전 인코더의 다중 레이어 특징을 입력으로 받아 중요한 시각 정보를 추출하며, 2단계 파인튜닝을 통해 초기 생성과 수정 과정을 학습한다.

## Results

경량 모델은 MS COCO 등 다양한 데이터셋에서 기존 소규모 모델을 능가하며, LLaVA-7B와 유사한 성능을 보였고, Sharp-Eyed Refinement 적용 시 CIDEr 점수가 최대 3.9 상승하였다.

## Limitations

경량 모델과 기존 MLLM과 마찬가지로 시각적 집착 문제로 인해 일부 의미 오류가 발생하는 한계가 존재한다.

## Conclusion

본 연구는 경량 캡셔닝 모델과 세밀한 재검토 메커니즘을 통해 엣지 디바이스 상에서 실시간으로 적용 가능한 고성능 시각 이해 시스템 개발 가능성을 입증하였다.

# 18. [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](http://arxiv.org/abs/2508.21435v1)

## Introduction

- 본 연구의 목표는 합성 두부 X선 영상과 실제 임상 영상 간의 도메인 적응을 위하여 다중 도메인 간 고화질 비쌍대 이미지 변환을 수행하는 통합 클래스 조건부 생성 모델 MedShift를 제안하는 것이다.
- 합성 의료 데이터는 강건한 모델 학습을 위한 확장 가능한 솔루션이나, 도메인 간 차이로 인해 실제 임상 환경에서의 일반화 성능이 제한되는 문제가 존재한다.
- 본 논문은 흐름 매칭과 슈뢰딩거 브리지를 기반으로 하는 MedShift 모델, X-DigiSkull 데이터셋의 공개 및 여러 최신 생성 모델과의 포괄적 성능 비교를 주요 기여로 삼았다.

## Method

MedShift는 사전에 학습된 VAE를 활용하여 도메인에 무관한 잠재 공간을 학습하고, 조건부 흐름 매칭 방법으로 복수 도메인 간 변환을 수행한다.  
추론 시에는 입력 이미지를 잠재 공간으로 인코딩한 뒤, 중간 시간 파라미터 τ에서 도메인 간 매핑을 적용하여 이미지의 해부학적 구조를 보존하며 도메인 스타일을 변환한다.  
이 과정은 짝지어진 데이터 없이 단일 모델로 여러 도메인 쌍 간의 변환을 가능하게 하며, CFG로 조건부 스코어 추정도 지원한다.

## Results

MedShift는 CycleGAN-Turbo, Z-STAR, SDEdit 등 최신 기법과 비교하여 perceptual fidelity와 구조적 일관성 간 최적의 균형을 보이며, 모델 크기가 작고 계산 효율이 뛰어남을 입증하였다.

## Limitations

현재 MedShift의 생성 과정은 다단계 샘플링을 필요로 하여 확산 모델보다 추론 속도가 느린 점이 존재한다.

## Conclusion

MedShift는 비쌍대 고해상도 X선 영상 도메인 변환에 있어 도메인 불변 잠재 공간 학습과 다중 도메인 간 조건부 흐름 매칭을 성공적으로 통합한 효율적이고 확장 가능한 생성 모델임이 확인되었다.

# 19. [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](http://arxiv.org/abs/2508.21430v1)

## Introduction

- 본 연구의 목표는 의료 다중모달 대형언어모델(MMLMs)의 보상모델과 평가자를 평가하기 위한 최초의 벤치마크인 Med-RewardBench를 제안하는 것이다.
- 의료 분야에서는 높은 정확성, 문맥 민감성, 전문적 정합성이 요구되나 기존 보상모델과 평가자에 대한 연구 및 임상 기준을 반영한 평가체계가 부족하였다.
- Med-RewardBench는 13개 장기계통과 8개 임상과를 아우르는 1,026건의 전문가 주석 데이터셋과 6가지 임상 중요 차원을 통해 보상모델 및 평가자의 성능을 종합적으로 평가하는 방식을 도입하였다.

## Method

- 다섯 개 공개 의료 데이터에서 이미지-질문 쌍을 수집하여 다섯 개 소형 MLLM으로 난이도 필터링 후 임상 전문가가 고품질 질문을 선정하였다.
- 이후 12개 최첨단 MLLM이 각 쌍에 답변을 생성하고 두 응답군에서 무작위로 추출하여 쌍별 비교 데이터셋을 구성하였다.
- 세 명의 전문의가 6개 차원(정확성, 관련성, 포괄성, 창의성, 반응성, 종합평가)에서 응답 우월성을 기록하며 일관성을 검증하였다.

## Results

- 32개 MLLM 평가 결과, 상용 및 대규모 모델이 높은 성능(최고 68.86%)을 보인 반면, 의료 특화 모델은 54~55% 수준에 머무르며 의료 전문가 수준 판단과의 정합에 한계가 확인되었다.

## Limitations

- 본 연구에서는 Qwen2-VL 모델을 기반으로 SFT와 DPO 기법만 적용하였으며, 향후 다른 모델과 다양한 학습전략 탐색이 필요하다.

## Conclusion

- Med-RewardBench는 의료 다중모달 보상모델 및 평가자의 신뢰성 향상과 임상 정합성 평가를 위한 기반을 마련하여 차세대 의료용 MLLM 개발에 기여한다.

# 20. [AHELM: A Holistic Evaluation of Audio-Language Models](http://arxiv.org/abs/2508.21376v1)

## Introduction

- Goal: 본 논문은 오디오-언어 모델(Audio-Language Models, ALMs)의 성능을 전반적으로 평가하기 위한 종합 벤치마크인 AHELM을 제안하는 것이다.
- Motivation: 기존 평가 벤치마크들이 일부 능력만을 측정하고 공정성, 안전성 등 중요한 평가 요소를 배제하며, 모델 간 비교가 어렵다는 한계가 존재하였다.
- Contribution: AHELM은 10가지 핵심 평가 항목을 정의하고, 14개 기존 데이터셋과 두 개의 신규 합성 데이터셋을 통합하여 표준화된 평가 방식을 제시하였다.

## Method

AHELM은 오디오 인식, 지식, 추론, 감정 인식, 편향, 공정성, 다중언어성, 견고성, 유해성, 안전성 등 10가지 측면을 포괄적으로 평가한다.  
평가는 16개 모델과 간단한 ASR 기반 baseline 시스템들을 대상으로 동일한 zero-shot 프롬프트와 설정으로 수행하였다.  
평가지표로는 단어 오류율, 정확도, BLEU 점수 및 GPT-4o를 이용한 자동 평가 등을 사용하며, 공정성 검증을 위한 통계적 검정도 포함되었다.

## Results

Gemini 2.5 Pro 모델이 다섯 가지 평가 측면에서 우수한 성능을 보였으나, 자동 음성 인식 작업에서 집단간 불공정성이 관찰되었고, baseline 시스템들도 여러 측면에서 경쟁력 있는 결과를 나타내었다.

## Limitations

본 연구는 다양한 모델과 데이터셋을 아우르나, 일부 평가 시나리오에 한정되어 있으며, 지속적인 데이터셋 및 모델 추가가 필요하다.

## Conclusion

AHELM은 ALMs의 다면적 성능을 균등하게 비교할 수 있는 투명하고 확장 가능한 표준 평가 체계를 제시하여 이 분야 연구 발전에 기여한다.
