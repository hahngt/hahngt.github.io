---
layout: post
title: Daily Papers — 2025-09-10"
date: 2025-09-10 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)

## Introduction
- 본 연구의 목표는 강화학습을 통해 대규모 언어모델에 병렬 사고(parallel thinking) 능력을 효과적으로 습득시키는 것이다.  
- 기존의 감독학습 기반 미세조정 방식은 탐색과 일반화보다는 단순 모방에 치중하여 병렬 사고 활성화에 한계가 존재한다는 문제점에서 동기를 얻었다.  
- 본 논문은 점진적 커리큘럼과 보상 설계를 결합한 최초의 강화학습 프레임워크 Parallel-R1을 제안하여 실제 복잡한 수학 추론 문제에 병렬 사고를 성공적으로 도입하였다.  

## Method  
병렬 사고 행동은 중요한 추론 단계에서 다수의 추론 경로를 병렬로 탐색한 후 결과를 요약하는 탐색-요약 방식으로 정의하였다.  
단순한 수학 문제에서 감독학습으로 병렬 사고 형식을 학습하는 초기 단계 이후, 보다 어려운 문제에서 강화학습으로 병렬 사고를 탐색하고 일반화하는 점진적 커리큘럼을 설계하였다.  
또한, 순차적 아키텍처와 추론 경로를 격리하는 구조적 아키텍처를 모두 이용하고, 최종 정답 정확도와 병렬 사고 사용을 번갈아 보상하는 보상 설계 전략을 도입하였다.  

## Results  
Parallel-R1은 AMC23, MATH, AIME25 등 다양한 수학 벤치마크에서 기존 순차적 모델 대비 최대 8.4% 정확도 개선을 달성하였으며, 특히 중간 훈련 시 병렬 사고 탐색 단계를 도입하여 AIME25에서 기준선 대비 42.9% 성능 상승을 보였다.  

## Limitations  
구조적 아키텍처를 적용한 모델은 병렬 사고 일반화가 어려워 훈련 효과가 저하되는 등 아키텍처 설계에 대한 제약과 최적화 난제가 존재한다.  

## Conclusion  
본 연구는 강화학습과 점진적 커리큘럼을 바탕으로 대규모 언어모델에 병렬 사고 능력을 성공적으로 학습시키고, 그 전략적 진화와 효과를 체계적으로 분석하였다.

# 2. [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/abs/2509.07979)

## Introduction
- Goal: 본 연구는 멀티모달 대형 언어 모델(MLLM)의 시각적 표현 정렬을 통해 세밀한 시각 정보를 보존하고 향상시키는 것을 목표로 한다.  
- Motivation: 기존 MLLM은 텍스트 중심 지도 학습으로 인해 시각 경로에 대한 직접적 감독이 부족하여, 객체 수 세기 및 공간 추론과 같은 시각 중심 과제에서 성능 한계가 존재한다.  
- Contribution: 본 논문에서는 MLLM 내부 시각 표현을 사전학습된 비전 파운데이션 모델(VFM)의 표현과 정렬하는 VIRAL 정규화 전략을 제안하여, 세밀한 시각 세부정보 유지 및 복합 시각 추론 능력을 향상시켰다.  

## Method  
VIRAL은 MLLM의 중간 시각 표현을 강력한 VFM의 시각 특성과 코사인 유사도를 이용한 정렬 손실로 직접 맞추는 방식을 채택한다.  
이 방법은 텍스트 중심 손실에 시각 표현 정렬 손실을 추가하여 내부 시각 경로가 원본 비전 인코더의 풍부한 시각 정보를 잃지 않도록 한다.  
다양한 VFM을 교사 모델로 활용하며, 중간 계층(특히 16번째 층)을 중심으로 정렬하여 최적의 성능을 도출한다.  

## Results  
DINOv2와 같은 강력한 VFM을 활용한 VIRAL은 CLIP 및 SigLIPv2 인코더 기반 MLLM에서 시각 중심, 환각 감지, 일반 멀티모달 과제 전반에 걸쳐 일관된 성능 향상을 기록하였다.  

## Limitations  
원천 비전 인코더의 제한적인 표현 능력이나 특정 벤치마크에서는 표현 정렬이 오히려 성능 저하를 초래할 수 있다는 점이 일부 확인되었다.  

## Conclusion  
본 연구는 내부 시각 표현의 명시적 정렬을 통해 멀티모달 언어 모델의 시각 정보 활용능력을 크게 개선할 수 있으며, 이는 시각 정보 통합 효과적 학습의 중요한 방향임을 시사한다.

# 3. [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual   Search](https://arxiv.org/abs/2509.07969)

## Introduction  
- Goal: 본 연구의 목표는 이미지 기반 도구와 강화학습을 활용하여 다중 단계의 깊이 있는 시각 탐색 문제 해결 능력을 확장하는 것이다.  
- Motivation: 기존 공개 소스 모델들은 단조로운 추론 패턴과 제한된 상호작용 횟수로 인해 복잡한 시도-오류 탐색이 필요한 어려운 시각 문제에서 낮은 성능을 보였다.  
- Contribution: 본 연구에서는 미니-o3(Mini-o3)를 제안하며, 독창적인 Visual Probe 데이터셋, 다양한 추론 패턴을 유도하는 반복적 데이터 수집 파이프라인, 그리고 오버턴 마스킹 기법을 포함하는 학습 전략을 제시한다.  

## Method  
Mini-o3는 수십 단계에 걸친 다중 상호작용 추론을 지원하는 시스템으로, Visual Probe 데이터셋을 통해 도전적인 시각 문제를 학습하였다.  
콜드 스타트 단계에서 다양한 추론 경로를 생성하여 초기 지도학습을 진행하고, 이후 강화학습 과정에서는 오버턴 마스킹 기법을 도입해 최대 상호작용 횟수를 초과하는 경로에 대한 부정적 제재를 예방하였다.  
이로써 훈련 중 제한된 상호작용 횟수에도 불구하고 테스트 시에는 수십 회까지 상호작용을 확장하며 성능을 향상시킬 수 있었다.  

## Results  
Mini-o3는 Visual Probe, V* Bench, HR-Bench 등 여러 시각 탐색 벤치마크에서 7B 규모 공개 소스 모델들을 크게 능가하는 최첨단 성능을 기록하였다.  

## Limitations  
훈련 시 상호작용 횟수는 최대 6회로 제한되어 있으며, 이로 인해 초기 수렴 속도와 성능 향상을 위한 추가적인 상호작용 확대가 필요하다.  

## Conclusion  
Mini-o3는 다중 상호작용 이미지 도구 사용과 강화학습 기반 혁신적 학습 기법을 통해 복잡한 시각 탐색 문제에서 뛰어난 추론 다양성과 깊이를 달성함으로써 시각-언어 모델의 한계를 확장하였다.

# 4. [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)

## Introduction
- Goal: 본 논문은 통합 멀티모달 모델(Unified Multimodal Models, UMMs)의 시각 이해와 생성 능력을 향상시키기 위한 새로운 후학습 방법을 제안하는 것을 목표로 한다.  
- Motivation: 기존 UMM 학습은 이미지-텍스트 짝에 의존하며, 텍스트 캡션이 시각적 세부 정보를 충분히 포착하지 못해 이해와 생성 간 불일치 문제를 발생시킨다.  
- Contribution: 본 연구는 시각 이해 인코더의 임베딩을 활용하여 텍스트 대신 풍부한 의미론적 '밀집 프롬프트'로 활용하는 자기지도식 이미지 재구성 손실 기반의 후학습 기법 RecA를 제안한다.  

## Method  
RecA는 UMM이 자신의 시각 이해 인코더 임베딩을 조건으로 입력 이미지 재생성을 학습하도록 하여 이해와 생성 간 정렬을 강화한다. 기존 텍스트-이미지 쌍을 사용하는 방식 대신, 의미론적으로 풍부한 시각 임베딩을 통한 자기지도식 재구성 손실을 적용한다. 이 방법은 캡션 없이도 시각적 세부사항을 효과적으로 지도하며, 다양한 UMM 아키텍처에 범용적으로 적용 가능하다.  

## Results  
RecA를 27 GPU-시간 후학습한 15억 파라미터 모델은 GenEval, DPGBench 및 이미지 편집 벤치마크에서 GPT-4o 및 더 큰 모델들을 능가하는 최첨단 성능(GenEval 0.90, DPGBench 88.15, ImgEdit 3.75, GEdit 7.25)을 달성하였다.  

## Limitations  
정보 부족.  

## Conclusion  
RecA는 자가시각이해 임베딩을 활용한 간단하면서도 강력한 후학습 전략으로서, UMM의 이해-생성 정렬과 시각 생성 품질을 효율적이고 일반적으로 향상시킨다.

# 5. [UMO: Scaling Multi-Identity Consistency for Image Customization via   Matching Reward](https://arxiv.org/abs/2509.06818)

## Introduction
- Goal: 본 논문은 다양한 참고 이미지에서 다중 인물의 정체성을 일관되게 유지하며 이미지 커스터마이징의 정체성 확장성을 높이는 방법을 제안하는 것이다.  
- Motivation: 기존의 1대1 매핑 기반 이미지 커스터마이징 기법들은 다중 인물 정체성 간의 혼동을 완화하지 못해 대규모 다중 인물 시나리오에서 정체성 보존 성능이 저하된다.  
- Contribution: 다중 인물 간 최적 글로벌 매칭 문제로 재구성한 다대다 정체성 매칭 패러다임과 이를 위한 강화학습 기반의 통합 다중 정체성 최적화(UMO) 프레임워크를 제안하였다.  

## Method  
UMO는 각 생성된 정체성을 참조 이미지와 최적으로 매칭시키는 문제를 이분 그래프의 할당 문제로 정의하고 헝가리안 알고리즘으로 해결한다.  
이와 함께 단일 정체성의 임베딩 코사인 유사도를 활용한 단일 정체성 보상(SIR)과 이를 다중 정체성에 확장한 다중 정체성 매칭 보상(MIMR)을 제안하였다.  
ReReFL이라는 강화학습 학습법으로 기존 확산모델 기반 커스터마이징 모델에 통합하여 학습한다.  

## Results  
XVerseBench와 OmniContext에서 UMO는 기존 최신 커스터마이징 기법 대비 ID-Sim(정체성 유사도)과 ID-Conf(정체성 혼동도) 지표에서 크게 향상된 성능을 보여 다중 인물 정체성 일관성 유지 및 혼동 완화에 우수함을 입증하였다.  

## Limitations  
제안 방법은 다중 정체성 데이터셋 구축과 강화학습 과정에서 계산 비용과 학습 복잡성이 증가하는 한계가 존재한다.  

## Conclusion  
UMO는 다대다 매칭 보상과 강화학습 기반 최적화로 다중 인물 정체성 일관성을 크게 향상시키며, 여러 커스터마이징 모델에 적용 가능한 범용성 높은 새로운 최첨단 기법임을 증명하였다.

# 6. [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)

## Introduction
- Goal: 본 연구의 목표는 추가적인 데이터 없이도 대규모 언어모델이 자기 자신의 상호작용을 통해 성능을 향상시키는 강화학습 방법을 제안하는 것이다.  
- Motivation: 기존 대규모 언어모델 학습은 방대한 고품질 데이터에 의존하며, 데이터 부족이 학습의 근본적인 병목 현상이었다.  
- Contribution: 본 논문은 언어모델이 스스로 경쟁하는 게임 이론적 자기대전(self-play) 프레임워크를 도입하여, 데이터 없이도 지속적으로 성능을 개선하는 Language Self-Play (LSP) 알고리즘을 제안하였다.  

## Method  
제안하는 방법에서는 단일 언어모델이 Challenger와 Solver 역할을 번갈아 수행하며, Challenger는 점차 어려운 쿼리를 생성하고 Solver는 이에 답변을 학습하는 미니맥스 게임으로 학습된다.  
주요 강화학습 보상은 Solver의 성능 극대화 및 Challenger가 Solver의 약점을 탐색하도록 설계되었으며, KL-발산 정규화를 통해 의미 없는 적대적 입력 생성을 방지하였다.  
또한, 모델이 자기 자신이 생성한 데이터에서 얻은 자기보상(self-reward)을 활용하여 안정적인 무한 자기대전을 가능하게 하였다.  

## Results  
AlpacaEval 벤치마크에서 데이터 없이 LSP를 통해 학습한 모델은 기존 데이터 기반 강화학습 방식과 대등하거나 더 우수한 성능을 보였으며, 특히 대화형 데이터셋인 Vicuna에서 현저한 성능 향상이 관찰되었다.  

## Limitations  
실험 결과 LSP가 일부 사용자 유형의 쿼리(예: Koala 데이터셋)에서는 성능 저하를 초래하기도 하여, 자기대전 과정에서 보다 다양하고 균형 잡힌 쿼리 생성이 필요하다.  

## Conclusion  
본 연구는 데이터 의존성을 제거한 자기대전 강화학습 프레임워크 LSP를 통해 대규모 언어모델의 자가 개선 가능성을 제시하며, 향후 실제 세계 경험 수집을 통한 지속적 성능 향상 가능성을 내포한다.

# 7. [F1: A Vision-Language-Action Model Bridging Understanding and Generation   to Actions](https://arxiv.org/abs/2509.06951)

## Introduction
- Goal: 본 논문은 동적 시각 환경에서 언어 조건화된 과제 수행을 위한 비전-언어-액션 모델인 F1을 제안하는 데 목적이 있다.  
- Motivation: 기존 VLA 모델들은 반응적 상태-행동 매핑에 의존하여 장기 계획이 어려우며, 동적 환경에서 견고성과 일반화 능력이 부족하다.  
- Contribution: F1은 시각적 예측을 의사결정에 통합하는 Mixture-of-Transformer 아키텍처를 도입하고, 3단계 점진적 학습 방식을 통해 장기 예측 기반의 역동작 모델을 실현하였다.  

## Method  
F1은 이해, 예측, 제어의 역할을 가진 3개의 전용 전문가 집단으로 구성되며, 언어 지시와 현재 관찰을 바탕으로 목표 조건화된 시각 예측을 생성한다. 이 예측 이미지를 활용해 미래 상태를 목표로 하는 역동작 문제로 행동 생성 문제를 재정의하며, 이는 예측 기반의 계획적 행동을 가능하게 한다. 학습은 (1) 이해-생성 전문가 정렬, (2) 대규모 로봇 데이터로의 사전학습, (3) 세부 작업 적응 단계로 구성된다.  

## Results  
F1은 실세계 복수 플랫폼 9개 작업과 LIBERO, SimplerEnv Bridge 시뮬레이션 벤치마크에서 기존 VLA 모델 대비 평균 성공률과 일반화 능력이 현저히 우수함을 보였다.  

## Limitations  
본 논문에는 F1의 계산 비용 최적화 및 다양한 복잡한 환경에서의 한계에 대한 상세 분석은 부족하였다.  

## Conclusion  
F1은 시각 예측을 명시적 계획 목표로 활용하여 이해, 생성, 행동을 유기적으로 결합함으로써 동적·장기 과제에서 견고하고 일반화 가능한 비전-언어-액션 정책을 성공적으로 제시하였다.

# 8. [Staying in the Sweet Spot: Responsive Reasoning Evolution via   Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)

## Introduction
- Goal: 이 논문은 강화학습에서 문제 난이도와 모델 능력에 적응하는 힌트 스캐폴딩 기법을 통해 대형 언어모델의 추론 능력 향상을 목적으로 한다.  
- Motivation: 기존 RLVR 기법은 문제 난이도와 모델 능력 간 부조화로 탐색 효율성이 저하되는 한계가 존재한다.  
- Contribution: 본 연구는 문제 난이도를 실시간으로 조절하여 학습 효율성을 최적화하는 SEELE 프레임워크를 제안한다.  

## Method  
SEELE는 각 문제에 적응형 길이의 힌트를 부여하여 문제 난이도를 50%의 정답률에 맞춰 조절하고, 이를 위해 다중 라운드 롤아웃과 문항반응이론(Item Response Theory) 기반 정확도-힌트 예측 모델을 활용한다. 이 과정에서 모델의 실시간 피드백을 반영하여 힌트 길이를 조절함으로써 문제 난이도와 모델 역량 간 정합성을 높인다. 학습 손실 함수에 강화학습 손실과 힌트에 대한 모방 학습 손실을 결합하였다.  

## Results  
SEELE는 6개의 수학 추론 및 3개의 일반 추론 벤치마크에서 기존 RLVR 기법 대비 평균 +11.8점의 성능 향상을 보이며, 특히 GRPO 및 SFT와 비교 시 우수한 결과를 나타냈다.  

## Limitations  
제안된 방법의 성능은 롤아웃 라운드 수와 힌트 조절 정확도에 일부 민감한 특성을 보인다.  

## Conclusion  
본 연구는 문제 난이도에 따른 학습 효율성 이론을 제시하고, 능력 적응형 힌트 조절을 통한 강화학습 기반 추론 능력 향상 방안을 새롭게 제안하여 학습 데이터 효율성을 크게 개선하였다.

# 9. [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830)

## Introduction
- Goal: 본 연구는 방사선학 분야에 적용 가능한 다중 모달 기초 모델 Curia를 개발하고 평가하는 데 목적이 있다.  
- Motivation: 기존의 단일 작업 전용 AI 모델들은 다양한 영상 모달리티와 질병을 포괄하기에는 한계가 있으며, 이를 극복할 수 있는 범용 기초 모델의 필요성이 대두되었다.  
- Contribution: 대규모 임상 횡단 영상 데이터(150,000건, 130TB)를 활용해 자가 지도 학습으로 Curia를 사전학습하고, 19개 의료 영상 과제에서 방사선과 레지던트 및 기존 모델들을 능가하는 성능을 보임으로써 광범위한 일반화 능력을 입증하였다.  

## Method  
- Curia는 대규모 병원 영상 데이터(CT 및 MRI 2억여 장)를 DINOv2 자가 지도 학습 알고리즘과 비전 트랜스포머(ViT-B, ViT-L) 아키텍처를 통해 사전학습하였다.  
- 이후 19개 과제(CuriaBench)에서 경량 분류기를 Curia의 고정된 특징 추출기 위에 학습시켜 평가하였다.  
- 벤치마크에는 2D/3D 영상 분류, 회귀, 세그멘테이션, 생존 예측 등 다양한 의학적 진단 테스크와 크로스 모달, 소수 샷 학습 환경도 포함되었다.  

## Results  
Curia는 다중 영상 모달리티에서 장기 인식, 질병 검출(뇌출혈, 심근경색 등), 종양 악성도 분류 및 생존율 예측 등 다양한 과제에서 기존 방사선학 레지던트와 최신 기초 모델들을 능가하는 우수한 성능을 보였으며, 특히 적은 라벨 데이터 및 크로스 모달 환경에서도 뛰어난 일반화 능력을 입증하였다.  

## Limitations  
정보 부족.  

## Conclusion  
Curia는 방대한 임상 횡단 영상 데이터를 활용한 자가 지도 학습 기반의 다중모달 기초 모델로서, 다양한 임상 영상 진단 과제에서 높은 정확도와 범용성을 보여 방사선학 AI 도구의 발전에 중요한 이정표가 된다.

# 10. [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)

## Introduction
- Goal: 본 연구는 강화학습을 통해 대형 언어 모델(LLMs)에서 인간과 유사한 계층적 추론 능력이 자발적으로 형성되는 메커니즘을 규명하는 것을 목표로 한다.  
- Motivation: 기존 강화학습 기법들이 복잡한 추론 능력 향상에 효과적이나 그 성공 원리에 대한 이해가 부족하여 학습 동적 과정을 명확히 해석할 필요가 있다.  
- Contribution: 본 연구는 추론 과정이 저수준 절차적 실행 숙련 단계에서 고수준 전략적 계획 탐색 단계로 학습 부담이 전환되는 두 단계 동역학을 발견하고, 이를 반영한 계층 인지 신용 할당(HICRA) 알고리즘을 제안하여 성능을 크게 개선하였다.  

## Method  
저수준 실행 토큰과 고수준 전략 계획 토큰을 구분하는 전략그램(Strategic Grams) 기법을 도입하였으며, RL 훈련 중 전략 계획 토큰에 최적화 집중을 강화하는 HICRA 알고리즘을 개발하였다. HICRA는 GRPO 기반 정책 그래디언트에서 전략 계획 토큰에 대해 이득 가중치를 증폭하여 학습 신호를 선택적으로 강화한다. 이를 통해 계획 토큰의 의미론적 다양성 증가와 전략 탐색 효과를 촉진하였다.  

## Results  
HICRA는 다양한 공개 LLM 및 멀티모달 모델에서 GRPO 및 기타 강력한 베이스라인 대비 수학 및 복합 추론 벤치마크에서 일관되게 우수한 정확도를 기록하였다.  

## Limitations  
HICRA는 저수준 절차적 실행 능력이 충분히 갖춰진 모델에 의존하며, 기반 실행 기술이 부실한 경우 이득이 제한적이고 학습이 불안정해지는 한계가 있다.  

## Conclusion  
강화학습 과정에서 고수준 전략 계획 학습이 추론 능력 향상의 핵심 병목이며, 이를 목적에 맞게 집중 최적화하는 계층 인지 신용 할당 기법이 효과적인 해결책임이 입증되었다.

# 11. [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)

## Introduction
- Goal: 본 연구의 목표는 표준 인과(attention) 메커니즘의 한계를 극복하여 키(key)를 지속적으로 갱신하는 새로운 인과 어텐션 메커니즘인 CASTLE를 제안하는 것이다.  
- Motivation: 기존의 인과 어텐션은 각 토큰 키가 고정되어 이후 토큰의 정보를 반영하지 못해 전역 문맥 포착 능력과 자연어 이해에 제한이 존재한다.  
- Contribution: CASTLE는 각 토큰의 키를 문맥 전개에 따라 업데이트하여 미래 정보를 부분적으로 интегレ이트하면서도 자기회귀성을 엄격히 유지하고, 효율적인 병렬 학습 알고리즘을 제시한다.  

## Method  
CASTLE는 각 위치의 키를 두 부분, 즉 고정된 인과 키와 이후 토큰 정보를 포함하는 가변적 룩어헤드 키로 분리하여 정의한다. 룩어헤드 키는 시그모이드 기반의 가중합을 사용해 미래 토큰 정보를 선택적으로 통합하며, 이로 인해 토큰 간의 더 풍부한 상호작용이 가능하다. 제안된 수학적 동형성에 따라 룩어헤드 키를 명시적으로 생성할 필요 없이 효율적인 병렬 학습 및 추론이 가능하다.  

## Results  
50억 토큰 학습 후 다양한 크기의 모델에서 CASTLE은 표준 인과 어텐션 대비 검증 손실과 당혹도(perplexity)를 낮추고, ARC, BoolQ, MMLU 등 여러 다운스트림 작업에서 일관되게 우월한 성능을 보였다.  

## Limitations  
제안 메커니즘은 키만 갱신하는 데 초점을 맞추었으며, 값(value)을 갱신하는 효율적 병렬 학습 기법은 향후 연구 대상이다.  

## Conclusion  
CASTLE는 문맥에 따라 키를 동적으로 갱신하여 자기회귀성을 유지하면서도 성능을 향상시키는 인과 어텐션 메커니즘임을 실험적으로 입증하였다.

# 12. [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric   Knowledge](https://arxiv.org/abs/2509.07968)

## Introduction
- Goal: 본 연구는 대형 언어 모델의 매개변수 기반 사실성(parametric factuality)을 평가하기 위한 신뢰도 높은 벤치마크인 SimpleQA Verified를 제안하는 데 목적이 있다.  
- Motivation: 기존 OpenAI의 SimpleQA 벤치마크는 라벨 오류, 주제 편향, 질문 중복 등의 한계로 신뢰성 있는 사실성 평가에 어려움이 존재하였다.  
- Contribution: 다단계 필터링을 통한 데이터 정제와 자동평가기 개선을 통해 1,000개의 엄선된 평가 질문집을 구축하고, Gemini 2.5 Pro가 최첨단 성능을 달성함을 입증하였다.  

## Method  
SimpleQA Verified는 중복 문서 제거, 의미론적 및 TF-IDF 기반 중복 질문 배제, 웹 발행자의 크롤링 정책 준수, 주제 및 답변 유형 균형 재조정, 충돌 출처 재검증 등 엄격한 절차로 원본 SimpleQA 데이터를 정제하였다. 또한, 자동평가기의 평가 가이드라인을 보완하여 숫자 답변의 허용 오차 범위 명시, 과다 응답 무시 및 헤징 응답 판정 기준 명확화를 적용하였다. 이런 종합적 데이터와 평가 방식의 개선을 통해 벤치마크의 신뢰성과 난이도를 동시에 확보하였다.  

## Results  
SimpleQA Verified에서 Gemini 2.5 Pro는 55.6의 F1-스코어를 기록하며 GPT-5 등 최첨단 모델들을 능가하는 사실성 평가 성과를 보였다.  

## Limitations  
본 연구는 매개변수 기반 사실성 평가지표에 초점을 맞추어 외부 정보 탐색이나 장문 생성의 정확성 평가는 포함하지 않았다.  

## Conclusion  
SimpleQA Verified는 기존의 한계를 극복한 고품질 벤치마크로서 LLM 사실성 연구 커뮤니티에 신뢰할 수 있는 평가 도구를 제공하여 미래 AI 신뢰성 향상에 기여한다.

# 13. [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human   Preference](https://arxiv.org/abs/2509.06942)

## Introduction
- Goal: 본 연구는 텍스트-투-이미지 확산 모델을 세밀한 인간 선호에 직접적으로 정렬시키는 새로운 강화학습 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 기존 방법들은 다중 단계 디노이징 과정에서 발생하는 계산 비용과 보상 모델의 오프라인 조정 필요성 때문에 제한을 받으며, 이로 인해 후기 확산 단계에만 최적화가 가능하여 보상 해킹 문제가 빈번했다.  
- Contribution: 본 연구는 Direct-Align과 Semantic Relative Preference Optimization(SRPO)이라는 두 가지 핵심 기술을 도입하여 초기 및 후기 확산 단계 모두에서 효율적이고 안정적으로 인간 선호에 모델을 정렬하며, 온라인 보상 조정을 가능하게 하여 훈련 효율성과 생성 이미지의 현실감 및 미적 품질을 대폭 향상시켰다.  

## Method  
Direct-Align은 이미지에 미리 정의된 가우시안 노이즈를 주입하여 어느 시점에서도 원본 이미지를 정확히 복원할 수 있도록 하며, 이를 통해 초기 확산 단계에서 효과적인 역전파가 가능하게 한다.  
SRPO는 보상 신호를 텍스트 조건부 신호로 재정의하여 긍정 및 부정 프롬프트를 통한 온라인 보상 조정을 실현하고, 보상 편향을 억제하기 위해 조건부 보상 간 상대적 차이를 목적함수로 활용한다.  
이 두 기법은 함께 사용되어 FLUX.1.dev 모델을 단 10분간의 효율적인 훈련으로 보상 해킹 없이 인간 평가 기반 현실감과 미적 품질을 약 3배 이상 향상시켰다.  

## Results  
제안한 방법은 HPDv2 벤치마크에서 인간 평가 기준 현실감과 미적 품질 측면에서 FLUX.1.dev 대비 각각 약 3.7배 및 3.1배 향상된 성능을 보이며, DanceGRPO 대비 약 75배의 훈련 효율성을 달성하였다.  

## Limitations  
본 방법은 특정 제어 토큰이나 텍스트 제어 메커니즘의 제한 및 표현력 문제로 인해 보상 모델 훈련 데이터에 없는 스타일이나 컨트롤에 대한 반응이 다소 떨어질 수 있다.  

## Conclusion  
본 연구는 대규모 확산 모델의 현실감과 인간 미적 선호 정렬을 효과적으로 개선하는 최초의 강화학습 프레임워크를 제시하며, 효율성과 성능 측면에서 현존 최첨단 기법을 능가한다는 점을 입증하였다.

# 14. [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with   Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)

## Introduction
- Goal: 본 연구는 적은 횟수의 denoising 단계(few-step)를 사용하는 확산 모델에서 양자화 인지 스케줄링을 도입하여 모델 압축과 이미지 품질을 동시에 개선하는 방법을 제안하는 것이다.  
- Motivation: 기존의 텍스트-이미지 확산 모델은 대규모 파라미터와 많은 연산량으로 인해 추론 비용이 매우 높으며, 기존 양자화 기법은 전체 정밀도 보정(calibration)에 의존하여 실제 현장에서 적용에 한계가 있었다.  
- Contribution: Q-Sched라는 새로운 양자화 인지 노이즈 스케줄러를 도입하여 모델 가중치를 변경하지 않고도 샘플링 경로를 최적화하고, 참조 이미지 없이 텍스트-이미지 적합도와 이미지 품질을 동시에 고려하는 JAQ 손실 함수를 활용해 고성능의 압축 모델을 구현하였다.  

## Method  
Q-Sched는 적은 단계의 확산 샘플링에서 두 개의 학습 가능한 사전조건 계수(cx, cϵ)를 도입하여 양자화된 노이즈 예측 값과 입력을 재조정하는 방식이다.  
기존 스케줄러 대신 Q-Sched는 이미지 품질과 텍스트-이미지 정렬을 고려하는 JAQ 손실을 통해 사전조건 계수를 최적화하며, 이는 소량의 캘리브레이션 프롬프트만으로 수행되어 효율적이다.  
이 방법은 U-Net, DiT와 같은 백본 구조에 독립적으로 작용하며, 기존의 few-step 확산 모델에 모듈식으로 적용 가능하다.  

## Results  
Q-Sched는 FLUX.1[schnell], SDXL-Turbo 등의 대규모 few-step 확산 모델에서 기존 PTQD 대비 최대 16.1% FID 개선을 기록하고, 80,000건 이상의 사용자 평가에서도 우수한 이미지 품질 선호도를 보여주었다.  

## Limitations  
Q-Sched는 낮은 비트폭(예: 4비트) 양자화에서 성능이 탁월하지만 일부 높은 비트폭(예: 8비트) 조건에서는 성능이 다소 저하될 수 있다.  

## Conclusion  
Q-Sched는 양자화와 few-step 증류를 결합하여 모델 크기는 4배 감소시키면서도 Full-precision 수준의 이미지 품질을 유지하는 효과적인 모델 압축 기법임이 입증되었다.

# 15. [ΔL Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)

## Introduction
- Goal: 본 논문은 Reinforcement Learning with Verifiable Rewards(RLVR)에서 동적인 응답 길이 특성에 적합한 손실 집계 기법인 ∆L Normalization을 제안하는 것을 목표로 한다.  
- Motivation: RLVR은 대규모 언어 모델의 추론 능력 향상에 기여하지만, 응답 길이의 큰 변동으로 인해 높은 그래디언트 분산과 불안정한 최적화 문제가 존재한다.  
- Contribution: 기존의 길이 의존적 정규화 방법들이 편향을 가지거나 높은 분산 문제를 안고 있음에 착안하여, 편향 없이 최소 분산을 보장하는 ∆L Normalization 방식을 이론적·실험적으로 제시하였다.  

## Method  
본 연구는 샘플별 응답 길이에 비례하여 증가하는 그래디언트 분산 특성을 확인하고, 최소 분산 편향 추정량 문제로 정식화하였다. 제안한 ∆L Normalization은 응답 길이에 대한 α-조절식을 통해 편향이 없으며 분산이 최소가 되는 선형 결합 계수를 도출한다. α=1일 때가 최소 분산 해이며 α 값은 분산과 긴 응답의 활용도 간의 균형을 조절한다.  

## Results  
Qwen2.5-3B 및 7B 모델에 대해 CountDown, Math 등 다양한 태스크 및 최대 응답 길이 환경에서 ∆L Normalization은 기존 GRPO, DAPO, Dr. GRPO 대비 학습 안정성과 정확도 모두에서 일관되게 우수한 성능을 보였다.  

## Limitations  
추가적인 하이퍼파라미터 α 조절과 도메인별 긴 응답 활용성 차이에 따라 최적 설정이 달라질 수 있어 별도의 튜닝이 필요하다.  

## Conclusion  
∆L Normalization은 RLVR 환경의 높은 길이 변동성 문제를 해결하는 무편향·최소분산 손실 집계 방안으로, 이론적 일관성과 실무적 성능 향상 모두를 달성하였다.

# 16. [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)

## Introduction
- 본 연구의 목표는 복합적인 정보 검색 과제를 포함하는 다양한 작업들에 대해 정보 검색 모델의 성능을 종합적으로 평가할 벤치마크를 구축하는 것이다.  
- 대형 언어 모델(LLM)의 등장과 함께 사용자의 검색 요구가 점점 복잡해지고 있으나 기존 검색 모델과 평가 자원은 단일 측면 질의에 치중되어 있어 현실적 복합 검색 과제 대응 능력 평가가 부족하다는 점에 착안하였다.  
- 본 연구는 현실적이고 다면적인 8가지 복합 검색 작업을 통합한 CRUMB 벤치마크를 제안하고, 이를 통해 최신 검색 모델들을 평가하여 현 모델들의 한계와 복합 질의 처리 능력 향상을 위한 연구 방향을 제시한다.  

## Method  
복합 검색 작업을 대표하는 영화, 법률, 과학 논문, 임상 시험, 프로그래밍 코드, 수학 정리 등 총 8개 과제를 하나의 통일된 마크다운 형식으로 통합하였다.  
다양한 도메인과 query 유형별 특징을 유지하면서 문서 단위 및 구간 단위의 평가를 가능하게 하는 문서 전처리 및 청킹(chunking) 방식을 적용하였다.  
또한 LLM 기반 질의 확장 및 재작성 기법이 검색 성능에 미치는 영향을 실험적으로 분석하였다.  

## Results  
최고 성능 모델의 평균 nDCG@10은 0.346, R@100은 0.587로 나타나 복합 검색 작업에 대해 현 최신 모델 역시 만족스러운 성능을 내지 못하며, LLM 기반 재작성은 약한 모델에 도움이 되나 강력한 모델에서는 오히려 성능 저하를 보였다.  

## Limitations  
실제 정보 요구의 복합성과 다양성을 완벽히 포착하는 평가 세트 구축은 여전히 어려우며 일부 특정 복합 유형이나 현실적 맥락이 충분히 반영되지 못한 제약이 존재한다.  

## Conclusion  
복합 다면적 검색을 위한 CRUMB 벤치마크를 통해 현존 검색 모델들의 한계와 개선 방향을 밝히는 데 기여하며, 향후 복잡한 정보 접근 기술 개발의 촉진을 희망한다.

# 17. [From Noise to Narrative: Tracing the Origins of Hallucinations in   Transformers](https://arxiv.org/abs/2509.06938)

## Introduction
- Goal: 본 연구의 목적은 사전 학습된 트랜스포머 모델에서 환각(hallucination)이 발생하는 기제를 희소 오토인코더를 활용하여 실험적으로 통제된 입력 공간에서 추적하는 데 있다.  
- Motivation: 트랜스포머 모델의 환각 현상은 고위험 분야에서 인공지능에 대한 신뢰와 도입을 저해하며, 이에 대한 심층적인 이해가 절실하다.  
- Contribution: 본 연구는 환각이 입력 정보 구조의 붕괴에 따라 트랜스포머 모델 내부에서 의미 개념이 확장되면서 발생하고, 중간 계층 개념 활동 패턴으로 환각을 예측 및 완화할 수 있음을 보였다.  

## Method  
희소 오토인코더(SAE)를 사용하여 다양한 층의 트랜스포머 잔류 스트림 활성화에서 의미 개념을 학습하고, 순수 노이즈 및 교란된 이미지·텍스트 입력에 대해 실험을 수행하였다.  
의미 개념의 순도와 조정 가능성을 평가해 모델이 의미 없는 입력에도 구조화된 개념을 할당함을 검증하였다.  
또한, 개념 활성화 패턴을 활용하여 모델 출력의 환각 정도를 부분 최소 제곱 회귀로 예측하고 일부 개념 억제를 통해 환각 완화를 시도하였다.  

## Results  
입력 정보가 무작위로 교란될수록 트랜스포머 모델의 중간층에서 활성화되는 의미 개념의 수가 증가하며, 이 개념 집합을 기반으로 출력 환각 정도를 실험적으로 예측 및 효과적으로 완화할 수 있었다.  

## Limitations  
본 연구 결과가 10억 파라미터 이상의 최신 대형 트랜스포머 모델에 일반화될지 여부는 추가 검증이 필요하다.  

## Conclusion  
트랜스포머 모델의 환각 현상이 입력 데이터 구조의 붕괴에 따른 내부 의미 개념의 과도한 활성화와 연관되며, 이 현상을 계층별 해석 가능한 개념 공간 분석을 통해 모니터링하고 제어할 수 있는 실용적 방법론을 제시하였다.
