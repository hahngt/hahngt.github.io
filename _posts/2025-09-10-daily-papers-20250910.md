---
layout: post
title: Daily Papers — 2025-09-10"
date: 2025-09-10 08:15:00
tags: [papers, hugginface]
categories: []
---

# 1. [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)

## Introduction

- Goal: 본 연구의 목표는 강화학습을 통해 대형 언어모델에서 병렬 사고 능력을 학습시키는 것이다.
- Motivation: 기존 지도학습 방식은 병렬 사고의 탐색과 일반화 능력 향상에 한계가 있으며, 실제 복잡한 문제에 적용하기 어려웠다.
- Contribution: 본 논문은 병렬 사고 행위를 위한 최초의 강화학습 프레임워크 Parallel-R1과 이를 위한 점진적 커리큘럼 및 보상 설계 방법론을 제시한다.

## Method

병렬 사고는 탐색과 요약의 두 단계로 구성되며, 모델은 <Parallel>, <Path>, <Summary> 토큰으로 병렬 추론을 수행한다. 점진적 커리큘럼으로 쉬운 문제에 대해 지도학습으로 형식을 학습한 뒤, 강화학습으로 난이도 높은 문제에 병렬 사고 능력을 일반화한다. 또한, 구조적 변형을 적용한 모델과 그렇지 않은 모델 각각에 대해 효과적인 보상 설계를 통해 병렬 사고를 유도한다.

## Results

Parallel-R1은 수학 벤치마크(AIME25, AIME24, AMC23, MATH)에서 기존 순차 사고 강화학습 대비 최대 8.4% 정확도 향상과 AIME25 기준 42.9% 성능 향상을 달성하였다.

## Limitations

구조적 주의 메커니즘을 적용한 변형 모델은 쉬운 문제에서 학습한 주의 마스크가 어려운 문제에 일반화되지 않아 성능이 저하되는 한계가 존재한다.

## Conclusion

병렬 사고를 강화학습으로 학습하는 Progressive Curriculum과 보상 설계는 대형 언어모델의 복잡한 수학 추론 능력 향상에 효과적이며, 학습 과정에서 탐색과 검증 전략의 진화 양상을 밝혀냈다.

# 2. [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/abs/2509.07979)

## Introduction

- Goal: 본 연구는 멀티모달 대형 언어 모델(MLLM)의 시각 표현 손실 문제를 해결하고 시각적 추론 능력을 강화하는 것을 목표로 한다.
- Motivation: 기존 MLLM이 텍스트 중심의 감독 방식으로 인해 시각 경로에서 세밀한 시각 정보를 충분히 보존하지 못한다는 점이 한계로 지적되었다.
- Contribution: 본 논문은 사전 학습된 시각 기초 모델(VFM)의 내재적 시각 표현과 MLLM의 중간 표현을 정렬시키는 정규화 기법인 VIRAL을 제안하여 세밀한 시각 표현 유지와 향상된 시각 추론 능력을 실현하였다.

## Method

VIRAL은 MLLM 내부의 중간 시각 표현을 사전 학습된 VFM의 시각 특징과 코사인 유사도 기반으로 정렬하여 세밀한 시각 정보를 보존한다. 학습 시 기존의 텍스트 언어 모델링 손실에 정렬 손실을 추가하여 모델이 시각 경로의 세부 정보를 유지하도록 유도한다. DINOv2 등 강력한 VFM을 교사 모델로 활용하여 시각 정보의 풍부성을 극대화하였다.

## Results

제안한 VIRAL은 다양한 시각 인코더와 대형 언어 모델 백본에서 광범위한 벤치마크(예: CV-Bench2D, MMVP, POPE 등) 전반에 걸쳐 일관된 성능 향상을 확인하였다.

## Limitations

본 연구는 아직 복합적 공간 추론이나 특정 시각 인코더 한계에 따른 성능 저하 문제에 대해 완전한 해결책을 제시하지 못하였다.

## Conclusion

VIRAL은 MLLM 시각 경로 내 표현 정렬을 통해 시각 정보의 손실을 방지하고, 강력한 시각 추론 능력을 획득하는 새로운 방향성을 제시하였다.

# 3. [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)

## Introduction

- Goal: 본 연구는 어려운 시각 탐색 문제를 해결하기 위해 깊고 다양한 추론 패턴 및 다중 턴 상호작용을 확장하는 멀티모달 시스템 Mini-o3를 제안하는 것이다.
- Motivation: 기존의 공개된 시각-언어 모델들은 단조로운 추론 패턴과 제한된 상호작용 횟수로 인해 시도와 오류가 필요한 어려운 문제에 취약하다.
- Contribution: Mini-o3는 Visual Probe 데이터셋 구축, 다양하고 복합적인 추론 궤적 생성을 위한 반복적 데이터 수집, 그리고 over-turn masking 전략으로 테스트 시 턴 수 확장을 지원하는 강화학습 기법을 새롭게 도입하였다.

## Method

Mini-o3는 사용자 질의와 이미지 입력을 받아 생각과 행동을 반복 생성하며, 각 행동은 이미지 툴을 호출해 관찰 결과를 얻고 이를 바탕으로 상호작용 궤적을 쌓는다.  
학습은 ① Visual Probe라는 고난도 시각 탐색 데이터셋으로 다중 턴 지도학습을 진행하고, ② 기존 VLM을 활용한 cold-start 다중 턴 행위 궤적을 수집하며, ③ 확장 가능한 테스트 성능을 위해 제한 턴을 초과하는 경로에 손실을 부과하지 않는 over-turn masking 방식으로 강화학습을 수행한다.  
추론 시에는 반복적 추론 경로가 수십 턴에 달하며 온도 샘플링을 통해 어휘 반복 현상을 완화한다.

## Results

Mini-o3는 Visual Probe 및 다수의 벤치마크에서 기존 공개 모델을 큰 폭으로 뛰어넘는 시각 탐색 정확도를 달성하였으며, 테스트 시간에 상호작용 턴 수 증가에 따라 성능이 지속적으로 향상됨을 확인하였다.

## Limitations

훈련 시 상호작용 횟수 제한과 컨텍스트 길이 제약 문제로 인해 최적 턴 수 설정과 학습 안정화가 요구된다.

## Conclusion

본 연구는 다중 턴 이미지 툴 사용과 강화학습을 결합하여 복합적이고 깊이 있는 시각 추론 능력을 갖춘 Mini-o3를 개발하였으며, 이는 시도와 오류가 필요한 난제 시각탐색 문제 해결에 있어 실질적 진전을 이루었다.

# 4. [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)

## Introduction

- Goal: 본 연구는 통합 멀티모달 모델(UMM)의 생성 능력과 편집 정확도를 향상시키기 위한 리컨스트럭션 얼라인먼트(RecA) 방법을 제안하는 것이다.
- Motivation: 기존 UMM은 이미지-텍스트 쌍의 희소한 캡션에 의존하여 미세한 시각 정보를 포착하지 못해 이해와 생성 간 정렬 불일치 문제가 존재한다.
- Contribution: RecA는 시각 이해 인코더의 임베딩을 조밀한 “텍스트 프롬프트”로 활용하여 자기 지도 학습 방식으로 입력 이미지를 재생성하도록 모델을 후학습함으로써 모든 UMM 아키텍처에 걸쳐 생성 및 편집 성능을 크게 개선하였다.

## Method

RecA는 시각 이해 인코더에서 추출한 의미론적 임베딩을 템플릿 텍스트와 결합해 UMM에 입력하고 입력 이미지 재생성 손실을 최적화하는 후학습 기법이다. 이러한 조밀한 의미 기반 신호는 기존 텍스트 캡션보다 풍부하고 미세한 시각 속성을 포함하여 이해-생성 간 표현 불일치를 해소한다. 후학습 과정은 표준 UMM 추론과 다르지 않아 실사용성이 유지된다.

## Results

RecA는 1.5B 파라미터 UMM을 27 GPU시간 동안 후학습하여 GenEval, DPGBench, ImgEdit, GEdit 벤치마크 전반에서 GPT-4o 및 큰 모델들을 능가하는 최첨단 성능을 달성하였다.

## Limitations

RecA는 텍스트-이미지 데이터 없이도 성능을 개선하지만, 더 깊은 언어 추론 능력 향상은 본 연구 범위를 벗어난 문제로 남았다.

## Conclusion

RecA는 시각 이해 임베딩을 활용한 단순하면서도 범용적인 후학습 전략으로 통합 멀티모달 모델의 생성 및 편집 능력을 현저히 향상시키는 효과적인 방법임이 입증되었다.

# 5. [UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward](https://arxiv.org/abs/2509.06818)

## Introduction

- Goal: 본 연구의 목적은 다중 참조 이미지 기반 이미지 커스터마이징에서 일관된 다중 신원 보존을 확장하는 것이다.
- Motivation: 인간은 얼굴에 매우 민감하여 다중 신원 이미지에서 신원이 혼동되는 문제로 인해 커스터마이징 모델의 신원 확장성이 제한된다.
- Contribution: 본 논문에서는 다중 신원 생성을 전역 할당 최적화 문제로 재구성하고, 확장 가능한 다중 신원 일관성 유지를 위한 통합 다중 신원 최적화(UMO) 프레임워크를 제안한다.

## Method

UMO는 단일·다중 신원 참조 임베딩 간 코사인 유사도를 기반으로 한 보상 신호를 확장해 다중 신원 매칭 문제를 이분 그래프로 모델링하고, 헝가리안 알고리즘으로 최적 쌍을 할당한다.  
이 과정은 확산 모델에 강화학습(ReReFL)을 적용하여 기존 이미지 커스터마이징 기법 전반에 일관성 향상을 도모한다.  
또한, 다중 참조 이미지를 포함하는 대규모 커스터마이징 데이터셋과 다중 신원 혼동 정도를 측정하는 새 평가 지표(ID-Conf)를 함께 제안한다.

## Results

XVerseBench 및 OmniContext 벤치마크에서 UMO는 다양한 기준에서 ID-Sim과 ID-Conf 점수를 대폭 향상시키며, 기존 공개 소스 모델 대비 신원 보존 능력과 혼동 완화에서 최첨단 성능을 달성하였다.

## Limitations

정보 부족.

## Conclusion

UMO는 다중 신원 참조 기반 이미지 커스터마이징에서 전역 매칭 보상 학습을 통해 신원 일관성을 크게 개선하고 혼동을 완화하며, 높은 확장성과 범용성을 갖춘 최첨단 프레임워크임을 입증하였다.

# 6. [F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions](https://arxiv.org/abs/2509.06951)

## Introduction

- Goal: 본 논문은 언어 조건화된 작업을 동적 시각 환경에서 수행하기 위한 Vision-Language-Action(VLA) 모델인 F1을 제안하는 것이다.
- Motivation: 기존 VLA 모델들은 반응적인 상태-행동 매핑에 의존하여 동적 환경에서 단견적 행동과 낮은 강인성을 보이는 한계가 존재한다.
- Contribution: F1은 시각 예측(visual foresight)을 의사결정에 통합하는 Mixture-of-Transformer 아키텍처와 3단계 훈련 전략을 도입하여 이해, 생성, 행동을 통합하고, 실험에서 기존 방법보다 우수한 성능을 입증하였다.

## Method

F1은 이해 전문가, 생성 전문가, 행동 전문가로 구성된 Mixture-of-Transformer 구조를 채택하며, 생성 전문가는 다음 단계 시각 상태를 예측하여 명시적 계획 목표를 제공한다.  
예측된 시각 정보를 활용해 행동 생성을 역동역학 문제로 재구성하며, 3단계로 구성된 점진적 훈련 과정을 통해 일반화 가능하고 견고한 예측 기반 정책을 학습한다.  
또한, 이해-생성-행동 전문가 간 진행적 주의(attention) 메커니즘을 도입하여 정보 교환을 최적화한다.

## Results

F1은 136개 작업, 33만 개 이상의 궤적 데이터로 훈련되어 실제 로봇 및 시뮬레이션 벤치마크에서 다른 SOTA VLA 모델 대비 작업 성공률과 일반화 능력에서 현저한 향상을 보였다.

## Limitations

현재 F1의 성능 및 효율성 개선을 위한 계산 비용과 복잡도 문제에 대한 상세한 논의는 부족하다.

## Conclusion

본 연구는 시각적 예측을 통합한 차세대 VLA 모델 F1을 통해 이해, 생성, 행동을 유기적으로 결합하여 동적 환경에서의 로봇 조작 작업 수행 능력을 크게 향상시켰다.

# 7. [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)

## Introduction

- Goal: 본 연구의 목표는 추가 데이터 없이 대형 언어 모델(LLM)의 성능을 향상시키는 강화학습 기법을 제안하는 것이다.
- Motivation: 기존 LLM 학습은 대량의 고품질 데이터에 의존하는데, 데이터 확보의 한계로 학습의 근본적인 병목현상이 발생하기 때문이다.
- Contribution: 본 연구는 자기 대결(self-play)이라는 게임 이론적 틀을 도입하여 단일 모델이 스스로 도전 문제를 생성하고 해결하며 학습하는 Language Self-Play(LSP) 알고리즘을 제안하였다.

## Method

Language Self-Play는 Challenger와 Solver 두 역할을 하나의 LLM이 번갈아 수행하며 Challenger는 Solver가 해결하기 어려운 문제를 생성하고 Solver는 이를 해결하며 상호 경쟁 게임을 통한 강화학습을 진행한다.  
Solver는 주어진 쿼리에 대한 답변으로 최대 보상을 획득하는 것을 목표로 하고, Challenger는 Solver의 보상을 최소화하는 난제들을 생성함으로써 쿼리 난이도를 점진적으로 높인다.  
학습 과정에는 KL 발산 규제를 적용하여 무의미한 적대적 입력 생성을 방지하며, 자체 생성한 품질 평가 점수를 보상에 추가하여 안정적인 자기 대결 훈련이 가능하도록 하였다.

## Results

AlpacaEval 벤치마크에서 Llama-3.2-3B-Instruct 기반의 LSP는 기존 데이터 기반 강화학습 대비 동등하거나 우수한 전반적 성능을 보였으며, 특히 대화형 태스크에서 큰 성능 향상을 기록하였다.

## Limitations

본 방법은 자기생성 데이터 품질에 크게 의존하고, 일부 사용자 유형 쿼리에서는 성능 저하가 확인되어 쿼리 다양성과 모델 품질 유지 간 균형 조절이 필요하다.

## Conclusion

추가 데이터 없이 LSP 알고리즘을 활용한 자기대결 학습은 LLM의 지속적인 자율 성능 향상 가능성을 보여주며, 향후 AI의 자가 경험 데이터 확보와 확장에 기여할 수 있다.

# 8. [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)

## Introduction

- Goal: 본 연구는 대형 언어 모델(LLM)의 추론 능력 향상을 위해 문제 난이도를 모델 역량에 맞춰 동적으로 조절하는 강화학습 기반 프레임워크 SEELE를 제안하는 것이다.
- Motivation: 기존 RLVR 방법들은 문제 난이도와 모델 능력 간 불일치로 인해 탐색 효율이 저하되는 한계가 존재하였다.
- Contribution: SEELE는 문제별 실시간 난이도 조절을 위한 다중 라운드 롤아웃 샘플링과 항목반응이론 기반 정확도 예측 모델을 도입하여 학습 효율을 극대화하였다.

## Method

SEELE는 기존의 고정된 오프폴리시 힌트 방식과 달리 문제마다 힌트 길이를 동적으로 조절하여 약 50%의 롤아웃 정확도를 유지한다. 이를 위해 각 라운드에서 수집된 힌트 길이와 정확도 데이터를 바탕으로 항목반응이론(IRT) 3PL 모델을 통해 최적 힌트 길이를 예측한다. 이 과정을 반복하며 모델 능력 변화에 대응하여 문제 난이도를 지속적으로 조절한다.

## Results

SEELE는 수학 및 일반 도메인 추론 9개 벤치마크에서 기존 GRPO 대비 평균 +11.8점, SFT 대비 +10.5점 향상을 달성하며 우수한 성능을 보였다.

## Limitations

본 연구는 훈련에 사용되는 힌트 및 정답 해설의 품질에 의존하며, 힌트 생성 자동화 및 다양한 문제 유형에 대한 일반화에 대한 논의는 부족하였다.

## Conclusion

SEELE는 모델 역량에 맞춰 문제 난이도를 능동적으로 조절하는 힌트 기반 강화학습 프레임워크로서, RL 학습 효율성과 추론 능력을 현저히 향상시켰다.

# 9. [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830)

## Introduction

- Goal: 본 연구의 목표는 대규모 다중 영상 모달리티를 활용한 방사선학 분야의 파운데이션 모델 Curia를 개발하고 평가하는 것이다.
- Motivation: 기존의 방사선학 AI는 단일 과제에 특화된 모델 중심으로 개발되어 임상 영상의 다양성과 복잡성을 포괄하기 어렵다.
- Contribution: Curia는 150,000건 이상의 임상 CT 및 MRI 데이터를 통해 자기지도학습으로 학습되었으며, 19개 임상 과제를 포함하는 새 벤치마크 CuriaBench에서 기존 모델을 능가하는 성능을 보였다.

## Method

본 연구는 DINOv2 자기지도학습 알고리즘을 사용하여 ViT 기반 모델을 200만 장 이상의 2D CT 및 MRI 영상으로 선학습시켰다. 전이학습 없이 고정된 모델 피처에 경량 분류기를 학습하여 다양한 분류, 회귀, 생존 예측, 분할 과제를 수행하였다. 또한 다중 모달 간 일반화와 적은 학습 데이터 환경에서의 성능을 체계적으로 평가하였다.

## Results

Curia는 19개 방사선학 과제에서 방사선 전문의 및 기존 파운데이션 모델 대비 동등하거나 우수한 성능을 나타내었으며, 특히 교차 모달리티 일반화, 적은 데이터 학습 능력, 복잡한 종양 예후 예측 분야에서 탁월한 성과를 보였다.

## Limitations

정보 부족.

## Conclusion

Curia는 임상 방사선 영상 해석을 위한 강력하고 범용적인 파운데이션 모델로서, 다중 의료 영상 모달리티를 통합하여 높은 진단 정확도와 데이터 효율성을 구현하였다.

# 10. [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)

## Introduction

- Goal: 본 연구의 목표는 강화학습을 통해 대형 언어 모델(LLM) 내에서 emergent hierarchical reasoning, 즉 고수준 전략 계획과 저수준 절차 실행으로 구분되는 계층적 추론 능력이 어떻게 형성되는지 규명하는 것이다.
- Motivation: 기존 LLM 강화학습 과정에서 관찰되는 'aha moments', 'length-scaling', 토큰 엔트로피 변화 등의 현상이 별개의 현상이 아니라 하나의 통합된 계층적 추론 구조의 징후임에도 불구하고 이에 대한 이해가 부족하기 때문이다.
- Contribution: 본 연구는 강화학습 과정에서 저수준 절차 스킬의 마스터링 후 고수준 전략 계획 단계로 학습 초점이 전환됨을 실험적으로 확인하고, 이를 기반으로 전략적 계획 토큰에 집중하는 새로운 강화학습 알고리즘 HICRA를 제안하여 기존 방법 대비 성능을 대폭 향상시켰다.

## Method

강화학습 과정에서 추론을 고수준의 전략 계획 토큰과 저수준의 실행 토큰으로 기능적으로 구분하고, 전략 계획 토큰에 집중하는 계층 인식 크레딧 할당(HICRA) 알고리즘을 설계하였다.  
HICRA는 GRPO 기반 정책 이득 함수에 전략 계획 토큰에 가중치를 부여하여 최적화 신호를 강화함으로써 학습 효율을 높였다.  
또한 고수준 전략 탐색의 다양성을 측정하기 위해 의미적 엔트로피를 도입하여 토큰 단위 엔트로피의 한계점을 극복하였다.

## Results

HICRA는 Qwen, LLama, MiMO 등 다양한 모델과 수학·멀티모달 추론 벤치마크에서 기존 GRPO 및 엔트로피 규제 기반 방법을 일관되게 능가하며 전략적 계획 능력 개발에 유의미한 향상을 보였다.

## Limitations

HICRA는 기저 모델이 충분한 저수준 절차 신뢰도를 확보한 경우에 효과적이며, 절차 신뢰도가 낮은 모델에서는 전략적 탐색 강화가 오히려 학습 안정성을 저해할 수 있다는 제한점이 존재한다.

## Conclusion

본 연구는 대형 언어 모델의 강화학습 추론 향상이 계층적 학습 동역학에 의해 주도되며, 전략 계획 토큰에 집중하는 학습 방식이 고성능 추론 개발의 핵심임을 밝혔다.

# 11. [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)

## Introduction

- Goal: 본 논문은 표준 인과(attention) 메커니즘의 한계를 극복하기 위해 토큰의 키(key)를 문맥 전개에 따라 지속적으로 갱신하는 새로운 인과 주의 메커니즘 CASTLE을 제안하는 데 목적이 있다.
- Motivation: 기존 인과 주의는 각 토큰의 쿼리, 키, 값(QKV)이 고정되어 미래 정보를 반영하지 못해 전역 문맥 파악과 자연어 이해 능력에 제약이 발생한다.
- Contribution: CASTLE은 과거 위치에 속하면서도 이후 토큰 정보를 통합하는 '전망(lookahead) 키'를 도입하여 자기회귀 특성을 보존하면서도 문맥에 따라 키를 갱신하고, 수학적 동등성을 활용해 효율적인 병렬 학습을 가능하게 한다.

## Method

CASTLE은 각 토큰에 대해 키를 과거 문맥에 해당하는 인과 키와 미래 토큰 정보를 포함하는 전망 키로 나누어 반영한다.  
전망 키는 문맥 내 토큰의 재귀적 갱신을 통해 미래 문맥 정보를 부분적으로 포함하되, 쿼리는 자기회귀 구조에 따라 수정되지 않고 미래 토큰 정보를 사용하지 않는다.  
이 메커니즘은 병렬 학습에 적합하도록 수학적 동등 변환을 통해 모든 전망 키를 직접 계산하지 않고도 효율적인 연산을 지원한다.

## Results

FineWeb-Edu 데이터셋에서 50B 토큰으로 사전학습한 결과, 모든 모델 규모에서 CASTLE은 표준 인과 주의 대비 검증 손실 및 퍼플렉서티가 감소하고 다양한 다운스트림 작업에서 일관되게 성능을 향상시켰다.

## Limitations

전망 값을 갱신하는 방안에 관한 연구는 진행되지 않아, 키 갱신에 제한된 현재 설계의 확장 가능성은 향후 연구 대상으로 남아 있다.

## Conclusion

CASTLE은 문맥 정보에 따라 키를 동적으로 업데이트함으로써 표준 인과 주의 대비 더 나은 언어 모델링 성능과 다운스트림 태스크에서의 향상을 동시에 달성하는 효과적인 자기회귀 주의 메커니즘임을 입증하였다.

# 12. [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)

## Introduction

- 본 연구의 목표는 분산된 이질적 컴퓨팅 노드 네트워크에서 강화학습(RL)을 활용해 언어 모델( LM)을 효과적으로 후처리 학습하는 것이다.
- 기존 RL 후처리 방식은 대규모 GPU 클러스터 동기화와 높은 비용, 통신 병목 현상을 요구하여 확장성에 한계가 존재한다.
- 본 논문에서는 이러한 한계를 극복하기 위해 완전 분산 및 비동기식 RL 알고리즘인 Swarm sAmpling Policy Optimization(SAPO)를 제안하였다.

## Method

SAPO는 각 노드가 자체 정책 모델을 학습하면서 롤아웃을 네트워크 내 다른 노드들과 공유하는 방식으로 경험을 분산하여 학습 효율을 높인다.  
네트워크 동기화, 지연, 하드웨어 균질성 가정을 제거하며 노드들은 독립적으로도 동작 가능하다.  
롤아웃 샘플링 및 경험 필터링 기능을 적용하여 학습 데이터의 다양성과 품질을 유지한다.

## Results

ReasoningGYM 데이터셋을 활용한 통제된 실험에서 SAPO는 기존 방식 대비 최대 94%에 달하는 누적 보상 향상을 달성하였다.

## Limitations

외부 롤아웃 의존도가 과도할 경우 학습 안정성이 저하되어 진동 및 망각 현상이 발생하는 경향이 관찰되었다.

## Conclusion

SAPO는 경험 공유를 핵심으로 하는 완전 분산 RL 후처리 알고리즘으로서 비용과 확장성 문제를 해결하며 LMs의 추론 능력 향상을 위한 협력적 후처리 학습의 실용적 방향을 제시한다.

# 13. [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)

## Introduction

- Goal: 본 연구는 OpenAI의 SimpleQA 데이터를 정제하여 더 신뢰성 높고 도전적인 1,000개 문항의 단답형 사실성 평가 벤치마크인 SimpleQA Verified를 제안하는 데 목적이 있다.
- Motivation: 기존 SimpleQA는 노이즈 및 오류 레이블, 주제 편향, 질문 중복 등의 한계로 인해 모형의 실제 사실성 능력을 정확히 평가하기 어렵다.
- Contribution: 다단계 중복 제거, 토픽 균형 조정, 출처 재검증 등의 엄격한 필터링 절차를 도입하고 오토레이팅 프롬프트를 개선하여 더 신뢰할 수 있는 평가 도구를 구축하였다.

## Method

SimpleQA Verified는 동일 출처 중복 제거, 임베딩과 TF-IDF 기반 의미 중복 제거, 웹 게시자 정책 준수, 답변 유형 및 주제 분포 균형화, 상충 출처 조정, 수치 답변 허용 범위 재설정 순서로 데이터 셋을 단계별 정제하였다. 모델의 답변 평가를 위해 수치 오답 판정을 위한 허용 오차를 명시하고, 불확실 및 장황한 응답에 대한 자동채점 방식을 개선하였다. 이 과정에서 최고 난이도 문항을 선별하여 벤치마크의 도전성을 유지하였다.

## Results

Gemini 2.5 Pro 모델이 SimpleQA Verified 상에서 F1-스코어 55.6으로 현존 최고 성능을 기록하며, GPT-5 등 경쟁 모델보다 우수한 성과를 보였다.

## Limitations

SimpleQA Verified 벤치마크는 외부 검색 도구 없이 모델 내부 지식만을 평가하는 단답형 사실성에 국한되어 있어 장문 생성 사실성 등 다른 영역에 적용이 제한적이다.

## Conclusion

SimpleQA Verified는 기존 SimpleQA의 한계를 극복한 신뢰도 높은 평가 도구로서, 파라메트릭 지식 사실성 측정에 있어 첨단 모델의 성능 변별력을 향상시키고 AI 신뢰성 연구 발전에 기여한다.

# 14. [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)

## Introduction

- Goal: 본 연구는 텍스트-투-이미지 확산 모델을 세밀한 인간 선호도에 정렬하는 새로운 강화학습 프레임워크를 제안하는 것을 목표로 한다.
- Motivation: 기존 방식은 연산 비용이 큰 다중 단계 디노이징과 보상 모델의 오프라인 반복 조정이 필요하여 실제 적용에 제약이 있었다.
- Contribution: 저자들은 초기 단계부터 효율적으로 이미지를 복원하는 Direct-Align 기법과 텍스트 조건화된 보상을 통한 온라인 조정이 가능한 Semantic Relative Preference Optimization (SRPO)을 제안하였다.

## Method

Direct-Align은 정해진 가우시안 노이즈를 이미지에 주입하여 어느 시점에서도 원본 이미지를 단일 보간으로 복원할 수 있게 하여 보상 신호를 초기에 안정적으로 전달한다.  
SRPO는 긍정 및 부정 프롬프트를 이용해 보상 신호를 텍스트 조건화함으로써 오프라인 보상 미세조정 없이도 온라인에서 보상을 세밀하게 조절할 수 있도록 하였다.  
두 방법을 결합해 FLUX.1.dev 모델을 단 10분의 훈련으로 기존 대비 3배 이상의 사실감과 미학 품질 향상을 달성하였다.

## Results

제안 방법은 HPDv2 벤치마크에서 기존 최첨단 온라인 강화학습 기법 대비 최대 3.7배의 인간 평가 기반 사실감 향상과 3.1배의 미학 품질 개선을 이루었으며, 75배 이상의 훈련 효율성을 기록하였다.

## Limitations

본 기법은 보상 모델이 인지하지 못하는 제어 토큰에 대해서는 통제력이 저하되고, 잠재 공간 유사도에 의존하기에 일부 제어 텍스트가 의도와 다르게 작용할 가능성이 존재한다.

## Conclusion

본 연구는 대규모 텍스트-투-이미지 확산 모델에서 보상 히킹 문제를 완화하고, 보상을 텍스트 조건화하여 온라인에서 세밀한 인간 선호도를 효과적으로 반영함으로써 사실감과 미학 품질을 크게 향상시키는 새로운 강화학습 방법을 제안하였다.

# 15. [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)

## Introduction

- Goal: 본 연구는 적은 단계(2~8단계)의 확산 모델에 적응형 양자화 인식 스케줄러(Q-Sched)를 도입하여 모델 크기와 계산 비용을 줄이는 동시에 생성 이미지 품질을 유지하는 것을 목표로 한다.
- Motivation: 텍스트-이미지 확산 모델은 매우 큰 모델과 다수의 단계 평가를 요구해 계산 비용이 높아, 효율적 압축 및 가속화 기법이 필요하다.
- Contribution: Q-Sched는 기존 모델 가중치가 아닌 노이즈 스케줄러를 조정하는 새로운 사후 양자화 방식과, 텍스트-이미지 정합성 및 화질을 동시에 최적화하는 참조 없는 JAQ 손실함수를 제안한다.

## Method

Q-Sched는 few-step 확산 모델의 샘플링 경로를 양자화 인식 스케줄러로 학습 조정하여 기존 모델 대비 4배 이상 축소된 크기로도 성능 저하 없이 고품질 이미지를 생성한다.  
두 개의 학습 가능한 선형 계수(cx, cϵ)를 통해 이전 상태와 양자화된 노이즈 예측에 각각 다르게 적용하여 왜곡과 인공물 문제를 완화한다.  
JAQ 손실은 텍스트-이미지 호환성과 이미지 품질 지표를 결합해 참조 이미지 없이 세밀한 최적화를 가능하게 한다.

## Results

Q-Sched는 기존 full-precision 대비 15.5% 이상의 FID 향상과 사용자 선호도 조사에서 믹스DQ, PTQD 대비 높은 이미지 품질 평가를 기록하였다.

## Limitations

제안 기법은 고비트양자화(W8A8) 환경에서 일부 경우 FID 지표가 낮아지는 경향을 보이며, 민감한 조정이 요구된다.

## Conclusion

Q-Sched는 few-step 확산 모델과 양자화를 결합하여 모델 압축과 속도 개선을 동시에 달성하며, 텍스트-이미지 생성 분야에서 효율적인 고품질 생성의 경계를 확장하였다.

# 16. [ΔL Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)

## Introduction

- 본 논문은 강화학습 기반 루징 집계 과정에서 발생하는 응답 길이의 동적 변동 문제를 해결하기 위한 ∆L Normalization이라는 손실 집계 방법을 제안한다.
- RLVR에서 응답 길이가 크게 달라짐에 따라 그래디언트 분산이 커지고 최적화가 불안정해지는 문제점이 존재한다.
- 제안 방법은 기존 방식들의 편향 또는 높은 분산 문제를 해결하여, 최소 분산의 편향 없는 손실 집계를 구현한다.

## Method

- RLVR에서 길이에 따른 그래디언트 분산이 응답 길이에 비례함을 이론적·실험적으로 분석하였다.
- 최소 분산 편향 추정 문제로 문제를 재구성하여, 길이의 역수를 가중치로 하는 ∆L Normalization을 도입하였다.
- 하이퍼파라미터 α를 통해 긴 응답의 활용과 분산 감소 효과 간 균형을 조절할 수 있다.

## Results

- 제안된 ∆L Normalization은 여러 모델 크기, 최대 응답 길이, 그리고 CountDown 및 Math 과제에서 기존 방법 대비 안정적 학습과 우수한 성능을 일관되게 달성하였다.

## Limitations

- 본 연구는 하이퍼파라미터 α의 설정과 관련한 최적화 및 더 다양한 RLVR 응용 분야에 대한 적용 가능성 검토가 추가로 필요하다.

## Conclusion

- ∆L Normalization은 RLVR에서 길이 변화가 큰 출력에 적합한 편향 없고 분산이 최소화된 손실 집계 기법으로, 안정적인 학습과 높은 성능 향상을 보였다.

# 17. [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)

## Introduction

- 본 연구의 목표는 다양한 복합 정보검색 과제에 대해 최신 정보검색 모델들의 성능을 종합적으로 평가하는 벤치마크를 구축하는 것이다.
- 사용자의 검색 요구가 점점 복합적이고 다면화됨에 따라 기존의 단일 측면 쿼리 중심 평가체계로는 실제 정보검색 시스템의 성능 한계를 평가하기 어렵기 때문이다.
- 본 논문은 8개의 현실적이고 다양한 복합 검색 과제를 포함하는 CRUMB 벤치마크와 이를 활용한 최첨단 정보검색 모델들의 평가 결과를 제시한다.

## Method

복합 쿼리가 포함된 8가지 상이한 과제를 엄선하여 CRUMB 벤치마크를 구성하였다.  
각 과제는 다중 요구사항, 논리 연산, 특정 도메인 전문용어 등 다양한 복합성 요소를 반영하였으며, 문서들은 통일된 마크다운 형식으로 전처리 및 구조화하였다.  
최신 신경망 기반 검색 모델들과 LLM을 이용한 쿼리 확장 및 재작성 기법을 적용 · 비교 평가하였다.

## Results

CRUMB 평가 결과 최고 성능 모델도 nDCG@10 0.346, R@100 0.587로 낮은 성능을 보여 복합 정보검색 과제에 대한 현행 모델들의 한계를 드러냈다.

## Limitations

본 연구는 복합성 측면에서 다양한 과제를 포함했으나 여전히 모든 유형의 복잡한 쿼리를 완전하게 포괄하지는 못하였다.

## Conclusion

CRUMB 벤치마크와 분석 결과는 복합 검색 과제에서 정보검색 모델 성능 향상을 위한 중요한 기준과 방향성을 제공하며, 차세대 모델 개발에 기여할 것으로 기대된다.

# 18. [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)

## Introduction

- 본 연구의 목표는 사전학습된 트랜스포머 모델에서 환각(hallucination)이 어떻게 발생하는지 내부 개념 표현을 통해 규명하는 것이다.
- 트랜스포머 모델이 불확실하거나 혼란스러운 입력에 대해 일관성 있으나 입력에 무관한 의미론적 특성을 활성화하여 환각을 유발하는 문제가 AI 신뢰성 및 안전성을 저해한다는 점에서 연구 동기가 제기되었다.
- 본 연구는 희소 오토인코더(Sparse Autoencoders, SAE)를 활용하여 노이즈 입력 조건에서 트랜스포머 내부 의미 개념의 확장 양상을 분석하고, 개념 활성화 패턴으로 모델 환각을 예측할 수 있음을 보였다.

## Method

- 다양한 크기와 유형의 시각 및 언어 트랜스포머 모델 잔여 스트림 활성화를 대상으로 SAE를 학습시켜 내부 개념 공간을 추출하였다.
- 노이즈 및 입력 변형(문장 단어 셔플링, 이미지 패치 셔플링)을 통해 입력 불확실성을 조작하고 이에 따른 내부 개념 활성화 변화를 실험적으로 탐색하였다.
- 환각 평가를 위해 Gemma 2B-IT 모델에 대해 텍스트 요약 과제를 수행시키고, SAE 개념 활성화로부터 부분 최소제곱 회귀모델을 통해 환각 점수를 예측하였다.

## Results

- 입력 정보가 구조적으로 불명확할수록 중간 층에서 활성화되는 의미론적 개념 수가 증가하였고, 사전학습된 트랜스포머가 의미 없는 노이즈에도 일관된 의미 개념을 생성하며, 이 개념들로부터 모델 출력 환각 경향성을 유의미하게 예측할 수 있었다.

## Limitations

- 연구 결과는 다양한 모델 및 모달리티에서 일관되게 관찰되었으나, 10억~100억 파라미터급 대형 트랜스포머 모델에 대한 일반화 가능성은 추가 검증이 필요하다.

## Conclusion

- 본 연구는 트랜스포머 모델 내부에서 환각을 유발하는 개념 공간의 확장과 동작 메커니즘을 규명함으로써 AI 안전성, 신뢰성 확보 및 자동 환각 위험 평가를 위한 근거를 제공하였다.
