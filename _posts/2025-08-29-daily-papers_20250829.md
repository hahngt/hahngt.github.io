---
layout: post
title: Daily Papers — 2025-08-29"
date: 2025-08-29 08:15:00
tags: [papers, arxiv, ai]
categories: []
---


# 1. [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable   Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)

## Introduction
- 본 연구의 목표는 텍스트-이미지 변환(Text-to-Image, T2I) 강화학습에서 보상 해킹 문제를 완화하고 안정적인 학습을 가능케 하는 PREF-GRPO라는 쌍대 선호(pairwise preference) 보상 기반 GRPO 방법을 제안하는 것이다.  
- 기존 점수화(pointwise) 보상 모델의 미미한 보상 차이가 과도하게 증폭되는 환상적 이점(illusory advantage)으로 인해 보상 해킹 현상과 학습 불안정이 발생한다는 점에 착안하였다.  
- 본 연구는 환상적 이점 문제를 분석하고, 이를 해결하는 PREF-GRPO 방법과 세밀한 평가가 가능한 통합 텍스트-이미지 생성 벤치마크인 UNIGENBENCH를 함께 제시하였다.  

## Method  
- PREF-GRPO는 기존 절대 점수 최대화 방식을 이미지 쌍별 선호 순위 적합(pairwise preference fitting)으로 대체하여 각 이미지 쌍 간 선호도를 평가하고, 승률 기반 보상으로 정책을 최적화한다.  
- 이를 통해 보상 분산이 증폭되고, 점수 소규모 변동에 따른 과최적화를 방지함으로써 학습을 안정화하고 보상 해킹을 경감시키는

# 2. [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)

## Introduction
- 본 연구의 목표는 에이전트 강화학습(agentic reinforcement learning)을 통해 최첨단 수준의 수학적 추론 능력을 갖춘 14억 파라미터 규모의 rStar2-Agent 모델을 개발하는 것이다.  
- 기존의 긴 Chain-of-Thought(CoT) 접근법이 복잡한 문제에서 중간 단계 오류나 창의적 사고 전환에 한계를 가지는 반면, 도구 사용과 피드백 반영을 통해 더 스마트한 추론을 구현하고자 하였다.  
- 본 논문은 효율적인 RL 인프라 구축, 환경 잡음을 극복하는 새로운 RL 알고리즘(GRPO-RoC), 그리고 계산 자원 소모를 최소화하는 다단계 RL 학습법을 제안한다.  

## Method  
- 코딩 도구 및 해석기를 통합한 다중 턴 수행 환경에서 에이전트가 실행 결과를 반영하여 자율적으로 탐색 및 검증하며 추론 과정을 개선하도록 설계되었다.  
- GRPO-RoC 알고리즘은 환경 잡음으로 인한 오류가 포함된 경로를 걸러내고, 품질 높은 성공적 경로에 우선순위를 둬 학습 안정성과 효율을 향상시킨다.  
- 효율적인 인프라와 로드 밸런싱 스케줄

# 3. [USO: Unified Style and Subject-Driven Generation via Disentangled and   Reward Learning](https://arxiv.org/abs/2508.18966)

## Introduction
- Goal: 본 논문은 주제 기반 및 스타일 기반 이미지 생성 작업을 통합하여 하나의 모델로 동시 최적화하는 USO(Unified Style-Subject Optimized) 프레임워크를 제안하는 것을 목표로 한다.  
- Motivation: 기존 연구들은 스타일 중심 생성과 주제 중심 생성을 분리된 과제로 다루면서 스타일 유사성과 주제 일관성 확보 간 상충 현상이 존재함에 주목하였다.  
- Contribution: 스타일과 주제의 상호 보완적 특성을 활용하는 공동 비분리(co-disentanglement) 학습 및 보상 학습(style reward learning)을 통한 통합 모델과 대규모 삼중 데이터셋 구축, 그리고 새로운 평가 벤치마크 USO-Bench를 제안하였다.  

## Method  
USO는 스타일 이미지, 주제 이미지, 스타일이 반영된 주제 이미지로 구성된 삼중 데이터셋을 구축하여 학습에 활용한다. 스타일 정렬 학습과 내용-스타일 비분리 학습을 병행하면서, 스타일 보상 학습을 도입하여 스타일 유사도의 최적화를 지원한다. 이렇게 분리된 인코더와 다중 조건 입력 방식을 통해 스타일과 주제 정보를 함께 처리하며 자유로운 스타일-주제 재조합이 가능하다.  

## Results  
제안된 USO 모델은 USO

# 4. [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)

## Introduction
- Goal: 본 논문은 Agentic AI의 “learning from practice” 패러다임 실현을 위해 효율적인 경험 생성과 대규모 상호작용을 지원하는 오픈소스 프레임워크 AWORLD를 제안하는 것을 목표로 한다.  
- Motivation: 현재 복잡한 벤치마크인 GAIA에서 에이전트-환경 간 상호작용의 비효율성이 강화학습에서 근본적인 병목 현상으로 작용하고 있기 때문이다.  
- Contribution: AWORLD는 분산 아키텍처를 통해 경험 생성 속도를 14.6배 가속화하며, 이를 활용해 Qwen3-32B 기반 에이전트를 성공적으로 강화학습시켜 GAIA 벤치마크에서 최첨단 독점 모델을 능가하는 성능을 달성하였다.

## Method  
AWORLD는 에이전트 구축, 메시지 통신 프로토콜, 분산 상태 관리, 그리고 외부 강화학습 프레임워크와 통합되는 학습 조율 기능을 포함하는 모듈형 구조로 설계되었다.  
이 프레임워크는 높은 동시성 환경에서 에이전트와 도구 간 상호작용을 효율적으로 관리하며, Kubernetes 기반 분산 클러스터를 통해 롤아웃 경험 생성의 병목을 해결한다.

# 5. [TCIA: A Task-Centric Instruction Augmentation Method for Instruction   Finetuning](https://arxiv.org/abs/2508.20374)

## Introduction
- Goal: 본 연구는 대형 언어 모델의 특정 작업 적응력을 높이기 위해 다양성과 과제 관련성을 동시에 유지하는 과제 중심 명령어 증강 방법인 TCIA(Task-Centric Instruction Augmentation)를 제안하는 것이다.  
- Motivation: 기존 자동 명령어 생성 방법들은 명령어 다양성은 확보하나 실제 적용되는 과제와의 관련성(task drift)을 간과하여 실무에서 요구되는 과제 특화 지식 반영이 어렵다는 문제가 존재한다.  
- Contribution: TCIA는 명령어를 구속조건과 쿼리로 분해하고, 작업 유형 기반 데이터베이스에서 유사 제약을 탐색하는 BFS 기반 증강 기법을 도입하여 명령어의 다양성 및 과제 적합성을 유지하면서도 실제 업무용 과제에서 기존 모델 대비 평균 8.7% 향상된 성능을 달성하였다.  

## Method  
TCIA는 자연어 명령어를 기본 쿼리와 명확히 구분된 제약조건 집합으로 분해하고, 관련 작업별 명령어 데이터베이스를 구축하여 의미 기반 임베딩으로 적합한 제약조건을 검색한다.  
이후 BFS 알고리즘을 통해 추가, 삭제, 교체 연산으로 제약조건을 다양하게 조합하고, 이를 다시 자연어 명령어

# 6. [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)

## Introduction
- Goal: 본 연구의 목표는 장시간의 동영상 생성에서 발생하는 장기 문맥 메모리 문제를 해결하는 것이다.  
- Motivation: 기존의 확산 트랜스포머 기반 모델은 자기주의(self-attention)의 제곱 복잡도 때문에 긴 시퀀스 처리에 어려움이 존재한다.  
- Contribution: 본 연구는 학습 가능한 희소 어텐션 라우팅 모듈인 Mixture of Contexts(MoC)를 제안하여 효율적이고 적응적인 장기 메모리 검색을 가능하게 하였다.  

## Method  
본 방법은 동영상 시퀀스를 프레임, 샷, 자막 단위로 의미론적으로 정렬된 청크로 나누고, 각 쿼리 토큰이 top-k 선택 방식을 통해 가장 관련성 높은 청크만을 선택하도록 학습한다. 텍스트 토큰과 샷 내 로컬 윈도우 토큰은 고정된 앵커로 포함하여 지역적 일관성을 보장하며, 인과적 라우팅 마스크를 적용해 순환 고리를 방지한다. 선택된 청크에 대해서만 플래시 어텐션 연산을 수행하여 메모리와 계산 비용을 선형 수준으로 줄이고 장시간 동영상 생성을 실현한다.  

## Results  
MoC는 180,

# 7. [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World   Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)

## Introduction
- 본 논문은 실제 복잡한 다단계 작업에서 도구를 활용하는 대규모 언어 모델(LLM) 에이전트를 평가하기 위한 벤치마크 MCP-Bench를 제안하였다.  
- 기존 벤치마크들은 단일 도메인, 명확한 도구 이름 제공, 짧은 작업 흐름 등 현실의 복잡한 도구 사용 시나리오를 제대로 반영하지 못한다는 한계가 있다.  
- MCP-Bench는 MCP 프로토콜 기반 28개 서버와 250개 도구를 연결하여, 모호한 지시문에서 도구 선택, 다중 목표·다중 도메인 작업 수행 능력 등을 평가할 수 있는 종합 플랫폼을 제공하였다.  

## Method  
MCP-Bench는 도구 간 입력-출력 종속성을 분석하여 실제적이고 해결 가능한 다단계 작업을 자동으로 합성하고, 이 작업들을 모호한 지시문으로 표현하여 에이전트의 도구 검색 및 계획 능력을 시험한다.  
평가에는 도구 이름 유효성, 스키마 준수, 실행 성공률을 확인하는 규칙 기반 검증과 LLM을 활용한 과제 완성도, 도구 사용 적합성, 계획 효율성 평가 루브릭을 병행 적용하였다.  
다수의 서버와 도

# 8. [CogVLA: Cognition-Aligned Vision-Language-Action Model via   Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)

## Introduction
- Goal: 본 논문은 인간 인지 과정을 모사한 명령어 기반 라우팅 및 희소화 기법을 활용하여 효율적이고 일관된 비전-언어-행동(Vision-Language-Action, VLA) 모델인 CogVLA를 제안하는 것을 목적으로 한다.  
- Motivation: 기존 VLA 모델들은 시각 정보를 충분히 압축하지 못해 큰 연산 비용을 초래하고 있으며, 비전-언어-행동 간 의미적 일관성 부족으로 성능 한계가 있었다.  
- Contribution: 본 연구는 3단계 점진적 구조(EFA-Routing, LFP-Routing, CAtten)를 통해 명령어에 따른 시각 정보 희소화와 교차 모달 논리 및 행동 연속성 유지를 가능케 하여 높은 효율성과 우수한 작업 성공률을 동시에 달성하였다.  

## Method  
CogVLA는 인간의 시각집중기(Visual and Attention System), 보조운동영역(Supplementary Motor Area), 전운동피질(Premotor Cortex)의 기능을 모방하여 명령어 주도형 인코더-LLM 라우팅과 시각 토큰 희소화를 수행한다. 첫 단계에서는 FiLM 모듈을 활용해 명령어에 맞게 시각 토큰을 25%

# 9. [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn   Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)

## Introduction
- Goal: 본 연구의 목표는 다중 회차 대화에서 대화 목표를 추적하고 시각화하여 사용자들이 대화 진행 상황을 효과적으로 평가하고 검토할 수 있도록 하는 것이다.  
- Motivation: 대형 언어 모델과의 긴 대화가 복잡해지면서 사용자가 대화 목표를 명확히 파악하고 진행 상태를 검토하기 어려운 문제를 해결할 필요가 있다.  
- Contribution: OnGoal이라는 목표 추적 및 시각화 기능이 포함된 대화 인터페이스를 제안하고, 이를 통해 사용자들이 목표 관련 정보를 실시간으로 확인하고 조작할 수 있도록 하였다.  

## Method  
OnGoal 시스템은 대화 목표를 추론, 병합, 평가하는 3단계 파이프라인을 통해 목표의 변화를 모델링한다.  
대화 내 목표 진행 상황을 인라인 기호와 별도의 진행 패널에서 시각적으로 제공하며, 텍스트 하이라이팅을 통해 LLM의 목표 대응 방식을 명확히 보여준다.  
사용자 연구를 통해 OnGoal의 목표 추적 시각화가 사용자들의 목표 관리와 대화 이해를 지원하며, 커뮤니케이션 전략 변화에 긍정적 영향을 미침을 확인하였다.  

## Results  
OnGoal 사용자는 목표 달성을 위해 소요하는 시간과 노력을 줄이고, 오해를

# 10. [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)

## Introduction
- Goal: 본 논문은 사용자가 원하는 의상을 착용하고 지정한 참조 영상의 동작을 따라 움직이는 고품질 5초 분량의 가상 착용 영상을 생성하는 Dress&Dance 비디오 확산 프레임워크를 제안한다.  
- Motivation: 기존 가상 의상 착용 기술은 주로 정적인 2D 이미지 생성에 국한되어 사용자가 의상을 착용한 상태에서의 동적인 움직임을 체험하는 데 한계가 있었다.  
- Contribution: 본 연구는 다중 모달 입력을 통합하는 CondNet 조건 네트워크, 합성 트리플릿 데이터 활용 및 다단계 점진적 학습 전략을 도입하여 높은 해상도의 동영상 기반 가상 착용을 가능케 했다.  

## Method  
Dress&Dance는 사용자 이미지, 의상 이미지, 참조 동영상, 선택적 텍스트 입력을 토큰 시퀀스로 인코딩하여 하나의 확산 백본 네트워크에 전달하며, CondNet 모듈을 통해 다중 모달 정보를 교차 주의 메커니즘으로 통합한다. 모델은 커리큘럼 기반 의상 워밍업과 점진적 해상도 증가 전략으로 단계별 학습을 수행하며, 8FPS 영상 출력을 24FPS로 고화질 보정하는 비디오 리파
