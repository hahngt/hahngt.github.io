---
layout: post
title: Daily Papers — 2025-08-29"
date: 2025-08-29 08:15:00
tags: [papers, arxiv, ai]
categories: []
---

# 1. [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)

## Introduction

- Goal: 본 연구는 텍스트-이미지 생성 분야에서 기존 점수 기반 강화학습의 불안정성을 해소하기 위해 쌍대 선호 보상 기반 GRPO 방법을 제안하는 것이다.
- Motivation: 기존의 점수 정규화 방식은 그룹 내 이미지 간 미미한 보상 점수 차이를 과대 해석하는 환상 우위 현상으로 인해 보상 해킹 문제와 최적화 불안정을 초래하였다.
- Contribution: 본 논문에서는 환상 우위 문제를 규명하고, 이를 극복하기 위해 쌍대 선호에 기반한 PREF-GRPO 방법과 세밀한 평가가 가능한 UNIGENBENCH 벤치마크를 함께 제안하였다.

## Method

PREF-GRPO는 기존 점수 최대화 방식 대신 그룹 내 모든 이미지 쌍을 선호 비교하여 각 이미지의 승률을 보상 신호로 활용함으로써 정규화에 따른 과도한 이점을 완화한다. 이 방법은 보상 분산을 증가시키고 보상 잡음에 둔감하며 인간의 상대적 판단과 일치하는 장점을 가진다. 또한, MLLM 기반의 자동화된 평가 파이프라인을 통해 UNIGENBENCH 벤치마크를 구축하여 세부 평가 기준에서의 안정적인 성능 측정을 지원한다.

## Results

PREF-GRPO는 UNIGENBENCH에서 기존 점수 최대화 기반 방법 대비 전체 점수 5.84%, 논리 추론 및 텍스트 일관성에서 각각 12% 이상 향상된 성능과 함께 보상 해킹 문제를 효과적으로 완화하였다.

## Limitations

본 연구는 환상 우위 문제 극복에 초점을 두었으나, 복잡한 논리 추론과 텍스트 렌더링 등 일부 과제에서는 여전히 개선의 여지가 존재한다.

## Conclusion

PREF-GRPO와 UNIGENBENCH는 텍스트-이미지 생성의 강화학습 안정성과 세밀한 평가를 동시에 달성하는 효과적인 접근법임이 실험을 통해 확인되었다.

# 2. [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)

## Introduction

- 본 연구의 목표는 agentic 강화학습을 활용하여 14억 파라미터 규모의 수학적 추론 모델 rStar2-Agent를 개발하고, 최첨단 수준의 문제 해결 능력을 달성하는 것이다.
- 기존의 긴 Chain-of-Thought(CoT) 방식은 복잡한 문제에 대해 중간 과정에서 발생하는 미묘한 오류를 탐지하거나 스스로 수정하는 데 한계가 있어, 보다 똑똑한 사고를 하는 고도화된 인지 능력 부여가 요구되었다.
- 본 논문에서는 대규모 agentic RL을 위한 효율적 인프라 구축, 환경 노이즈를 극복하는 GRPO-RoC 알고리즘, 그리고 최소한의 연산 자원으로 우수한 성능을 내는 다단계 RL 학습 절차를 제안하였다.

## Method

rStar2-Agent는 Python 코딩 도구를 이용하여 복잡한 문제 풀이 과정에서 중간 단계를 탐색·검증·수정하는 agentic 강화학습 방식을 도입하였다. 이를 위해 높은 병렬성을 가진 신뢰성 높은 코드 실행 환경과 GPU 리소스 활용을 극대화하는 동적 작업 스케줄러를 구축하였다. 또한, GRPO-RoC 알고리즘은 정답 기반 보상을 유지하면서 고품질 성공 경로를 선별, 환경 노이즈에 강인하게 강화학습을 진행하였다.

## Results

rStar2-Agent-14B는 단 510회의 RL 학습 만에 AIME24에서 80.6%, AIME25에서 69.8%, HMMT25에서 52.7%의 평균 정답률을 기록하며, 671억 파라미터 규모의 DeepSeek-R1 모델 성능을 뛰어넘음으로써 최첨단 수학 추론 능력을 보여주었다.

## Limitations

환경에서 발생하는 도구 호출 오류와 포맷 문제는 여전히 도전 과제로 남아 있으며, 일부 복잡한 단계에 대한 완전한 오류 수정 능력은 제한적이다.

## Conclusion

rStar2-Agent는 agentic 강화학습과 효율적인 대규모 인프라, 고도화된 RL 알고리즘 설계를 통해 최소한의 연산 비용으로도 최첨단 수학 추론 성능과 확장성을 동시에 달성하였다.

# 3. [USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning](https://arxiv.org/abs/2508.18966)

## Introduction

- Goal: 본 논문은 스타일 주도와 주제 주도 이미 생성 작업을 하나의 통합된 프레임워크로 결합하는 USO 모델을 제안하는 것이다.
- Motivation: 기존 연구는 스타일 유사성과 주제 일관성을 각기 다른 과제로 분리하여 상호 대립적인 문제로 인식하였으나, 스타일과 내용의 분리 및 재조합이라는 공통 주제로 통합 가능하다고 주장하였다.
- Contribution: 대규모 스타일-주제-스타일화 이미지 삼중체 데이터셋 구축, 교차 작업 공동 분리 학습, 스타일 보상 학습 도입, 그리고 스타일 유사도와 주제 일관성을 함께 평가하는 최초의 USO-Bench 벤치마크를 제안하였다.

## Method

USO는 스타일과 주제 조건을 위한 분리된 인코더와 점진적 스타일 정렬 및 내용-스타일 분리 학습으로 두 작업을 통합한다. 삼중체 데이터셋을 활용해 스타일-주제 공동 분리를 수행하며, 스타일 보상 학습(SRL)으로 성능을 추가 향상시켰다. 이러한 통합 접근법은 스타일과 주제 정보의 상호 보완적 학습을 가능하게 하여 각 작업에서 불필요한 특징을 효과적으로 배제한다.

## Results

USO는 USO-Bench 및 DreamBench 벤치마크에서 주제 일관성(CLIP-I, DINO)과 스타일 유사성(CSD) 측면에서 모든 개별 및 통합 과제에서 기존 최첨단 오픈소스 모델을 능가하는 성능을 보였다.

## Limitations

논문에서 직접 언급된 한계점은 없으나, 스타일과 주제를 완전히 분리하되 자유롭게 재조합하는 과정에 따른 복잡성 및 학습 비용 증가가 잠재적 과제로 추정된다.

## Conclusion

USO는 교차 작업 공동 분리 학습과 스타일 보상 학습을 바탕으로 스타일 주도, 주제 주도 및 이들의 결합 과제를 단일 모델에서 우수하게 수행하는 통합 생성 프레임워크임을 입증하였다.

# 4. [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)

## Introduction

- Goal: 본 논문은 Agentic AI를 위한 “learning from practice” 패러다임 구현을 위해 대규모 에이전트-환경 상호작용 시스템인 AWORLD를 제안하는 것이다.
- Motivation: 기존 복잡한 벤치마크인 GAIA에서 경험 생성이 비효율적이라는 병목 현상이 Agentic AI 발전을 크게 저해하기 때문이다.
- Contribution: AWORLD는 분산 구조를 통해 경험 생성 속도를 14.6배 가속하여 대규모 강화학습을 실현하고, 이를 바탕으로 Qwen3-32B 기반 에이전트를 성공적으로 훈련시켜 기존 모델 대비 GAIA 정확도를 크게 향상시켰다.

## Method

AWORLD는 에이전트 구성, 통신 프로토콜, 런타임 상태 관리, 훈련 조율의 네 가지 핵심 구성요소로 설계되었다. 분산 클러스터 환경에서 다중 에이전트 병렬 실행을 지원하여 고효율 경험 생성이 가능하며, 외부 강화학습 프레임워크와 유기적으로 연동된다. 에이전트는 다양한 도구와 환경을 활용하여 복잡한 장기 과제 해결을 목표로 하며, 실시간 통신 및 상태 관리를 통해 안정적인 작업 수행이 보장된다.

## Results

AWORLD 프레임워크를 활용한 Qwen3-32B-AWORLD 에이전트는 GAIA 벤치마크에서 기본 모델 대비 정확도를 21.59%에서 32.23%로 10.6%p 향상시켰으며, 최고 난이도 문제에서는 16.33%의 점수로 최상위 상용 모델을 능가하였다.

## Limitations

복잡한 환경에서 롤아웃 단계가 여전히 비용이 크고, 다중 에이전트 및 자율적 자기개선 시스템 구현은 향후 연구 과제로 남아 있다.

## Conclusion

AWORLD는 경험 생성 병목을 극복하여 Agentic AI의 “learning from practice”를 현실화하는 오픈소스 프레임워크로서, 실질적이고 확장 가능한 에이전트 훈련 파이프라인을 제시하였다.

# 5. [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)

## Introduction

- Goal: 본 연구는 대형 언어 모델의 특정 업무(task) 중심의 명령어(instruction) 증강 방법인 TCIA(Task-Centric Instruction Augmentation)를 제안하는 데 목적이 있다.
- Motivation: 기존 자동 명령어 생성 방법들은 명령어 다양성은 확보하나, 실제 업무에의 적합성(task relevance)을 유지하지 못하는 문제점이 존재한다.
- Contribution: TCIA는 명령어를 기반 질의어와 명확한 제약 조건으로 분해하고, BFS 탐색과 임베딩 기반 검색을 결합하여 명령어 다양성 및 업무 적합성을 동시에 개선하는 체계적 프레임워크를 개발하였다.

## Method

TCIA는 자연어 명령어를 쿼리와 구조화된 제약 조건으로 분해하고, 유사 업무 유형별 명령어 데이터베이스에서 제약 조건을 검색하여 BFS 기반으로 명령어 집합을 다각적으로 확장한다. 확장된 명령어는 LLM을 통해 자연어 문장으로 재구성 및 검증되고, 다중 LLM 평가를 통해 품질이 필터링된다. 최종적으로 생성된 고품질 명령어-응답 쌍은 감독 학습에 사용되어 모델을 특정 업무에 맞게 미세조정한다.

## Results

TCIA는 네 가지 실제 업무에 적용한 실험에서 고정 명령어, 기존 자동 증강법(WizardLM) 대비 평균 8.7%의 성능 향상을 보여주었으며, 일부 업무에서는 GPT-4o를 포함한 선도적 폐쇄형 모델을 능가하였다.

## Limitations

TCIA는 현재 다중 대화(turn) 상황과 멀티모달 학습 환경을 포함하지 않아 이러한 영역으로의 확장 가능성에 대한 연구가 요구된다.

## Conclusion

TCIA는 명령어 다양성과 업무 적합성을 동시에 확보하여 개방형 대형 언어 모델을 실제 과제 중심 환경에 효과적으로 적응시키는 강력하고 확장성 높은 방법임이 입증되었다.

# 6. [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)

## Introduction

- Goal: 본 연구의 목표는 긴 영상 생성 시 발생하는 장기 문맥 메모리 문제를 효율적으로 해결하는 것이다.
- Motivation: 기존의 확산 변환기(Dense self-attention)를 긴 문맥에 적용하면 계산과 메모리 비용이 제곱적으로 증가하여 실용적이지 못하다.
- Contribution: 본 논문은 학습 가능한 희소 어텐션 라우팅 모듈인 Mixture of Contexts(MoC)를 제안하여, 동적이고 적응적인 문맥 라우팅으로 긴 영상의 핵심 정보를 효과적으로 검색하고 유지할 수 있음을 보인다.

## Method

Mixture of Contexts는 시각 및 텍스트 토큰을 프레임, 샷, 캡션 단위로 의미론적으로 구분하여 청크를 구성하고, 각 쿼리가 이 중 가장 관련성 높은 상위 k개의 청크와 필수 앵커 토큰에만 어텐션하도록 학습된 파라미터 없는 토크 셀렉터를 활용한다.  
이때 인과적 라우팅 마스크를 도입하여 루프 폐쇄를 방지하며, 선택된 청크만 Flash-Attention을 통해 계산함으로써 메모리와 연산 비용을 거의 선형으로 줄인다.  
훈련 과정에서는 점진적인 청크 세분화와 라우팅 희소화를 통해 모델이 핵심 문맥에 집중하도록 유도하며, 라우팅의 견고성을 높이기 위해 문맥 드롭인/드롭오프 정규화를 적용한다.

## Results

MoC는 LCT 기반 장기 영상 생성 모델과 비교하여 85% 이상의 어텐션 토큰 페어를 제거하며 FLOPs를 7배 이상 절감하고, 2.2배의 생성 속도 향상과 함께 장기간의 행동 및 장면 일관성을 유지하거나 향상시킨다.

## Limitations

현재 MoC는 최대 수분 길이 영상에 대해 효과적임을 보였으나, 더 긴 시퀀스에 대한 효율성 및 적용 가능성은 추가 연구가 필요하다.

## Conclusion

학습 가능한 희소 어텐션 라우팅인 MoC는 장기 문맥 기억 문제를 극복하고 실용적 비용으로 수분 단위 길이 영상을 생성할 수 있는 새로운 확산 기반 영상 생성 패러다임을 제시한다.

# 7. [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)

## Introduction

- Goal: 본 연구는 대규모 언어 모델(LLM) 에이전트의 복잡한 실제 도구 활용 작업 수행 능력을 평가하기 위한 대규모 벤치마크 MCP-Bench를 제안한다.
- Motivation: 기존 벤치마크들은 도구 사용이 제한적이고, 명시적 도구 지정이나 단순한 다단계 작업에 국한되어 현실적 도구 조합과 긴 계획 능력을 충분히 평가하지 못하였다.
- Contribution: MCP-Bench는 28개 MCP 서버에 연결된 250개의 현실적인 도구를 활용하여 불명확한 지시를 통한 도구 탐색, 다중 목표 계획, 정보 근거 기반 추론, 그리고 교차 도메인 워크플로우 조정을 평가하는 체계적 도구 사용 벤치마크를 구축하였다.

## Method

MCP-Bench는 MCP 표준 프로토콜로 통합된 다양한 MCP 서버에서 도구 간 종속성 체인을 자동 탐색하고, 이를 기반으로 LLM을 통한 자연어 작업 지시를 합성한다.  
에이전트는 최대 20회차의 다중 턴 툴 호출을 통해 작업 목표를 달성하며, 규칙 기반 유효성 검사와 LLM 심판 평가를 결합하여 도구 스키마 이해, 계획 품질, 실행 성공률을 종합 평가한다.  
또한, 퍼지(fuzzy)한 작업 서술을 도입하여 명시적 도구 명칭 없이 의미 추론 및 실행 경로 생성 능력을 시험한다.

## Results

20개 최첨단 LLM 실험 결과, 도구 호출 및 스키마 준수는 대부분 모델에서 높은 수준으로 수렴되었으나, 긴 호라이즌 계획, 복잡한 다중 서버 워크플로우 및 정보 근거 추론 능력에서는 gpt-5, o3 등 상위 모델만이 월등한 성능을 보이며 핵심 역량 차이를 드러냈다.

## Limitations

본 벤치마크는 높은 작업 난이도와 복잡성으로 인해 일부 중소형 모델에서 수행 능력이 현저히 떨어지며, 도구 및 서버 간 의존성 해석과 실행 전략 최적화에서 여전히 개선 여지가 존재한다.

## Conclusion

MCP-Bench는 현실 세계의 복합 도구 생태계를 반영하여 LLM 에이전트의 도구 사용, 계획 수립 및 교차 도메인 협업 능력을 체계적으로 평가하는 표준화되고 확장 가능한 플랫폼임이 입증되었다.

# 8. [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)

## Introduction

- Goal: 본 연구는 인간의 인지 조율 원리를 모방하여 명령어 기반 라우팅과 희소화를 통한 인지 정렬형 시각-언어-행동(Vision-Language-Action, VLA) 모델인 CogVLA를 제안하는 것을 목표로 한다.
- Motivation: 기존 VLA 모델들은 대규모 후학습과 고비용 계산으로 확장성과 실시간 조작에 한계가 있으며, 시각, 언어, 행동 간 의미적 연계성을 충분히 고려하지 못해 효율성과 일관성 저하 문제가 존재하였다.
- Contribution: 본 논문에서는 EFA-Routing, LFP-Routing, CAtten의 3단계 인지 정렬 설계를 통해 명령어 기반 시각 입력 압축과 토큰 희소화, 그리고 행동 생성을 위한 교차 모달 논리적 일관성을 극대화하는 CogVLA 프레임워크를 제안하였다.

## Method

CogVLA는 인간의 시각 주의 체계, 보조 운동 영역, 전운동 피질 기능에서 영감을 얻은 3단계 구조로, 첫째로 Encoder-FiLM 기반 집합 라우팅(EFA-Routing)을 통해 명령어에 따라 시각 토큰을 25%로 압축한다. 둘째로 LLM-FiLM 기반 가지치기 라우팅(LFP-Routing)을 통해 의미적으로 무관한 시각 토큰을 선택적으로 제거하여 대량 토큰 희소화를 실현한다. 셋째로 V-L-A 결합 어텐션(CAtten) 모듈이 압축된 멀티모달 표현을 활용하여 시간적 행동 일관성과 명령어 조건의 시각-언어 추론을 유지하며 병렬 행동 디코딩을 수행한다.

## Results

LIBERO 벤치마크와 실제 로봇 조작 과제에서 CogVLA는 97.4%의 높은 성공률과 함께 OpenVLA 대비 학습 비용을 2.5배, 추론 지연 시간을 2.8배 절감하는 우수한 성능과 효율성을 입증하였다.

## Limitations

본 연구는 네트워크 지연이 존재하는 원격 통신 기반 실행 환경에서 평가되어, 향후 고성능 GPU 장치에서 로컬 실행을 통한 추가 지연 절감 연구가 필요하다.

## Conclusion

CogVLA는 명령어 구동형 희소화 및 라우팅을 통한 인지 정렬 VLA 모델로서, 효율성과 멀티모달 의미 일관성을 동시에 향상시켜 확장 가능한 자율 지능형 로봇 제어에 기여한다.

# 9. [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)

## Introduction

- Goal: 본 연구는 대화형 대형 언어 모델(LLM)과의 다중 회차 대화에서 대화 목표를 추적하고 시각화하여 사용자가 목표 진행 상황을 효과적으로 평가하고 검토할 수 있도록 하는 시스템을 개발하는 데 목적이 있다.
- Motivation: 다중 회차 대화가 길어지고 복잡해짐에 따라 사용자가 대화 목표를 명확히 관리하지 못하고 반복적인 프롬프트 입력이나 대화 재시작으로 이어지는 문제를 해결할 필요가 있다.
- Contribution: OnGoal이라는 실시간 피드백과 목표 시각화를 지원하는 LLM 챗 인터페이스를 제안하고, 이를 통해 사용자들이 목표 관리를 개선하는 방법과 향후 디자인 시사점을 도출하였다.

## Method

OnGoal은 사용자 메시지에서 질문, 요청, 제안 등의 대화 목표를 추론하고 유사 목표를 병합하며, 각 LLM 응답에 대해 목표 달성 여부를 평가하는 3단계 목표 파이프라인을 활용한다.  
대화 화면 내에서는 인라인 목표 표시, 상세 목표 진단, 그리고 목표 진행 상황을 시계열로 요약하는 별도의 패널을 제공하여 복잡한 대화를 효율적으로 관리할 수 있도록 하였다.  
또한, 텍스트 하이라이트 기능을 도입해 사용자가 LLM 응답 내에서 목표와 관련된 핵심 구절을 빠르게 파악하도록 지원한다.

## Results

20명의 참가자를 대상으로 한 사용자 연구에서, OnGoal을 사용한 그룹은 목표 달성에 소요되는 시간과 노력이 줄어들었으며, 목표 추적과 시각화가 LLM 대화에서 사용자 참여도와 회복력을 향상시키는 것으로 나타났다.

## Limitations

현재 OnGoal은 전역 목표 추적 위주로 설계되어 있어 문단이나 문장 단위의 세부 목표 관리 등 세분화된 대화 목표 추적은 지원하지 않는다.

## Conclusion

OnGoal은 다중 회차 LLM 대화에서 대화 목표의 추론, 병합, 평가, 시각화를 통합하여 사용자의 목표 관리와 대화 경험을 개선함으로써 미래 LLM 챗 인터페이스 디자인에 유용한 방향성을 제공한다.

# 10. [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)

## Introduction

- Goal: 본 연구는 사용자가 선택한 의상을 착용하고 지정한 동작으로 움직이는 5초 길이의 고해상도 동영상 가상 착용을 생성하는 Dress&Dance 비디오 확산 프레임워크를 제안하는 것이다.
- Motivation: 기존 가상 착용 방법은 정적인 2D 이미지 생성에 한정되어 있어 실제 착용감을 충분히 표현하지 못하며, 텍스트만으로는 복잡한 동작을 묘사하기 어려워 정확한 모션 제어가 힘들다.
- Contribution: 다양한 의상과 모션을 동시에 처리하고 다양한 입력 방식을 통합하는 CondNet 조건화 네트워크와 다단계 점진적 학습 전략을 도입하여 뛰어난 착용 품질과 모션 충실도를 달성하였다.

## Method

Dress&Dance는 사용자 이미지, 의상 이미지, 참조 모션 영상 및 선택적 텍스트를 입력받아 주의(attention) 기반의 CondNet으로 다중 모달 조건을 통합하고, 8FPS 비디오를 생성한 후 후처리 영상 정제기로 24FPS 고해상도 동영상을 생성한다.  
의상 등록을 위한 커리큘럼 학습 기반 웜업 단계와 해상도 점진 상승 훈련 전략으로 제한된 데이터와 컴퓨팅 자원 하에서도 효과적으로 훈련된다.  
합성된 트리플릿 데이터 구성으로 기존 중간 표현에 의존하지 않고 학습과 추론 간의 간극을 해소하였다.

## Results

Dress&Dance는 다중 의상 동시 착용, 타인의 의상 전이 등 다양한 상황에서 기존 공개 소스 및 상용 모델 대비 높은 PSNR, SSIM, LPIPS 및 GPT 기반 평가 지표에서 우수한 착용 충실도와 모션 표현력을 보였다.

## Limitations

복잡한 모션을 텍스트로만 표현하는 기존 방법과 달리, Dress&Dance는 참조 영상에 의존하여 모션을 제어하므로 참조 영상 선택에 제한이 존재한다.

## Conclusion

본 연구는 고해상도 영상 환경에서 다중 모달 조건을 통합하는 영상 확산 기반 가상 착용과 모션 생성 프레임워크를 최초로 제안하여 실용적인 동영상 가상 착용 경험을 가능하게 하였다.
