---
layout: post
title: "Daily Papers — 2025-11-18"
date: 2025-11-18 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.13655)

## Introduction
- Goal: 본 연구는 지구 관측 분야에 적합한 안정적이고 효율적인 다중 모달 위성 영상 기반 기초 모델 OlmoEarth를 개발하는 데 목적이 있다.  
- Motivation: 기존 지구 관측 기초 모델들은 대규모 학습과 복잡성으로 인해 비영리 및 인도주의 단체에서 실질적 적용에 어려움이 존재한다.  
- Contribution: 안정적인 자체 지도 학습 기법과 모달리티 인지 마스킹, 제약적인 손실 함수를 도입하여 OlmoEarth를 학습하고, 다양한 벤치마크 및 실제 파트너 과제에서 최첨단 성능을 달성하며 공개 플랫폼으로 배포하였다.  

## Method  
OlmoEarth는 Vision Transformer 기반 인코더-디코더 구조로, 공간, 시간, 모달리티를 통합하여 위성 이미지와 고품질 지도 데이터를 처리한다.  
학습에는 고정 랜덤 투영을 활용한 Latent MIM Lite 기법과 모달리티 인지 마스킹 전략, 패치 판별 대비 손실 및 인스턴스 대비 손실이 사용된다.  
이를 통해 자체 및 지도 학습을 일관된 방식으로 수행하며, 지구 관측 데이터의 특성을 효과적으로 반영하였다.  

## Results  
OlmoEarth는 12개 다른 기초 모델과 비교 평가에서 kNN 및 선형 프로빙 기준 24개 과제 중 15개, 전체 미세조정 기준 29개 과제 중 19개에서 최고 성능을 기록하였다.  

## Limitations  
OlmoEarth 대형 모델은 일부 시계열 기반 임베딩 과제에서 중간 크기 모델보다 성능이 낮은 경우가 관찰되어 대규모 확장 시 학습 복잡성 문제가 존재한다.  

## Conclusion  
OlmoEarth는 안정적인 학습 기법과 포괄적 평가를 통해 지구 관측 분야에 최적화된 기초 모델을 제시하며, 공개 플랫폼을 통해 실제 사회 문제 해결에 기여하도록 설계되었다.

# 2. [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.02767)

## Introduction
- 본 연구의 목표는 동영상과 텍스트 간의 표현 정렬을 통해 현대적 비디오 및 언어 인코더의 능력을 체계적으로 탐구하는 것이다.  
- 동영상 데이터가 갖는 시간적 특성을 고려한 다중 모달 표현 정렬은 이미지-텍스트 정렬에 비해 상대적으로 미흡하게 다루어져 왔다.  
- 본 연구는 다중 프레임 및 다수의 캡션 활용을 통해 테스트 시 정렬 성능을 크게 향상시키는 스케일링 법칙을 제안하고, 정렬 정도와 다운스트림 과제 성능 간 상관관계를 규명하였다.  

## Method  
서로 다른 모달리티인 비디오와 텍스트 임베딩 공간에서 상호 k-최근접 이웃(MkNN) 매트릭스를 사용해 정렬 점수를 계산한다.  
비디오에서는 다중 프레임을, 텍스트에서는 다중 캡션을 조합하여 각각의 표현을 평균화하는 방식으로 풍부한 정보를 반영한다.  
이후 비디오-텍스트 쌍을 활용해 정렬 지수를 평가하고, 파라메트릭 테스트 시간 스케일링 모델을 제안하여 데이터양에 따른 정렬 변화를 정량적으로 모델링하였다.  

## Results  
자기지도 학습 기반 비디오 인코더(VideoMAEv2)는 최신 이미지 인코더(DINOv2)를 능가하는 수준의 비디오-텍스트 정렬을 달성하였으며, 프레임 수 및 캡션 수 증가가 정렬 점수 향상에 크게 기여함을 확인하였다.  

## Limitations  
동영상과 텍스트의 시간적 정렬에서는 현재 언어 모델이 순서 민감성이 낮아 정렬 세부에서 한계를 가지며, 일부 시각-언어 모델 모두 시간 재배열에 취약한 점이 남아있다.  

## Conclusion  
비디오-텍스트 정렬은 시공간 정보가 포함된 비디오 표현의 일반적 이해 능력을 제로샷으로 효과적으로 평가하는 유용한 지표로 활용될 수 있다.

# 3. [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.13714)

## Introduction
- Goal: 본 논문은 인간의 주석 없이도 모든 수준의 세분화(Granularity)를 자유롭게 제어할 수 있는 자가 지도 학습 기반 분할 모델인 UNSAMv2를 제안하는 데 목적이 있다.  
- Motivation: 기존의 Segment Anything Model(SAM)은 제한된 세분화 수준과 인간 주석에 의존하는 한계로 인해 다양한 객체 수준에서 연속적이고 유연한 분할 제어가 어렵다.  
- Contribution: UNSAMv2는 자가 지도 학습을 통해 다양한 세분화 수준을 포함하는 위계적 가상 라벨을 생성하고, 이를 기반으로 연속적이고 정확한 세분화 조절이 가능한 경량 모듈을 SAM-2에 통합하여 적은 양의 라벨 없는 데이터로 첨단 성능을 달성하였다.  

## Method  
UNSAMv2는 Normalize-cut 기반의 MaskCut으로 인스턴스 마스크를 자동 발견하고, 유사 픽셀 병합을 반복 적용하여 위계적 마스크를 생성한다. 각 마스크에 상대적 크기를 기준으로 연속적인 세분화 스칼라를 할당하고, 이를 Fourier 임베딩으로 변환해 단일 포인트 프롬프트와 결합하여 세분화 해상도를 제어하는 경량 토큰을 마스크 디코더에 도입한다. 기존 SAM-2의 학습 절차는 동결하고 일부 디코더 파라미터만 미세조정하여 효율적으로 학습시킨다.  

## Results  
UNSAMv2는 6,000장의 라벨 없는 이미지 학습만으로 SAM-2 대비 인터랙티브 세분화 전반에서 10% 이상 향상된 클릭 수 및 IoU 성능을 보이며, 11개 이상의 벤치마크에서 최첨단 결과를 달성하였다.  

## Limitations  
UNSAMv2는 여전히 일부 대규모 인간 주석 데이터와 결합한 경량 지도 학습인 UNSAMv2+에서 성능 향상이 이루어지는 등 완전 무감독 방식만으로 한계가 존재한다.  

## Conclusion  
본 연구는 자가 지도 분할 학습이 비지도 데이터에서 위계적 세분화 구조를 발견하고 연속적이고 제어 가능한 객체 분할을 가능케 한다는 것을 입증하며, 분할을 단순한 예측이 아닌 조절 가능한 추론 문제로 전환하였다.
