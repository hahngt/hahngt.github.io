---
layout: post
title: "Arxiv - 2025-09-01"
date: 2025-09-01 08:15:00
tags: [papers, arxiv, ai]
categories: []
---

# 1. [DriveQA: Passing the Driving Knowledge Test](http://arxiv.org/abs/2508.21824v1)

## Introduction

- Goal: 본 연구는 대형 언어 모델(LLM)과 멀티모달 LLM(MLLM)이 운전 지식 시험을 통과할 수 있는지를 평가하기 위한 종합적인 텍스트 및 시각 기반 주행 지식 벤치마크인 DriveQA를 제안하는 데 목적이 있다.
- Motivation: 기존 자율주행 벤치마크는 주로 공간 인식과 시각적 이해에 집중하여 교통 법규 및 복잡한 우선 통행 규칙과 같은 실제 운전 필수 지식을 충분히 평가하지 못하는 한계가 존재한다.
- Contribution: DriveQA는 미국 전역의 공식 교통 수칙을 기반으로 19개 질문 유형과 220종의 교통 표지판을 포함한 47만 개 이상의 샘플을 구축하고, LLM과 MLLM의 규칙 이해, 수치 추론, 시각·공간적 판단 능력을 심층 분석하였다.

## Method

DriveQA는 텍스트 기반의 DriveQA-T와 이미지·텍스트 기반의 DriveQA-V로 구성되며, 전자는 공식 운전 핸드북에서 생성된 텍스트 질문을, 후자는 CARLA 시뮬레이터와 실제 데이터를 활용하여 다양한 환경 변화를 반영한 교통표지판 및 교차로 우선 통행 관련 질문을 포함한다.  
질문은 BERT 임베딩과 계층적 군집화를 활용해 유형별로 분류하였으며, 모델 평가는 체인 오브 싱크(Chain of Thought)와 정보 검색 기반 기법(Retrieval-Augmented Generation)을 적용하였다.  
또한, LoRA 기법으로 모델을 효과적으로 미세조정하여 규칙 인식과 복잡한 상황 판단에서 성능 향상을 도모하였다.

## Results

DriveQA를 통해 평가한 결과, GPT-4o를 포함한 최첨단 LLM 및 MLLM은 기본 교통 규칙에서는 우수한 성능을 보였으나, 수치적 정밀도, 복잡한 우선 통행 판단, 시각적 환경 변화 및 공간 인식 능력에서 여전히 제한적이며, DriveQA 기반 미세조정과 사전학습이 이러한 약점을 크게 개선하는 것으로 나타났다.

## Limitations

본 벤치마크는 정적 교통 규칙 중심으로 구성되어 있으며, 동적 영상 기반 주행 상황 및 수치 추론과 공간 인식을 강화할 수 있는 보다 진보된 기법과 데이터 다양성 확보가 필요하다.

## Conclusion

DriveQA는 운전 지식에 대한 모델의 이해도를 체계적으로 평가하고, 다중모달 학습 모델의 실제 운전 환경 적용 가능성을 향상시키기 위한 중요한 자원 및 평가 기준을 제공한다.

# 2. [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](http://arxiv.org/abs/2508.21815v1)

## Introduction

- Goal: 본 연구는 Rényi 미분 개인정보 보호(Rényi Differential Privacy, RDP)를 적용하여 공정성과 개인정보 보호가 보장되는 합성 데이터 생성을 달성하는 방법을 제안하는 데 목적이 있다.
- Motivation: GDPR, HIPAA, AI 법령 등 개인정보 보호 및 AI 윤리 규제가 강화되면서 민감 데이터 분야에서 위험 인지형 데이터 공유 및 모델 개발을 위한 합성 데이터 생성이 필요해졌다.
- Contribution: FLIP(Fair Latent Intervention under Privacy guarantees)라는 변형된 변분 오토인코더 기반 모델을 통해 하위 작업에 종속되지 않는 공정성 확보와 RDP 호환 균형 샘플링으로 그룹별 맞춤 노이즈를 적용하는 방법을 제안하였다.

## Method

FLIP은 두 단계 학습으로 고품질 데이터 표현을 먼저 학습하고 이후 잠재 공간에서 Centered Kernel Alignment(CKA)를 활용하여 보호 속성과의 독립성을 확보해 공정성을 도모한다. 또한, RDP를 적용한 DP-SGD를 통해 개인정보 보호를 보장하며, 그룹별 샘플링 비율에 따른 노이즈 조정으로 균형 잡힌 학습 배치를 구성한다. 다변량 이질형 테이블 데이터에 적합한 트랜스포머 기반 변분 오토인코더와 잠재 공간 확산모델을 결합하였다.

## Results

실험 결과, FLIP은 다양한 공개 데이터셋과 여러 하위 작업에 대해 차등 개인정보 보호 제약 하에서 공정성을 유의미하게 개선하면서 합성 데이터의 품질도 적절히 유지함을 보여주었다.

## Limitations

적용한 공정성 개념은 보호 속성과 데이터 표현 간 완전한 독립성을 목표로 하여 특정 하위 작업에서 최적이 아닐 수 있다.

## Conclusion

본 연구는 하위 작업 독립적 공정성과 Rényi 차등 개인정보 보호를 동시에 만족하는 합성 데이터 생성 모델로서 FLIP의 유효성을 입증하였다.

# 3. [VoCap: Video Object Captioning and Segmentation from Any Prompt](http://arxiv.org/abs/2508.21809v1)

## Introduction

- Goal: 본 논문은 다양한 형태의 프롬프트(텍스트, 박스, 마스크)를 입력으로 받아 영상 내 객체의 시공간 분할 마스크와 객체 중심의 캡션을 동시에 생성하는 영상 객체 분할 및 캡션 생성 모델 VoCap을 제안한다.
- Motivation: 기존 컴퓨터 비전 시스템은 객체의 시공간적 위치정보와 자연어를 통한 의미적 이해를 동시에 제공하지 못하며, 이에 따른 데이터 수집은 비용과 시간이 많이 소요된다.
- Contribution: 본 연구는 기존 대규모 세분화 영상 데이터셋에 비전-언어 모델을 활용한 자동 객체 캡션 생성 기법을 적용하여 SAV-Caption 데이터셋을 구축하고, 이와 다양한 데이터셋을 통합 학습한 VoCap 모델이 영상 내 객체 분할과 캡션생성에서 최첨단 성능을 달성함을 보였다.

## Method

VoCap은 프레임별 영상 특징 추출과 기억 모듈을 통한 시공간 정보 통합, 위치 및 텍스트 프롬프트 인코딩, 마스크 디코더와 텍스트 디코더로 구성된다. 본 모델은 기존 SAM2와 BLIP2의 구성에서 언어 인코더 및 디코더를 추가해 텍스트 입력과 캡션 출력을 모두 처리하며, 시공간적 객체 분할과 세밀한 텍스트 설명을 동시에 생성한다. 자동 캡션 데이터 생성에는 Gemini 1.5 Pro Vision VLM을 이용해 객체 마스크 강조 및 배경 블러 처리한 영상을 입력으로 하여 고품질 의사 라벨을 생성하였다.

## Results

VoCap은 영상 객체 캡션팅과 분할을 모두 수행하는 최초 모델로써 SAV-Caption 검증셋에서 기존 방법 대비 캡션 성능(CIDEr 지수)을 크게 향상시키고, Referring Expression Video Object Segmentation 및 세미 감독 학습 기반 영상 객체 분할에서 최첨단 성능을 기록하였다.

## Limitations

영상 내 첫 프레임에서 객체가 명확히 식별되지 않거나 가려진 경우 모델의 추적 편향 문제가 존재한다.

## Conclusion

본 연구는 다양한 형태의 프롬프트를 수용하며 영상 내 객체의 시공간 분할 및 기술을 통합해 수행하는 VoCap모델과 SAV-Caption 데이터셋을 제안함으로써 정밀한 시공간적 영상 이해 연구의 기반을 마련하였다.

# 4. [Tree-Guided Diffusion Planner](http://arxiv.org/abs/2508.21800v1)

## Introduction

- Goal: 본 논문은 pretrained diffusion 모델을 활용한 테스트 시점 가이드 기반 제어 문제 해결을 위한 계획 방법을 제안하는 것을 목표로 한다.
- Motivation: 기존의 gradient guidance는 볼록 및 미분 가능한 보상 함수에 최적화되나, 비볼록 목표, 비미분 제한 조건, 다중 보상 구조가 존재하는 실제 문제에서 효율이 크게 떨어진다.
- Contribution: 탐색과 착취를 균형 있게 수행하는 구조적 경로 생성을 통해 제약된 탐색 공간을 확장하고, zero-shot 테스트 시점 계획을 가능하게 하는 Tree-guided Diffusion Planner (TDP)를 제안하였다.

## Method

TDP는 테스트 시점 계획을 트리 탐색 문제로 정의하며, (1) training-free particle guidance를 이용해 다양한 부모 경로를 생성함으로써 광범위한 탐색을 수행하고, (2) 빠른 조건부 노이즈 제거 과정을 통해 서브 경로를 목표 함수에 맞게 정제한다. 이중 샘플링 구조로 비볼록, 비미분 목표들에 대응 가능하며, pretrained 모델과 테스트 시점 보상 신호만을 활용한다. 상태를 관찰 상태와 제어 상태로 자동 분해하며, 이를 통해 계획의 유연성과 확장성을 확보하였다.

## Results

TDP는 Maze gold-picking, 로봇팔 블록 조작, AntMaze 다중 목표 탐색 등 세 가지 다양한 테스트 환경에서 최신 기법 대비 일관되게 우수한 성능을 달성하였다.

## Limitations

복잡한 테스트 시점 목표에 대해 최적 가이드 강도를 찾는 데 태스크별 조율이 필요하며, 이 과정에서 연산 오버헤드가 존재한다.

## Conclusion

TDP는 pretrained diffusion 모델을 기반으로 한 zero-shot 계획 프레임워크로, 복잡 다변량의 실제 계획 문제를 효과적으로 해결하며 기존 지도학습 의존 방식을 극복하였다.

# 5. [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](http://arxiv.org/abs/2508.21793v1)

## Introduction

- Goal: 본 연구는 의료 분야에서 다양한 결측 및 이질적인 다중모달 데이터를 활용하여 견고한 임상 예측을 수행하는 Mixture of Experts (MoE) 프레임워크인 MoE-Health를 제안하는 데 목적이 있다.
- Motivation: 기존 다중모달 예측 모델은 완전한 모달리티 데이터가 필요하거나 수동 선택에 의존하여 실제 임상 현장의 불완전하고 다양한 데이터 가용성을 반영하지 못하는 한계가 존재하였다.
- Contribution: MoE-Health는 모달리티별 전문가 네트워크와 동적 게이팅 메커니즘을 통해 가용한 데이터에 따라 적절한 전문가를 선택·융합하며, 불완전한 데이터 환경에서 우수한 임상 예측 성능과 견고성을 달성하였다.

## Method

MoE-Health는 세 가지 주요 모달리티(EHR, 임상 기록 텍스트, 흉부 X선 영상)를 각각 전용 인코더로 임베딩한 후, 결측 모달리티에 대해 학습 가능한 결측 표시 임베딩을 사용하여 일관된 입력 표현을 생성한다.  
이후 미리 학습된 모달리티 조합별 전문가 네트워크 풀과 동적 게이팅 네트워크를 통해 입력 샘플에 최적화된 전문가 조합을 선택, 가중합으로 예측 결과를 산출한다.  
학습은 이진 분류 태스크에 대해 교차 엔트로피 손실과 전문가 활용 균형화 보조 손실을 결합한 형태로 수행된다.

## Results

MIMIC-IV 데이터셋의 세 가지 임상 예측 태스크(입원 사망, 입원 기간 장기화, 재입원 예측)에서 MoE-Health는 기존 다중모달 융합 방법들을 AUROC 및 F1 점수 기준으로 일관되게 능가하는 성과를 보였다.

## Limitations

본 연구는 단일 기관 데이터셋에 기반하여 평가되었으며, 타 기관 데이터 및 추가 모달리티에 대한 일반화 성능 평가는 향후 연구로 남아 있다.

## Conclusion

MoE-Health는 결측 및 이질적인 다중모달 의료 데이터를 효과적으로 융합하여 다양한 임상 환경에서 견고하고 유연한 예측 성능을 제공하는 혁신적 프레임워크임이 입증되었다.

# 6. [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](http://arxiv.org/abs/2508.21732v1)

## Introduction

- Goal: 본 연구의 목표는 디지털 측정 장치(DMD)의 값 판독을 지원하기 위해 대형 비전-언어 모델(LVLM)을 미세 조정하는 데 활용 가능한 합성 데이터 생성 도구인 CAD2DMD-SET을 제안하는 것이다.
- Motivation: 기존 LVLM은 복잡한 현실 환경에서 DMD 판독에 어려움을 겪으며, 이에 적합한 공개 데이터셋의 부재가 문제로 작용한다.
- Contribution: 본 연구는 CAD2DMD-SET 도구 개발, 실제 주석이 달린 유효성 검증 세트 DMDBench 제공, CAD2DMD-SET으로 생성한 대규모 합성 데이터셋을 통한 LVLM 미세 조정, 그리고 이를 통한 실험적 성능 향상을 증명하였다.

## Method

CAD2DMD-SET은 CAD 기반 3D 모델과 고품질 이미지 합성을 결합하여 다양한 렌더링 조건과 표지를 포함하는 합성 DMD 데이터셋을 생성한다. 데이터 생성 과정은 디스플레이 생성, Blender 기반 렌더링, 리얼한 배경과의 이미지 합성 세 모듈로 구성된다. 최종 데이터셋은 VQA 라벨을 포함하며 LVLM 미세 조정에 활용된다.

## Results

세 가지 최신 LVLM을 DMDBench에서 평가한 결과, CAD2DMD-SET 데이터로 미세 조정한 모델은 ANLS 지표 기준 최대 200%까지 성능이 향상되었으며, 다른 과제에서의 성능 저하 없이 견고성이 크게 개선되었다.

## Limitations

본 연구는 합성 데이터의 사실감 향상, 평가 척도 다변화 및 비알파벳 판독에 대한 OCR 한계 등에서 개선이 필요하다.

## Conclusion

CAD2DMD-SET은 DMD 판독 능력 강화를 위한 합성 데이터 제작에 효과적이며, 현실적 조건 하에서 LVLM 성능 개선에 기여한다.

# 7. [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](http://arxiv.org/abs/2508.21712v1)

## Introduction

- 본 연구의 목표는 소량의 데이터를 보유한 상황에서도 효율적으로 객체 검출을 위한 합성 데이터를 생성하는 경량화된 방법인 FLORA를 제안하는 것이다.
- 대용량 전체 미세조정 방식이 요구하는 높은 연산 자원과 데이터 요구량의 문제를 해결하고자 하는 동기가 존재한다.
- FLORA는 Flux 1.1Dev 확산 모델과 Low-Rank Adaptation(LoRA)을 결합하여 적은 계산 자원과 데이터로도 기존 대비 뛰어난 객체 검출 성능을 달성하는 효율적 합성 데이터 생성 파이프라인을 제시하였다.

## Method

FLORA는 두 단계로 구성되는데, 첫 단계에서는 각 객체 클래스별로 30개의 객체 크롭 이미지를 이용해 LoRA 모듈을 미세조정한다.  
두 번째 단계에서는 미세조정된 LoRA 모듈을 활용해 원본 이미지 내 객체 영역을 마스크 처리한 후, 해당 영역에 고품질 합성 객체를 생성하는 마스크 기반 인페인팅을 수행한다.  
이 과정은 소비자용 GPU에서도 실행 가능하도록 설계되어 데이터와 계산 비용에서 크게 효율적이다.

## Results

FLORA는 6개 공개 벤치마크 데이터셋에서 5,000개 합성 이미지로 학습된 기존 최고 성능기반 ODGEN 대비 10분의 1 규모인 500개 합성 이미지만으로도 5개 데이터셋에서 더 우수한 mAP@[.50:.95] 성능을 달성하였다.

## Limitations

MRI 데이터셋에서는 ODGEN이 소폭 더 높은 mAP를 보이는 등 일부 도메인에서 최고 성능을 완전히 대체하지는 못하였다.

## Conclusion

FLORA는 경량화된 LoRA 기반 미세조정과 정밀한 마스크 인페인팅으로 소량 데이터 환경에서도 고품질 합성 데이터를 저비용으로 생성하여 객체 검출 성능을 크게 개선할 수 있음을 입증하였다.

# 8. [Is this chart lying to me? Automating the detection of misleading visualizations](http://arxiv.org/abs/2508.21675v1)

## Introduction

- Goal: 본 연구는 시각화 차트의 왜곡 및 오해를 자동으로 탐지하는 방법을 제안하는 데 목적이 있다.
- Motivation: 사회관계망서비스 및 웹상에서 차트 디자인 원칙 위반으로 인한 오해의 소지가 있는 시각화가 잘못된 정보 확산을 유발하는 문제가 존재한다.
- Contribution: 12가지 유형의 오해 유발 요소가 주석된 2,604개의 실제 시각화 자료집 Misviz와 81,814개 합성 시각화 자료집 Misviz-synth를 공개하고, 이를 이용한 자동 탐지 모델 평가를 수행하였다.

## Method

본 연구는 실제와 합성 시각화 데이터셋에서 다중 레이블 분류 문제로 왜곡 시각화를 인식하는 방식을 채택하였다. Matplotlib 기반 합성 데이터에서 축 메타데이터를 추출하고 이를 활용하는 룰 베이스 검사기와 이미지 및 축 정보를 결합한 미세조정 분류기를 제안하였다. 또한 최첨단 다중모달 대형언어모델(MLLM)을 포함한 세 가지 접근법을 비교 평가하였다.

## Results

MLLM은 실제 시각화 데이터에서 우수한 성능을 보였으나, 합성 데이터에서는 축 정보에 의존하는 룰 검사기와 미세조정 분류기가 더 높은 정확성을 나타내어 데이터 유형에 따라 모델 성능 차이가 컸다.

## Limitations

합성 데이터셋의 차트 유형 및 왜곡 유형 다양성이 제한적이며, 축 추출기의 실제 사례 일반화가 미흡하여 일부 오류가 발생하였다.

## Conclusion

본 연구는 왜곡 시각화 탐지를 위한 최초의 대규모 공개 실제 및 합성 데이터셋을 제공하며, MLLM과 룰 베이스 및 미세조정 분류기의 장단점을 입증함으로써 후속 연구 기반을 마련하였다.

# 9. [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](http://arxiv.org/abs/2508.21581v1)

## Introduction

- Goal: 명확세포 신세포암(ccRCC) 환자의 수술 후 재발 위험을 개인별로 예측하기 위해 병리학적 전자슬라이드 이미지(WSI)와 수술 전 컴퓨터단층촬영(CT) 이미지를 통합하는 다중 모달 예측 모델을 개발하는 것이다.
- Motivation: 기존 Leibovich 점수는 병리학적 일부 변인만 사용해 재발 위험군을 분류하지만, 환자 단위 정밀도는 한계가 있으며 영상 정보를 포함하지 못한다.
- Contribution: 본 연구는 사전학습된 병리 및 CT 영상 인코더 기반의 모듈형 딥러닝 프레임워크와 중간 결합(intermediate fusion) 전략을 도입하여 임상 기반 점수와 견줄 만한 성능을 실증하였다.

## Method

본 연구는 TCGA-KIRC 코호트에서 156명의 ccRCC 환자 데이터(CT, WSI, 임상자료)를 활용하였다. 병리 WSI는 Trident 프레임워크와 CONCH–TITAN 조합 인코더로, CT 영상은 MedicalNet과 SwinUNETR 인코더를 통해 특징 벡터로 추출하였다. 이들 특징은 Cox 기반 신경망으로 재발 위험 점수로 매핑하며, 단일 모달, 후기 결합(late fusion), 중간 결합(intermediate fusion) 방식을 비교 평가하였다.

## Results

중간 결합 전략을 적용한 TITAN–CONCH 병리 인코더와 ResNet-18 CT 인코더 조합 모델이 C-index 0.775, AUROC 0.774로 임상 Leibovich 점수(조정판 C-index 0.8046, AUROC 0.8139)에 근접하는 성과를 보였으며, 병리 기반 단독 모델이 CT 단독 모델보다 우수하였다.

## Limitations

데이터셋 규모 제한으로 CT 인코더의 성능 불안정성이 관찰되었으며, 단순 특징 결합 방식만 적용되어 복합적 융합 기법 연구가 필요하다.

## Conclusion

사전학습된 병리 및 CT 영상 인코더를 활용한 다중 모달 통합은 ccRCC 재발 위험 예측에서 임상 점수에 가까운 개인 맞춤형 성능을 보이며, 향후 더 발전된 융합 기법과 대규모 데이터 확장이 필요하다.

# 10. [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](http://arxiv.org/abs/2508.21580v1)

## Introduction

- Goal: 본 연구의 목표는 4D 종적 의료 영상에서 시공간적 궤적을 학습하기 위한 통합 생성 모델인 Temporal Flow Matching (TFM)을 제안하는 것이다.
- Motivation: 기존 심층 학습 방법은 단일 시점만을 고려하거나 분류 및 회귀에 집중하여 정밀한 공간 예측 능력이 제한적이며, 시공간적 의료 영상 분야에서 일반화된 접근법이 부족하다.
- Contribution: TFM은 다중 시점 및 불규칙한 샘플링을 지원하며, 마지막 참조 영상(LCI)을 특수한 경우로 내포하고 차분 모델링(Difference Modeling) 방식을 통해 기존 방법보다 향상된 예측 성능을 보임을 입증하였다.

## Method

TFM은 Flow Matching 기법을 확장하여 다중 시점 3D 볼륨 영상을 입력으로 받아 목표 시점의 영상을 예측하는 시공간적 생성 모델이다. 차분 모델링을 통해 컨텍스트와 목표 시점 영상 간 변화만을 학습하며, 결측 데이터에 대해 인접 시점 영상을 채워 넣는 희소성 보완 전략을 사용한다. 이를 통해 불규칙하고 희귀한 획득 간격에서도 안정적으로 시간 흐름을 모델링한다.

## Results

ACDC, ISLES, Lumiere 등 세 개의 공개 종적 의료 영상 데이터셋에서 TFM은 기존 시공간적 모델과 LCI 기준선을 모두 능가하며, 차분 모델링과 희소성 보완의 효과를 경험적으로 증명하였다.

## Limitations

본 방법은 대규모 고품질 종적 의료 영상 데이터셋의 부재, 국소적 고해상도 모델링의 어려움, 풍부한 사전학습 모델 부재 등의 한계를 갖는다.

## Conclusion

TFM은 희소하고 불규칙한 시계열 의료 영상에서 차분 기반의 시공간적 흐름 예측을 통해 차세대 4D 의료 영상 분석을 위한 견고한 기준점을 마련하였다.

# 11. [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](http://arxiv.org/abs/2508.21570v1)

## Introduction

- 해양 염분은 해양 순환, 기후, 해양 생태계에 중요한 역할을 하며, 특히 드리프터 기반 데이터에서 측정이 희소하고 불규칙하며 잡음이 많아 정확한 추정이 필요하다.
- 기존 원격 탐사 및 최적 보간법은 선형성 및 정상성에 의존하며 클라우드, 센서 드리프트, 낮은 위성 재방문율 등의 한계가 있다.
- 본 연구는 드리프터 궤적의 희소성을 극복하기 위해 조수 고도 등 물리적 공변량을 조건으로 하는 확산 적대적 네트워크 기반 해양 염분 보간 시스템 OASIS를 제안한다.

## Method

- OASIS는 역정규화(RevIN)를 통한 데이터 정규화, 트랜스포머 기반 전역 종속성 포착 모듈, 조수 고도를 활용하는 스케줄러 확산 적대적 네트워크로 구성된다.
- 트랜스포머 모듈은 희소한 시공간적 데이터에서 장거리 상호작용을 학습하며, 확산 적대적 네트워크는 노이즈 스케줄링과 적대적 손실을 통해 보간의 견고함과 정확도를 높인다.
- 이러한 통합 구조는 별도의 장비 없이 조수 신호의 주기성을 물리적 운전인자로 활용한다.

## Results

- 실제 플로리다 Fort Pierce Inlet 데이터와 멕시코 만 시뮬레이션 데이터를 포함한 4개 벤치마크에서 기존 전통 기법과 신경망 기반 기법 대비 최대 52.5% MAE 감소를 달성하며 우수한 성능을 보였다.

## Limitations

- 정보 부족

## Conclusion

- OASIS는 희소한 드리프터 자료에서 조수 고도를 효과적으로 활용하여 해양 염분 보간 정확도를 크게 개선하며, 웹 기반의 실시간 보간 시스템으로 실무 적용 가능성을 입증하였다.

# 12. [ECHO: Ego-Centric modeling of Human-Object interactions](http://arxiv.org/abs/2508.21556v1)

## Introduction

- Goal: ECHO는 희소한 3포인트(머리와 손목) 추적만으로 1인칭 관점에서 인간-물체 상호작용(HOI)을 통합적으로 복원하는 모델이다.
- Motivation: 웨어러블 기기의 확산에 따라 소량의 센서 데이터로부터 실제적인 상호작용 정보를 복원하는 연구 필요성이 증대되었다.
- Contribution: 본 연구는 인간의 포즈, 물체의 움직임, 접촉 상태를 동시에 예측하는 삼변량 확산 변환기 기반의 통합 프레임워크와 헤드 중심 좌표계, 그리고 컨베이어 기반 시퀀스 추론 방법을 제안하였다.

## Method

ECHO는 머리와 손목의 위치·회전 추적 정보를 조건으로 인간 모션, 물체 모션, 접촉 시퀀스를 삼변량 확산 과정으로 모델링한다. 이 모델은 헤드 중심의 좌표계에서 작동하여 전역 방향성에 무관하고 다양한 입력 조합에 유연하게 대응 가능하다. 또한 프레임별 독립적인 노이즈 스케줄을 가진 확산 변환기와 순차적 컨베이어 추론을 결합하여 임의 길이의 시퀀스를 실시간으로 처리할 수 있다.

## Results

BEHAVE 및 OMOMO 데이터셋에서 기존 대비 인간과 물체 움직임 재현 정밀도가 향상되었으며, AMASS 데이터셋에서도 경쟁력 있는 인간 모션 생성 능력을 입증하였다.

## Limitations

현 연구는 객체가 작거나 미세한 조작을 요구하는 상호작용을 포괄하지 못하며, 장면 전체 상황이나 다양한 센서 입력과의 통합은 후속 연구가 필요하다.

## Conclusion

ECHO는 3포인트 웨어러블 센서만으로 실시간 1인칭 인간-물체 상호작용 복원이 가능한 최초의 모델로, 향후 확장성과 적용 가능성이 높은 새로운 연구 기반을 제시하였다.

# 13. [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](http://arxiv.org/abs/2508.21542v1)

## Introduction

- Goal: 본 연구의 목표는 단일 RGB 이미지로부터 완전한 3D 장면을 가우시안 스플랫 형태로 복원하는 것이다.
- Motivation: 기존 가우시안 스플랫 복원은 다수의 관측 이미지가 필요하며, 가려지거나 보이지 않는 영역 복원이 어려워 선명도 저하와 왜곡 문제가 존재한다.
- Contribution: 본 논문에서는 단일 이미지 조건에서 가려진 부분을 포함한 완전한 3D 가우시안 스플랫 분포를 생성하는 잠재 확산 모델과 이를 위한 무감독 잠재 공간 학습을 위한 Variational AutoReconstructor를 제안한다.

## Method

잠재 확산 모델을 사용하여 3D 가우시안 스플랫의 분포를 학습하며, 별도의 3D 정답 데이터 없이 다중 시점 투영 이미지만을 이용해 잠재 공간을 자기지도 학습으로 구성한다. 이 때, 고주파 세부 표현 손실을 방지하기 위해 디코더에 인코더의 초기 특성 맵을 연결하는 skip connection을 활용한다. 또한, 분류기-자유 가이드 방식을 통해 입력 이미지에 대한 재현성 및 다양성 사이에서 유연한 제어가 가능하다.

## Results

CO3D 및 RealEstate10K 데이터셋에서 본 방법은 기존 최첨단 실시간 복원 기법들 대비 더욱 선명하고 완성도 높은 3D 장면 복원 성능을 보였으며, 특히 가려진 영역에서 다양한 가능한 장면을 생성하는 능력을 Demonstrate하였다.

## Limitations

정보 부족.

## Conclusion

본 연구는 단일 RGB 이미지 단독 입력만으로 3D 가우시안 스플랫 완전 장면 복원이 가능한 확산 모델 학습 파이프라인을 제안하며, 2D 이미지 데이터만으로 고품질 잠재 공간을 학습하고 다양성 조절이 가능한 새로운 3D 장면 복원 방식을 실현하였다.

# 14. [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](http://arxiv.org/abs/2508.21496v1)

## Introduction

- Goal: 본 연구는 장시간 영상 이해에서 발생하는 의미 집계 환각(Semantic Aggregation Hallucination, SAH)을 체계적으로 평가하기 위한 ELV-Halluc 벤치마크를 제안하는 것이다.
- Motivation: 기존 연구들은 주로 단편적 짧은 영상에서 환각 문제를 다루었으며, 장시간 영상에서 다중 이벤트 간의 의미 오집계로 인한 SAH 현상을 간과하였다.
- Contribution: ELV-Halluc 벤치마크 구축, 14개 공개 및 2개 폐쇄형 모델 실험을 통한 SAH 분석, 위치 인코딩과 DPO 기법을 활용한 SAH 완화 방법을 제안하였다.

## Method

ELV-Halluc는 명확히 구분된 여러 이벤트로 구성된 긴 영상을 대상으로 이벤트별 세멘틱 복잡도를 고려해 환각을 분류하며, GPT-4o 기반의 적대적 질문 쌍을 활용한 QA 형식으로 SAH 발생을 정량화한다. 위치 인코딩 전략과 Direct Preference Optimization(DPO)을 이용해 모델의 의미 집계 정확도를 개선하도록 학습시킨다. 8,000 쌍의 적대적 데이터셋을 구축하여 장영상 환각 연구를 지원한다.

## Results

실험 결과 SAH는 이벤트 수 증가 및 의미 변화가 빠른 시각 세부사항에서 더 심화되며, 위치 인코딩과 DPO 적용 시 SAH 비율이 최대 27.7% 감소하고 장영상 및 기존 벤치마크에서 성능 향상이 확인되었다.

## Limitations

Gemini 모델이 생성한 초기 자막에 편향이 존재할 수 있으며, 이벤트 기반 장영상 구성과 데이터셋 규모 제한으로 실제 장시간 영상과 일부 차이가 존재한다.

## Conclusion

본 연구는 장영상 의미 집계 환각에 대한 최초의 체계적 벤치마크와 분석, 완화 기법을 제시하며 장영상 이해 모델의 신뢰성 향상을 위한 기반을 마련하였다.

# 15. [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](http://arxiv.org/abs/2508.21475v1)

## Introduction

- Goal: 본 연구는 복잡한 다중모달 웹 브라우징 작업을 위한 MMSearch-Plus라는 새로운 벤치마크를 제안하는 것이다.
- Motivation: 기존의 다중모달 벤치마크들이 단순한 이미지 검색에 의존하여 정교한 시각적 추론과 출처 검증, 장기 도구 활용과 같은 진정한 다중모달 과제를 숨기기 때문이다.
- Contribution: MMSearch-Plus는 공간 및 시간적 외삽을 통해 미세한 시각 신호를 탐색하고 반복적인 텍스트-이미지 검색과 출처 검증을 요구하는 311개의 고난이도 과제를 제공하며, 다중모달 검색 에이전트 프레임워크와 다양한 모델 평가 결과를 공개하였다.

## Method

본 벤치마크는 공간-시간적 외삽 기법을 통해 이미지 내 제한적이고 미세한 시각 정보를 바탕으로 외부 사실을 추론하는 문제를 생성하였다.  
자료는 유튜브, 빌리빌리, 아카이브 등에서 수집되었으며 시각적 단서와 텍스트 맥락을 이용하여 견고한 탐색과 출처 검증을 필요로 한다.  
검증 과정에서 교차검증과 시각 토큰 가림, 텍스트 정보 삭제 등 난이도 향상을 위한 적대적 필터링을 수행하였다.

## Results

최고 성능 모델인 o3는 검색 없이 15.1%, 반복적 검색을 통한 풀 롤아웃에서는 36.0%의 정확도를 달성하였으며, 강력한 오픈 소스 모델 Qwen-2.5-VL-72B-Instruct는 검색 전 0.0%, 20회 검색 후 6.9%로 상대적으로 저조한 성능을 보였다.

## Limitations

본 데이터셋은 공개 웹 소스 및 특정 도메인 편향을 내포하고, 영상 및 동적 인터페이스를 다루는 영역은 제한적이며 전문화된 시스템 평가가 포함되지 않았다.

## Conclusion

MMSearch-Plus는 미세한 시각 단서의 검출과 반복 검색 하에 출처 검증을 요구하는 진정한 다중모달 웹 브라우징 도전 과제를 제공하며, 현존하는 다중모달 언어 모델들의 한계와 발전 가능성을 명확히 제시한다.

# 16. [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](http://arxiv.org/abs/2508.21468v1)

## Introduction

- 본 연구의 목표는 Bayesian Flow Network와 gradient 기반 통합 지도를 활용하여 구조기반 약물 설계에 적합한 제어 가능한 3D 분자 생성 모델을 개발하는 것이다.
- 종래의 확산 기반 생성 모델이 결합 친화도에만 치중하여 합성 가능성과 선택성을 충분히 반영하지 못하는 한계에서 동기를 부여받았다.
- Bayesian Flow Network를 확장한 CBYG 프레임워크를 제안하고, 다양한 약리적 특성을 통합 제어하며 보다 신뢰성 있는 평가 체계를 구축한 것이 본 연구의 주요 기여이다.

## Method

- CBYG는 연속 및 범주형 데이터를 하나의 파라미터 공간에서 다루는 Bayesian Flow Network를 기반으로 하며, 외부 예측기를 이용한 gradient 기반 조건부 생성을 구현한다.
- 기존 확산 모델들의 불안정한 범주형 변수 가이드 및 중간 상태의 화학적 의미 부족 문제를 극복하기 위해 신뢰성 있는 확률적 업데이트를 도입하였다.
- 합성 가능성, 선택성, 친화도를 아우르는 실험적 평가 체계도 함께 설계하여 실용성을 강화하였다.

## Results

- 제안된 CBYG 모델은 다양한 도킹 소프트웨어와 합성 경로 평가에서 기존 최첨단 모델들을 통합적 성능 지표 전반에 걸쳐 일관되게 능가하였다.

## Limitations

- 다수의 생성 분자가 여전히 실제 합성에 어려움을 겪는 점에서 합성 가능성 평가 및 개선에 관한 후속 연구가 필요하다.

## Conclusion

- CBYG 프레임워크는 약물 개발에서 요구되는 다중 약리적 특성을 효과적으로 통합 제어함으로써 실용적인 3D 분자 생성의 새로운 표준 가능성을 제시하였다.

# 17. [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](http://arxiv.org/abs/2508.21451v1)

## Introduction

- Goal: 본 연구는 경량화된 이미지 캡셔닝 모델을 개발하여 실용적인 시각 전문화 시스템으로 재고하는 것을 목표로 한다.
- Motivation: 기존의 다중모달 대규모 언어모델(MLLM)은 고성능이나 높은 계산 비용으로 인해 엣지 디바이스에서의 배포가 어렵다는 한계가 있다.
- Contribution: 125M 파라미터 소형 언어모델 OPT-125M 기반의 경량 캡셔닝 전문가 모델과 사람의 ‘한 번 더 집중해서 보는’ 메커니즘을 모방한 Sharp-Eyed Refinement 프레임워크를 제안하였다.

## Method

경량 모델은 LLaVA-7B의 LLaMA-7B를 OPT-125M으로 대체하여 구축되었으며, 시각 인코더의 다중 계층 특징과 초기 캡션을 입력으로 사용하여 세밀한 시각 정보를 추출하는 DeepLens 멀티모달 커넥터를 도입하였다.  
초기 문장 생성 후 재검토·수정을 수행하는 ‘한 번 더 보기’ 전략으로 시각적 주의 집중과 세밀한 기초 정보를 강화하였다.  
두 단계의 파인튜닝을 통해 초기 생성 및 수정 과정을 학습하였으며, 수정 단계에서는 GPT-4o-mini를 활용한 오류가 포함된 초기사용 캡션을 활용하여 점진적 개선을 유도하였다.

## Results

경량화된 캡셔닝 전문가 모델은 MS COCO 및 DCI와 ShareGPT4V 데이터셋에서 소형 및 대형 일반 모델과 경쟁력 있는 정확도를 보였고, Sharp-Eyed Refinement 적용 시 CIDEr 점수를 최대 3.9점 향상시켰다.

## Limitations

시각 블라인드니스 문제로 인해 가끔 시각적 오류 또는 부정확한 설명을 생성하는 한계를 가진다.

## Conclusion

본 연구를 통해 경량 캡셔닝 모델과 Sharp-Eyed Refinement가 엣지 디바이스에서도 효과적이고 효율적인 이미지 설명 시스템 구현에 유용함을 입증하였다.

# 18. [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](http://arxiv.org/abs/2508.21435v1)

## Introduction

- Goal: 본 논문은 합성 및 실제 두부 X-선 영상 간 도메인 적응을 위한 고해상도 비짝지어진 이미지 변환 모델 MedShift를 제안하는 것이다.
- Motivation: 합성 의료 영상과 실제 임상 영상 간의 감쇠 특성, 잡음, 연부조직 표현 차이로 인한 도메인 격차는 합성 데이터 기반 모델의 임상 적용 가능성을 제한한다.
- Contribution: 본 연구는 Flow Matching과 Schrödinger Bridges를 활용한 단일 조건부 잠재 공간 학습 방식과 X-DigiSkull 데이터셋을 제안하여 다양한 방사선 선량과 뷰포인트에서 실험적 벤치마크를 수행하였다.

## Method

MedShift는 도메인 라벨에 조건화된 Flow Matching 기반 생성 모델로, VAE를 통한 잠재 공간에서 소스 영상을 도메인 불변 표현으로 인코딩한 후 타겟 도메인 특성으로 변환하는 두 단계 적분 과정으로 작동한다.  
이는 비짝지어진 데이터 환경에서도 단일 모델로 다중 도메인 간 변환을 지원하며, 추론 시 인위적 조절 파라미터를 통해 인지적 충실도와 구조적 일관성 간 균형을 조정할 수 있다.  
X-DigiSkull은 Mentice VIST® 시뮬레이터 기반 합성 영상과 필립스 Azurion 장비에서 획득한 실제 두부 X-선 영상을 다양한 각도와 선량 조건으로 정렬하여 구성된 최초의 공개 데이터셋이다.

## Results

MedShift는 구조 유지와 도메인 적응 사이의 균형에서 기존 CycleGAN-Turbo, Z-STAR, SDEdit 등 최신 기법들을 능가하며, 특히 모델 크기가 상대적으로 작아 추론 효율성 측면에서도 우수한 성능을 보였다.

## Limitations

현행 MedShift는 다단계 샘플링 프로세스를 요구하여 추론 속도 측면에서 GAN 기반 모델들에 비해 느리므로, 가속화 및 경량화 작업이 필요하다.

## Conclusion

본 연구는 MedShift 모델과 X-DigiSkull 데이터셋을 통해 의료 영상 도메인 적응 문제에 대한 효율적이고 유연한 통합 해결책을 제시하였으며, 후속 연구에서는 추론 효율화 및 다도메인 확장이 기대된다.

# 19. [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](http://arxiv.org/abs/2508.21430v1)

## Introduction

- 목표는 의료 멀티모달 대형언어모델(MLLM)의 보상모델과 평가자(판단자)를 임상적 요구에 맞게 평가하기 위한 최초의 벤치마크인 Med-RewardBench를 제안하는 것이다.
- 의료 분야에서는 진단 정확도와 임상 적합성이 필수적인 평가 요소임에도 불구하고, 기존 연구들은 보상모델과 판단자의 신뢰성 확보 및 임상 요구사항에 대한 평가가 미진한 상황이다.
- 본 연구는 13개 기관계와 8개의 임상과를 포함하는 1,026건의 전문가 주석 사례를 기반으로 다차원적 평가 체계를 구축하고, 32개의 최첨단 MLLM을 대상으로 보상모델 평가를 수행하였으며 기준선 모델의 미세조정을 통해 성능 향상 가능성을 입증하였다.

## Method

Med-RewardBench는 1) 다섯 개 공개 의료 데이터셋에서 이미지-질문 쌍을 수집하고 5개 소형 MLLM으로 난이도를 평가하여 의학 전문가가 임상적으로 엄밀하게 검증한 1,026개의 어려운 문제를 선정하였다.  
2) 12개 MLLM을 사용해 각 쌍에 대해 두 개의 응답을 생성하여 쌍별 비교 평가 데이터를 구축하였으며,  
3) 세 명의 임상의가 정확성, 관련성, 포괄성, 창의성, 반응성, 전반적 품질 등 여섯 차원에서 응답 쌍을 평가하여 인간 주석과 모델 성능을 비교하였다.

## Results

평가 결과 대형 및 독점 모델은 의료 특화 모델보다 높은 판단 일관성을 보였으나 상위 모델의 정확도도 최대 68.86%에 그쳤으며, 기관계 및 임상과별 성능 편차가 크고 의료 모델들은 복잡한 임상 추론에서 여전히 한계가 있음을 나타냈다.

## Limitations

본 연구는 Qwen2-VL 모델을 기준선으로 제한된 미세조정 방법만 적용하였기에, 향후 다른 모델과 다양한 학습 방식을 탐색하여 성능 개선 방향을 모색할 필요가 있다.

## Conclusion

Med-RewardBench는 의료 멀티모달 대형언어모델의 보상모델과 판단자를 다차원적 임상 기준에 따라 처음으로 체계적으로 평가하며, 신뢰성 있고 임상 적합한 의료 AI 개발의 기반을 제공한다.
