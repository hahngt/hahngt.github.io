---
layout: distill
title: "[개념 정리] WassersteinGAN"
date: 2023-01-29 18:02:43 +/-0000
description: Summary of mathematical concepts in wGAN
categories: Math
tags: math gan ai
use_math: true  # TAG names should always be lowercase
toc:
  sidebar: right
---

# WassersteinGAN Math

[Reference](https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i)




## Metric (Distance)

* 특징
  * 𝒅(𝑥, 𝑦) ≥ 0
  * 𝒅(𝑥, 𝑦) = 0 ⟷ x = y
  * 𝒅(𝑥, 𝑦) = 𝒅(𝑦, 𝑥) → x, y는 대칭
  * 𝒅(𝑥, 𝑧) ≤ 𝒅(𝑥, 𝑦) + 𝒅(𝑦, 𝑧)

* 공간 별 metric
  * 실수공간 ℝ, 복소공간 ℂ : \|𝑥 - 𝑦\|
  * 유클리드 공간 ℝn : 유클리드 거리 √(∑\|𝑥 - 𝑦\|²)
    * 맨헤튼 거리 : ∑\|𝑥 - 𝑦\|
  * **힐베르트 공간** : 내적 𝒅(𝑢, 𝑣) = √((𝑢, 𝑣) ∙ (𝑢, 𝑣))
  * 함수 공간(L1공간 L2공간)

* 수렴을 정의하기 위해 Metric 개념이 중요
  * 𝑥n ⇒ 𝑥   ⟺   𝓁𝒾𝓂 𝒅(𝑥n, 𝑥) = 0

  * fn과 f의 차를 제곱하여 적분한 값(L2거리)이 0으로 수렴 → L2수렴

    * ![L2수렴](https://user-images.githubusercontent.com/50629765/236161330-7d556c18-6b80-47ab-a9d0-0327c45b15bf.jpeg)
    


  * fn이 모든 𝑥에 대해 𝟄범위 안에 들어오면서 수렴 (L∞거리) → L∞수렴 or 균등 수렴 (uniformly converge)
    *  𝟄범위 벗어나면 측도 수렴 (converge in measure)

    * ![Linf수렴](https://user-images.githubusercontent.com/50629765/236161373-ade9e16f-4ca2-4714-ace3-4ebdc2d94bfe.jpeg)


> ⇨ **거리함수가 바뀌면 수렴의 방식이 바뀜**

* 수렴간의 비교
  * 𝒅₁-수렴이 𝒅₂수렴보다 강하다 (𝒅₁ is stronger than 𝒅₂)
    * 𝒅₁(𝑥, 𝑦) → 0 ⇒ 𝒅₂(𝑥, 𝑦) → 0
  * 거꾸로 성립 : 약하다 (weaker)
  * 양방향 성립 : 동등하다 (equivalent)
    * 유클리드 거리 and 맨헤튼 거리
  * 공간마다 차이 때문에 항상 비교 가능한건 아님

* 유한 측도를 가진 공간에서는 다음이 성립
  * L∞ ⇒ L2 ⇒ 측도 수렴 (converge in measure)
* WassersteinGAN에서는 확률분포 공간에서의 Wasserstein distance를 다룸

## Compact metric set

[Compact란?](https://ko.wikipedia.org/wiki/%EC%BD%A4%ED%8C%A9%ED%8A%B8_%EA%B3%B5%EA%B0%84)

* compact 집합을 가져온 이유
  * 연속함수들이 항상 최대 최소를 가짐 (최대 최소의 정리)
  * 모든 확률변수 𝑿에 대해 조건부 확률분포가 정의
  * 완비공간이다 (Complete space)


* 확률 측도 = 확률 분포 

## Different Distance (Metrics)

### Total Variation (TV)

* 두 확률측도의 측정값이 벌어질 수 있는 값들 중 가장 큰 값
!<img width="505" alt="tv" src="https://user-images.githubusercontent.com/50629765/236161449-093d069a-1d6d-46b2-83d3-a1f1cdef367c.png">

* 만약 교집합이 ∅ 이면, TV = 1

### Kullback-Leibler divergence

<img width="397" alt="KL divergence" src="https://user-images.githubusercontent.com/50629765/236162148-8d756c46-2021-44fb-a99b-7ebfc7ec0881.png">

* metric의 특징(대칭성, 삼각부등식)이 성립 X
  * 그래도 사용가능
* stronger than TV
* 𝛳 ≠ 0 → ㏒ = ∞ → KL = ∞ (발산)

### Jensen-Shanonon divergence

<img width="517" alt="js divergence" src="https://user-images.githubusercontent.com/50629765/236162178-783f8803-a1ec-45be-bde8-133ef7530d93.png">


* Equivalent with TV
* 𝛳 ≠ 0 → JS = ㏒2
* ㏒2 로 고정되어서 얼마나 먼지 모름

> TV, KL, JS는 두 확률분포가 다른 영역에서 측정된 경우 완전히 다르다 라고 판단
>   ⇨ GAN에서 Discrimitor의 학습이 죽는 원인

> 즉, 유연하면서 수렴에 Focus가 집중된 metric이 필요

### EM distance or Wasserstein distance

<img width="357" alt="wgan" src="https://user-images.githubusercontent.com/50629765/236161744-7ef175e7-113d-4a8b-b68a-36570710f799.png">


* 𝛱(P, Q) : P, Q의 결합확률분포
* 모든 결합 확률분포 중 𝒅(𝑥, 𝑦)의 기댓값 중 하한값

<img width="463" alt="wd" src="https://user-images.githubusercontent.com/50629765/236162109-dacfb84f-80bc-4301-8ff8-1665e7892103.png">


* 𝔼(𝒅(𝑥, 𝑦)) ≥ \|𝛳\|
* 𝑍₁ = 𝑍₂ → 𝒅(𝑥, 𝑦) = \|𝛳\|
* 즉, 𝑊 = \|𝛳\|
  
> EM distance와 JS divergence 비교

> <img width="807" alt="스크린샷 2023-05-04 오후 6 44 30" src="https://user-images.githubusercontent.com/50629765/236170268-5a70366a-aade-4a1a-a875-67f804b73296.png">


<script src="https://giscus.app/client.js"
        data-repo="HahnGyuTak/Hahn.github.io"
        data-repo-id="R_kgDONR0IQQ"
        data-category="General"
        data-category-id="DIC_kwDONR0IQc4Cka9V"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>