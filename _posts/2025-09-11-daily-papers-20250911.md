---
layout: post
title: Daily Papers — 2025-09-11"
date: 2025-09-11 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)

## Introduction
- Goal: 본 논문은 대규모 추론 모델(Large Reasoning Models, LRMs)을 위한 강화학습(Reinforcement Learning, RL)의 최신 발전을 종합적으로 조사하는 데 목적이 있다.  
- Motivation: RL은 복잡한 논리적 과제 처리에서 대규모 언어 모델(LLMs)의 능력을 획기적으로 향상시키며, LRMs로의 진화에 필수적인 방법으로 부상하였다.  
- Contribution: 본 연구는 RL의 기본 구성 요소, 핵심 문제, 학습 자원 및 다양한 응용 분야를 체계적으로 리뷰하고, 향후 ASI(Artificial SuperIntelligence) 실현을 위한 확장 전략을 모색한다.  

## Method  
본 논문은 RL을 LLM에 적용하는 기본 개념과 환경 설정을 설명하며, 대표적인 RL 기반 추론 모델들을 시간 순으로 소개하였다.  
주요 구성 요소인 보상 설계, 정책 최적화, 샘플링 전략을 세분화해 각 연구 트렌드와 기법들을 비교 분석하였다.  
또한 RL의 역할, 모델 선험 정보, 학습 레시피, 보상 유형 등 기저 문제들을 논의하고, 정적 데이터셋과 동적 환경 및 인프라 측면의 학습 자원을 평가하였다.  

## Results  
RL을 활용한 대표 모델인 OpenAI o1, DeepSeek-R1 등은 수학 및 코딩 문제 해결 능력을 크게 향상시키며, 확장 가능하고 자기교정적 사고 체인(chain-of-thought) 생성에 성공하였다.  

## Limitations  
확실한 자동 보상 시스템이 없는 주관적 또는 개방형 과제에서는 RL 확장에 여전히 근본적인 어려움이 존재한다.  

## Conclusion  
본 서베이는 LRMs에 특화된 RL 연구를 체계적으로 정리하고, 대규모 학습과 지속적 진화를 통해 인공 초지능에 도달하기 위한 향후 연구 방향을 제시한다.

# 2. [RewardDance: Reward Scaling in Visual Generation](https://arxiv.org/abs/2509.08826)

## Introduction
- Goal: 본 연구는 시각적 생성 모델에서 보상 모델(Reward Model, RM)의 효과적 확장을 위한 새로운 프레임워크인 RewardDance를 제안하는 데 목적이 있다.  
- Motivation: 기존 CLIP 기반 및 회귀(regressive) 방식의 RM들은 구조적 한계와 VLM(vision-language model)의 다음 토큰 예측 메커니즘과의 불일치로 인해 확장에 제약이 있었고, 보상 해킹 문제로 인해 실제 품질 향상에 어려움이 존재하였다.  
- Contribution: RewardDance는 생성적 보상 패러다임을 도입하여 RM을 VLM의 토큰 예측과 내재적으로 정렬시킴으로써 최대 260억 파라미터 규모까지 모델 및 컨텍스트 확장을 가능하게 하고, 텍스트-이미지, 텍스트-비디오, 이미지-비디오 생성에서 기존 기법 대비 우수한 성능과 보상 해킹 저항성을 입증하였다.  

## Method  
RewardDance는 보상 점수를 "yes" 토큰 예측 확률로 재구성하여 VLM의 생성적 특성과 일치시키고, 이를 통해 보상 모델 크기(1B~26B 파라미터)와 문맥(작업 지시문, 참조 이미지, 사고 사슬) 양방향 확장을 실현하였다.  
학습 단계에서는 작업-인식 CoT(Chain-of-Thought) 지시문과 이유 설명을 포함한 데이터로 보상 모델을 훈련하며, 추론 단계에서는 학습된 보상 모델이 강화학습 및 추론 시 보상 피드백을 제공한다.  
또한, 점진적 참조 이미지 비교와 BoN(Best-of-N) 샘플링 전략을 활용하여 고품질 생성물을 선별하고 모델 업데이트에 활용한다.  

## Results  
RewardDance는 텍스트-이미지, 텍스트-비디오, 이미지-비디오 생성 벤치마크에서 최대 26B 규모 RM 사용 시 귀납적 및 측정 지표상 기존 최고 성능 대비 최대 10.7% 향상과 +49% GSB 점수 향상을 달성하였다.  

## Limitations  
RewardDance가 높은 성능과 탐색 다양성을 보이나, 일부 내재된 데이터셋 특성에 따른 일반화 평가 및 모든 영상 생성 시나리오에 대한 완전한 검증은 추가 연구가 필요하다.  

## Conclusion  
RewardDance는 시각적 생성 모델에서 보상 모델 확장의 한계를 극복하고, 대규모 생성적 RM과 문맥 확장을 통해 강화학습 기반 품질 개선과 보상 해킹 방지를 효과적으로 달성하는 최초의 통합 프레임워크이다.

# 3. [3D and 4D World Modeling: A Survey](https://arxiv.org/abs/2509.07996)

## Introduction
- 본 논문은 3D 및 4D 세계 모델링과 생성에 관한 최초의 종합적인 서베이를 제공하는 것을 목표로 한다.  
- 기존 연구는 2D 이미지 및 비디오 데이터의 생성에 집중한 반면, 본 연구는 RGB-D 이미지, 점유 격자, LiDAR 포인트 클라우드와 같은 본질적인 3D/4D 표현을 활용하는 연구들의 중요성과 분산된 문헌 상황을 동기로 삼았다.  
- 본 연구는 세계 모델에 대한 명확한 정의와 계층적 분류체계를 제안하고, 3D/4D 환경에 특화된 데이터셋과 평가 지표를 체계적으로 정리함으로써 연구 발전을 위한 기초 자료를 제공한다.  

## Method  
3D/4D 세계 모델링은 주로 비디오 기반(VideoGen), 점유 격자 기반(OccGen), 그리고 LiDAR 기반(LiDARGen) 방법론으로 분류되며, 각 방법론은 장면 생성을 위한 데이터 엔진, 미래 예측을 위한 행동 해석기, 상호작용 시뮬레이션을 위한 신경 시뮬레이터, 그리고 부분 관찰에서의 장면 복원을 위한 장면 재구성기로 기능적으로 구분된다.  
각 접근법은 지리적 조건, 행동 조건, 그리고 의미적 조건 등 다양한 외부 신호를 모델 입력으로 받아 장면의 현실성, 일관성, 제어 가능성 및 확장성을 평가한다.  
또한, 변분 오토인코더, GAN, 확산모델, 그리고 자기회귀모델 등 다양한 생성 모델 아키텍처를 핵심 알고리즘으로 활용하여 3D/4D 데이터의 효과적인 생성과 예측을 수행한다.  

## Results  
본 서베이에서는 다양한 공개 데이터셋과 최신 모델들을 비교 분석하며, 3D/4D 세계 모델링 분야에서 비디오, 점유 격자, LiDAR 활용 방법들의 발전 양상과 대표 모델들의 성능 평가를 체계적으로 정리하였다.  

## Limitations  
현재의 연구들은 2D 중심 모델과 비교하여 본질적 3D/4D 데이터의 복잡성으로 인해 일관성 유지와 효율성 확보에 여전히 도전적이며, 표준화된 평가 기준 및 통합된 프레임워크가 부족하다.  

## Conclusion  
본 서베이는 3D 및 4D 세계 모델링 관련 연구를 위한 통일된 정의와 분류체계, 데이터셋, 평가 방법을 제공하여 해당 분야 연구자들이 향후 연구를 체계적으로 발전시키는 데 기초를 마련하였다.

# 4. [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making   through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)

## Introduction
- Goal: 본 연구는 대규모 언어 모델(LLM) 에이전트를 다중 턴 상호작용 강화학습을 통해 장기 과제 수행 능력을 학습시키기 위한 통합 프레임워크를 제안하는 데 있다.  
- Motivation: 기존 연구들은 단일 턴 과제에 집중하거나, 제한된 환경과 불안정한 최적화 문제로 인해 복잡하고 현실적인 다중 턴 상호작용 환경에서 효과적인 에이전트 학습이 어려웠다.  
- Contribution: 본 논문에서는 높은 확장성과 유연성을 가진 모듈화된 AgentGym-RL 프레임워크와 탐색-착취 균형을 고려한 점진적 상호작용 확장 학습법인 ScalingInter-RL을 제안하고, 이를 오픈소스로 공개한다.  

## Method  
AgentGym-RL은 에이전트, 환경, 학습 알고리즘을 모듈로 분리하여 PPO, GRPO 등의 주요 강화학습 기법을 지원하며 다양하고 실제적인 환경을 포함한다.  
ScalingInter-RL은 학습 초기에 상호작용 횟수를 제한하여 착취를 유도하고 점진적으로 상호작용 범위를 확대하여 탐색을 촉진함으로써 안정적이고 효율적인 최적화를 도모한다.  
이 접근법은 장기 계획, 반성 및 전략적 탐색 같은 고차원적 행동 발현을 가능하게 하여 에이전트의 다양한 문제 해결력을 강화한다.  

## Results  
AgentGym-RL과 ScalingInter-RL을 적용한 7B 규모의 오픈소스 모델은 5개 시나리오 27개 과제에서 상용 독점 모델과 대등하거나 이를 능가하는 성능 향상을 보였다.  

## Limitations  
본 연구는 복잡하고 개방형 환경에서 강화학습 효율성 및 안정성의 한계를 완전히 극복하지는 못하였고, 환경별 성능 차이가 존재한다.  

## Conclusion  
AgentGym-RL과 ScalingInter-RL은 LLM 에이전트의 다중 턴 장기 의사결정 능력을 효과적으로 향상시키는 안정적이고 확장 가능한 강화학습 프레임워크로서 향후 지능형 에이전트 연구에 중요한 기반을 제공한다.

# 5. [P3-SAM: Native 3D Part Segmentation](https://arxiv.org/abs/2509.06784)

## Introduction
- 본 연구의 목표는 복잡한 3D 객체를 자동으로 세분화하는 네이티브 3D 포인트-프롬프트 가능한 파트 세분화 모델 P3-SAM을 제안하는 것이다.  
- 기존 3D 파트 세분화 방법들은 2D 세분화 결과에 의존하여 3D 일관성 부족과 경계 불명확성 문제, 그리고 완전 자동화의 어려움을 안고 있다.  
- 본 논문은 3.7백만 개의 모델로 구성된 대규모 3D 파트 세분화 데이터셋을 활용하여, 정확하고 강건한 자동 세분화를 실현하는 P3-SAM 모델과 자동 마스크 병합 알고리즘을 제시한다.  

## Method  
P3-SAM은 포인트 트랜스포머 기반의 특징 추출기와 세 개의 세분화 헤드, 그리고 IoU 예측기로 구성되어 단일 포인트 프롬프트로 다양한 스케일의 마스크를 예측한다.  
자동 세분화를 위해 모델은 FPS(Farthest Point Sampling)를 통해 포인트 프롬프트를 샘플링하고 NMS(Non-Maximum Suppression)를 적용하여 중복 마스크를 병합한다.  
데이터셋 구축은 여러 소스에서 3D 모델을 수집하여 파트 정보를 추출하고, 비워터타이트(mesh watertight) 모델을 워터타이트 모델로 변환하여 표면 파트 라벨링을 수행하였다.  

## Results  
제안된 P3-SAM은 PartObj-Tiny, PartObj-Tiny-WT, PartNetE 데이터셋에서 비워터타이트 및 워터타이트 3D 메쉬, 점군 데이터 전반에 걸쳐 기존 방법 대비 뛰어난 정밀도와 강건성을 보이며 최첨단 성능을 기록하였다.  

## Limitations  
본 모델은 표면 기하 정보에만 의존하여 부분의 공간적 부피를 충분히 이해하지 못하는 한계가 존재한다.  

## Conclusion  
P3-SAM은 대규모 3D 파트 데이터셋에서 학습된 신뢰성 높은 네이티브 3D 파트 세분화 모델로, 다양한 세분화 작업과 데이터 유형에 광범위하게 적용 가능하며 향후 공간 분할 능력 강화 연구가 요구된다.

# 6. [Hunyuan-MT Technical Report](https://arxiv.org/abs/2509.05209)

## Introduction
- Goal: 본 보고서는 33개 언어 간 양방향 번역을 지원하며 중국어와 소수민족어 및 방언 간 번역에 중점을 둔 오픈소스 다국어 기계번역 모델 Hunyuan-MT-7B를 소개하는 것이다.  
- Motivation: 기계번역 분야에서 고자원 언어 위주의 편향과 소수민족어 번역의 부족 문제를 해결하고, LLM 기반 다국어 번역 시스템의 개발 방법론을 제공하는 데 목적이 있다.  
- Contribution: Hunyuan-MT-7B와 느린 사고 방식을 모티브로 한 Hunyuan-MT-Chimera-7B 모델을 제안하고, 단계적 사전학습과 지도 미세조정, 강화학습을 통한 훈련 프레임워크를 개발하였다.  

## Method  
모델은 일반 및 기계번역 특화 사전학습을 거쳐 약 1300억 토큰의 다국어 데이터를 기반으로 다단계 데이터 품질 평가와 비율 최적화를 실시하였다.  
이후 지도 미세조정 과정에서 고품질 병렬 말뭉치 데이터를 이용하고, 평가 지표에 따른 강화학습과 약한 모델 출력들을 합성하는 약-강 강화학습 기법을 도입하였다.  
특히 Chimera-7B 모델은 다수 번역 후보군을 입력 받아 이들을 종합하여 고품질 출력을 생성하는 학습 기반의 합성 메커니즘을 적용하였다.  

## Results  
Hunyuan-MT-7B와 Chimera-7B는 WMT2025 일반 기계번역(shared task)에서 31개 언어쌍 중 30개 평가 항목에서 1위를 달성하며 기존 상용 및 최신 LLM 대비 뛰어난 번역 성능을 보였다.  

## Limitations  
정보 부족이다.  

## Conclusion  
제안한 두 모델과 훈련 방법론은 중·저자원 언어, 특히 중국 소수민족어-중국어 간 번역 분야에서 최첨단 성능을 달성함으로써 다국어 번역 기술 발전에 기여하였다.

# 7. [<think> So let's replace this phrase with insult... </think> Lessons   learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)

## Introduction
- Goal: 본 연구는 대형 언어 모델(LLM)을 활용하여 독성 텍스트 합성 데이터를 생성하고 이를 해독(텍스트 디톡시피케이션) 모델 학습용 데이터로 활용할 수 있는지를 평가하는 것이다.  
- Motivation: 기존 연구에서는 LLM의 합성 데이터가 여러 NLP 작업에서 유용하지만, 민감하고 복잡한 독성 텍스트 분야에서는 성능과 다양성이 충분히 검증되지 않았다.  
- Contribution: Llama 3와 Qwen 모델을 사용해 중립 텍스트의 독성 대응문 생성 후, 이를 학습한 해독 모델의 성능을 인간 주석 데이터 기반 모델과 비교하여 LLM 합성 독성 데이터의 한계와 문제점을 밝혀냈다.  

## Method  
중립 텍스트는 ParaDetox와 SST-2 데이터셋에서 취득하였고, Llama 3, Qwen3 및 Cogito v1 모델을 활용해 독성 대응문을 생성하였다.  
생성된 합성 데이터를 바탕으로 BART-large 모델을 미세 조정했고, 인간 주석 데이터 기반 모델과 성능 차이를 평가하였다.  
평가는 스타일 전환 정확도(STA), 의미 유사도(SIM), 유창성(FL) 및 이 세 지표를 통합한 종합 성능 지표(J)를 사용하였다.  

## Results  
합성 독성 데이터로 학습한 디톡시피케이션 모델은 인간 주석 데이터 기반 모델 대비 최대 30% 성능 저하를 보였으며, 이는 LLM이 제한적이고 반복적인 욕설 어휘만 생성하여 독성 표현의 다양성이 부족한 데 기인한다.  

## Limitations  
LLM이 생성하는 독성 텍스트의 어휘 다양성 부족 및 반복적 특성으로 인해 실제 독성 언어의 뉘앙스와 복잡성을 충분히 반영하지 못하고, 이에 따른 해독 모델 성능 저하가 발생한다.  

## Conclusion  
현재 LLM 생성 독성 데이터는 인간 주석 데이터를 완전히 대체하기에 부적합하며, 향후 LLM의 언어 다양성과 스타일 복잡성 향상 연구가 필요하다.

# 8. [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)

## Introduction
- 본 연구의 목표는 다수결 방식이 항상 최적의 해를 도출하지 못하는 문제를 해결하기 위해 강화학습으로 학습된 집계 모델을 제안하는 것이다.  
- 대규모 언어 모델(LLM)의 난이도 높은 추론 문제 해결을 위해 다중 해답 생성 후 집계하는 기법이 중요하지만 기존 다수결 및 보상모델 기반 집계 방식은 한계가 존재한다.  
- 본 연구는 명시적 추론 능력으로 다수의 후보 해답을 검토·조정·통합하는 집계 모델을 강화학습으로 학습하여 성능을 크게 향상시켰다.  

## Method  
- 제안하는 AggLM은 LLM이 생성한 복수의 해답을 입력받아 오류 수정 및 부분적 아이디어 결합을 수행하는 집계 모델이다.  
- 강화학습의 검증 가능한 보상을 활용하여 후보 해답 중 소수지만 정답인 답변도 회복할 수 있도록 쉬운 문제와 어려운 문제의 균형 잡힌 학습 데이터로 모델을 학습한다.  
- 이렇게 함으로써 단순 다수결이나 보상모델 순위 기반 집계법을 뛰어넘는 명시적 학습된 추론형 집계 능력을 확보한다.  

## Results  
- MathArena의 수학 경시 대회 데이터셋 4종에서 Qwen3-1.7B 기반 AggLM-1.7B는 다수결 대비 최대 5점 이상 높은 정확도를 기록하며, 강력한 72B 파라미터 급 보상모델 선택법을 일관되게 능가하였다.  

## Limitations  
- 학습 과정에서 대량의 쉽고 어려운 문제 예제의 적절한 혼합이 필수적이며, 극단적인 비율 시 성능 저하가 발생할 수 있다.  

## Conclusion  
- 강화학습을 통한 명시적 집계 학습은 대규모 언어 모델의 난이도 높은 추론 문제 해결에 효과적이며, 다양한 모델과 해답 수에 대해서도 견고하게 일반화된다.

# 9. [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)

## Introduction
- Goal: EnvX는 오픈소스 GitHub 저장소를 지능형 자율 에이전트로 전환하여 자연어 기반 상호작용과 다중 에이전트 협업을 가능하게 하는 프레임워크이다.  
- Motivation: 기존 저장소 활용은 수동적이고 오류 가능성이 높으며 개발자가 문서 탐색, API 이해, 통합 코드 작성 등에 많은 시간과 노력을 투입해야 하는 한계가 존재한다.  
- Contribution: EnvX는 TODO 기반 환경 초기화, 인간 정렬형 자동화, 에이전트 간 통신 프로토콜(A2A)을 결합하여 저장소를 능동적인 에이전트로 재구성함으로써 이러한 문제를 극복한다.  

## Method
EnvX는 1) 저장소 의존성, 데이터, 검증 데이터셋을 포괄적으로 초기화하는 TODO 가이드 환경 설정, 2) 저장소별 에이전트를 생성하여 사용자 쿼리와 실제 작업을 자동화하는 인간 정렬형 에이전트 자동화, 3) 에이전트 간 협업을 위한 A2A 프로토콜을 통한 통신 기능을 순차적으로 수행한다. 이 과정에서 LLM과 다양한 도구를 통합하여 저장소 기능의 이해, 실행, 상호작용 전 과정을 자동화한다.  

## Results
GitTaskBench 벤치마크에서 EnvX는 18개 저장소 대상 74.07% 실행 완료율과 51.85% 작업 통과율을 기록하며 기존 프레임워크 대비 우수한 성능을 입증하였다.  

## Limitations
현재 평가가 주로 스크립트 기반 검증과 제한된 태스크에 의존해 장기 협업, 분포 변화에 대한 견고성, 보안 관련 실패 모드 등에 대한 포괄적인 검증이 부족하다.  

## Conclusion
EnvX는 저장소를 지능적이고 상호작용 가능한 에이전트로 전환하여 오픈소스 생태계 내 접근성과 협업을 혁신하는 새로운 자동화 및 커뮤니케이션 패러다임을 제시한다.

# 10. [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)

## Introduction
- Goal: 본 논문은 생성형 인공지능(Generative AI) 분야에서 통계학적 방법론이 수행하는 역할과 그 응용 가능성을 고찰하는 데 목적이 있다.  
- Motivation: 생성형 AI는 확률적 모델에 기반하여 동작하는데, 현재 정확성, 안전성, 공정성 등에서 보장되지 않는 문제점이 존재한다.  
- Contribution: 본 연구는 생성형 AI 전반에 적용 가능한 통계기법들을 네 가지 주제(행동 개선, 진단 및 불확실성 정량화, 평가, 실험 설계)로 나누어 체계적으로 리뷰하였다.  

## Method  
생성 모델의 출력, 입력, 알고리즘 요소를 조정하는 다양한 방법들을 통계적으로 해석하고, 특히 거절 확률 제어와 같은 통계적 보장 기법을 설명하였다.  
불확실성 정량화에서는 인식 불확실성(epistemic)과 내재 확률 불확실성(aleatoric)을 구분하고, 분포에 의존하지 않는 교환성 기반 검정법 등이 적용됨을 논의하였다.  
평가 영역에서는 한정된 데이터와 편향 문제를 고려한 표본 기반 통계추론 방법과 비교 평가 방법론을 제시하였다.  

## Results  
통계적 분포 제약이 없는 예측 집합(conformal prediction)과 교환성(exchangeability) 이론을 활용해 거절 확률을 효과적으로 제어함으로써 생성형 AI의 신뢰도를 개선할 수 있음을 보였다.  

## Limitations  
대부분의 통계기법이 아직 연구 단계에 머물러 있으며 주류 상업 제품에 충분히 적용되지 않았고, 복잡한 AI 모델 내부 작동을 완벽히 이해하고 활용하는 데 한계가 존재한다.  

## Conclusion  
통계학적 접근법은 생성형 AI 시스템의 신뢰성 향상과 평가 효율 극대화에 중요한 잠재력을 가지며, 향후 연구 및 실무 적용 방향에 대한 추가 개발이 요구된다고 결론지었다.

# 11. [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI   Assistants](https://arxiv.org/abs/2509.08494)

## Introduction
- Goal: 본 연구는 AI 비서가 인간의 주체성(human agency)을 얼마나 잘 지원하는지를 평가하기 위한 확장 가능하고 적응적인 벤치마크 HUMANAGENCYBENCH(HAB)를 개발하는 것이다.  
- Motivation: AI에 더 많은 업무와 결정을 위임함에 따라 인간이 개인적·집단적 미래 통제력을 점차 상실할 위험이 존재하기 때문이다.  
- Contribution: 여섯 가지 주체성 차원을 정의하고 이를 기반으로 20개 최신 대형언어모델(LLM) 비서를 평가하는 자동화된 AI-보조 벤치마크 시스템과 데이터세트를 공개하였다.  

## Method  
HAB는 여섯 차원(명확한 질문, 가치 조작 회피, 오정보 정정, 중요한 결정 연기, 학습 독려, 사회적 경계 유지) 별로 500개의 사용자 시뮬레이션 쿼리를 대형언어모델을 통해 생성·검증·클러스터링하여 테스트셋을 구성하였다.  
각 LLM 비서의 응답은 전용 평가 모델로 채점되며, 점수는 각각 0~1 범위로 정규화되어 차원별 지원도를 나타낸다.  
LLM 기반 평가의 신뢰성 검증을 위해 인간 평가자 468명과의 비교 분석 및 평가자 간 비교도 수행하였다.  

## Results  
HAB 평가 결과, 전반적으로 인간 주체성 지원 수준은 낮거나 중간 정도였으며, 모델 개발사와 차원별로 큰 편차가 존재하였고, 특히 Anthropic의 Claude 모델이 전반적 지원도에서 우수한 반면, 가치 조작 회피 차원에서는 상대적으로 낮았다.  

## Limitations  
본 벤치마크는 인간 주체성 개념의 복잡하고 주관적인 특성을 완전히 포괄하지 못하며, 장기적이고 미묘한 영향에 대한 측정이 제한적이다.  

## Conclusion  
본 연구는 AI 비서가 인간 주체성에 미치는 영향을 체계적으로 평가할 수 있는 도구인 HAB를 제안하여, AI 시스템이 미래에도 인간이 적절한 통제권을 유지하도록 하는 방향으로 연구를 촉진하고자 한다.
