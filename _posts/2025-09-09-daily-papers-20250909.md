---
layout: post
title: Daily Papers — 2025-09-09"
date: 2025-09-09 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)

## Introduction
- 본 논문은 오픈형 생성 문제에서 비용이 많이 드는 강화학습이나 교사 모델 의존 없이도 심층 추론 능력을 갖춘 언어 모델을 개발하는 것을 목표로 한다.  
- 기존의 강화학습과 명령 증류 방식은 명확한 보상 신호의 부재와 높은 비용 문제로 인해 오픈형 생성 분야에서 한계가 존재한다는 점에서 동기가 부여되었다.  
- REverse-Engineered Reasoning (REER)이라는 역설계 기반의 새로운 추론 패러다임과 20,000개의 심층 추론 경로 데이터셋인 DeepWriting-20K를 제안하였다.  

## Method  
- REER은 목표 출력물로부터 역방향으로 구조화된 추론 경로를 탐색하는 그래디언트 없는 지역 탐색 알고리즘을 통해 고품질의 사고 과정을 합성한다.  
- 합성된 추론 경로 데이터로 Qwen3-8B 기반의 DeepWriter-8B 모델을 미세조정하여 심층 추론능력을 내재화한다.  
- 인위적인 사고 패턴 삽입과 반복적인 경로 정제 절차를 통해 인간과 유사한 사고 과정을 모방하도록 설계되었다.  

## Results  
- DeepWriter-8B는 LongBench, HelloBench, WritingBench 벤치마크에서 최첨단 오픈소스 모델을 크게 능가하며, GPT-4o와 Claude 3.5와 같은 상용 대형 모델과 경쟁하는 성능을 달성하였다.  

## Limitations  
- 정보 부족  

## Conclusion  
- 본 연구는 비용 부담과 표식 신호 부재 문제를 극복하고, 심층적이고 구조화된 사고 경로 합성을 통해 오픈형 생성에 최적화된 강력한 추론 모델을 구축하는 새로운 가능성을 제시하였다.

# 2. [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)

## Introduction
- Goal: 본 연구의 목표는 장기 탐색이 가능한 웹 에이전트를 훈련하기 위한 효과적인 데이터 생성 및 학습 방법을 제안하는 것이다.  
- Motivation: 기존 공개 소스 웹 에이전트는 복잡한 정보 탐색 작업에서 능력이 제한적이며, 이의 근본적 원인은 도전적이고 질 높은 학습 데이터의 부족에 있다고 판단되었다.  
- Contribution: 본 논문은 모델 기반 탐색과 점진적인 장기-단기 쿼리 진화를 결합한 WebExplorer 프레임워크를 통해 고난이도 쿼리-답변 쌍을 생성하고, 이를 활용한 강화학습 기반 웹 에이전트 WebExplorer-8B를 개발하였다.  

## Method  
WebExplorer는 강력한 대형언어모델을 사용해 초기 정보 공간을 탐색하고 첫 쿼리-답변 쌍을 구성하며, 이후 쿼리에서 명확한 단서를 제거하고 모호성을 더하는 반복적 진화 과정을 거쳐 난이도를 체계적으로 증가시킨다. 학습은 감독학습을 통한 초기사용 능력 형성 이후 강화학습으로 확장된 문맥 길이와 툴 호출 횟수를 지원하며 장기 탐색 능력을 강화한다. 이 과정에서 두 가지 핵심 툴인 검색과 탐색을 반복 활용하여 다단계 추론과 복잡한 웹 내비게이션을 수행한다.  

## Results  
WebExplorer-8B 모델은 8B 파라미터 규모임에도 불구하고 BrowseComp-en/zh, WebWalkerQA, FRAMES 등 다수 정보 탐색 벤치마크에서 72B 이상의 대형모델을 능가하는 최첨단 성능을 달성하였다.  

## Limitations  
현재 WebExplorer-QA 데이터는 BrowseComp 기반 예시 및 위키백과 씨앗 엔티티에 주로 의존하여 완전한 현실세계 난이도를 재현하는 데 한계가 있다.  

## Conclusion  
WebExplorer 프레임워크는 대규모 강화학습을 통해 장기 탐색이 가능한 고성능 웹 에이전트를 구축하는 데 실용적인 발전 경로를 제시한다.

# 3. [Revolutionizing Reinforcement Learning Framework for Diffusion Large   Language Models](https://arxiv.org/abs/2509.06949)

## Introduction
- 본 연구의 목표는 확산 기반 대형 언어 모델(diffusion large language models, DLMs)을 위한 궤적 인식 강화학습(trajectory-aware reinforcement learning) 프레임워크인 TraceRL을 제안하는 것이다.  
- 기존의 DLM 강화학습은 전체 시퀀스 점수 추정에 의존해 최적의 추론 경로와 불일치하는 문제를 갖고 있었으므로 이를 개선하고자 하였다.  
- TraceRL은 다양한 아키텍처에 적용 가능하고, 확산 기반 가치 모델을 도입해 학습 안정성을 증대하며, 수학 및 코딩 분야에서 기존 최고 성능을 능가하는 DLM을 도출한 점이 본 연구의 주요 기여이다.  

## Method  
TraceRL은 모델이 생성하는 중간 추론 궤적 전체를 학습에 반영하는 강화학습 방식으로, 정책 최적화를 위해 궤적 단위 보상과 가치 함수를 확산 모델 기반으로 설계하였다.  
풀어쓰기(full-attention) 모델의 학습 효율성을 위해 인접 단계를 합치는 축소 파라미터를 도입하였고, 블록 확산(block diffusion) 모델은 블록 단위로 학습 데이터를 분할하여 고효율 학습이 가능하도록 설계하였다.  
또한, TraceRL은 선호하는 추론 경로에 따라 모델 적응을 강화하여 다양한 크기의 블록 사이즈 확대 및 가속화된 추론에 기여한다.  

## Results  
TraceRL을 적용한 TraDo 계열 모델은 4B 및 8B 파라미터 규모임에도 불구하고, 수학 추론 벤치마크 MATH500에서 Qwen2.5-7B-Instruct 대비 6.1%, Llama3.1-8B-Instruct 대비 51.3%의 정확도 향상을 달성하였다.  

## Limitations  
TraceRL은 복잡한 학습 과정에서 고성능 가치 함수 설계가 요구되며, 대규모 시퀀스 처리 시 계산 비용이 증가할 수 있다는 한계가 존재한다.  

## Conclusion  
본 연구는 다양한 확산 언어 모델 아키텍처에 적용 가능한 궤적 인식 강화학습 방법을 제안하여, 수학과 코딩 분야에서 최첨단 성능을 갖춘 모델을 구현하고 학습 안정성 및 추론 효율성을 함께 개선하였다.

# 4. [Does DINOv3 Set a New Medical Vision Standard?](https://arxiv.org/abs/2509.06467)

## Introduction
- Goal: 본 연구는 자연 이미지로 사전 학습된 최첨단 시각 변환기 모델 DINOv3가 의료 영상 분야에서 도메인별 추가 학습 없이 강력한 통합 인코더로서 활용 가능한지 평가하는 것이다.  
- Motivation: 의료 영상은 다양한 영상 모달리티와 희귀 데이터 특성 때문에 강력한 범용 시각 표현 추출기가 필요하며, 대규모 자연 이미지 사전 학습 모델이 이를 대체할 가능성을 검증하고자 한다.  
- Contribution: 본 연구는 DINOv3를 2D/3D 분류 및 분할 과제에 걸쳐 다중 의료 영상 데이터셋으로 체계적으로 벤치마킹하여 성능과 확장성, 도메인 특이적 한계를 분석하였다.  

## Method  
DINOv3 인코더를 고정한 상태에서 선형 프로빙, k-최근접 이웃 검증, 주의 기반 다중 인스턴스 학습 및 슬라이스 단위 3D 볼륨 피처 집계 방식으로 다양한 의료 영상 분류 및 분할 태스크에 적용하였다.  
2D 인코더 피처를 슬라이스별로 추출하여 3D 볼륨용 가상 3D 피처로 구성하고 경량 디코더와 결합하여 분할을 수행하였다.  
다양한 모델 크기 및 입력 해상도 변화에 따른 확장성도 함께 평가하였다.  

## Results  
DINOv3는 자연 이미지로만 훈련되었음에도 흉부 X선 및 3D CT 분류에서 도메인 특화 모델을 능가하는 강력한 기준점을 세운 반면, 병리학적 전체 슬라이드 영상, 전자현미경, PET 영상에서는 현저히 낮은 성능을 보였고, 모델 확장에 따른 성능 향상도 일관되게 나타나지 않았다.  

## Limitations  
본 연구는 DINOv3 모델군에 국한되었으며, 완전한 파인튜닝이나 다른 적응 기법의 잠재성 및 일부 의료 영상 모달리티와 태스크를 포함하지 못한 제한점이 존재한다.  

## Conclusion  
DINOv3는 일부 의료 영상 태스크에서 강력한 비의료 사전 학습 기반 표현력으로 유용한 기준점을 제공하지만, 도메인 특이적 영상 및 확장성 측면에서는 심각한 한계가 확인되어 향후 다중 뷰 일관성 등 고급 응용 방향 연구가 필요하다.

# 5. [Reinforced Visual Perception with Tools](https://arxiv.org/abs/2509.01656)

## Introduction
- Goal: 본 연구의 목표는 강화학습을 이용하여 멀티모달 대형언어모델(LLM)이 시각 도구를 효과적으로 활용하며 고도화된 시각 인지 및 추론 능력을 갖추도록 하는 것이다.  
- Motivation: 기존의 감독학습 기반의 시각 도구 사용 학습은 데이터 생성 비용이 높고, 데이터 필터링에 의존하며, 일반화 성능이 부족하다는 한계가 존재하였다.  
- Contribution: 본 논문에서는 GRPO 기반의 새로운 강화학습 알고리즘을 활용한 ReVPT 방법을 제안하여, 다양한 시각 도구를 통한 시각 문제 해결 능력이 기존 감독학습 및 텍스트 기반 강화학습 방법을 능가함을 증명하였다.  

## Method  
ReVPT는 객체 탐지, 깊이 추정, 엣지 감지, 확대 기능 등 4가지 시각 도구를 통합하여 멀티모달 LLM이 시각적 질문에 대해 도구를 선택하고 반복적으로 활용할 수 있도록 강화학습한다.  
우선 GPT-4.1로 생성한 도구 사용 데이터로 초기 정책을 습득한 후, GRPO 알고리즘을 통해 다양한 도구 활용 전략을 탐색하고 보상 기반으로 최적화한다.  
정확성 및 응답 형식에 기반한 룰-기반 보상 설계를 통해 안정적이고 검증 가능한 강화학습 환경을 구축하였다.  

## Results  
ReVPT는 CV-Bench, BLINK, MMVP, MMStar 등 5개 시각 인지 집중 벤치마크에서 3B 및 7B 모델 모두 기존 감독학습 및 텍스트 기반 RL보다 최대 9.82%까지 성능을 개선하며 최첨단 성능을 기록하였다.  

## Limitations  
시각 도구의 오류 또는 부적절한 활용으로 인한 성능 저하 사례가 있으며, 일부 경우 모델이 잘못된 도구 정보를 해석하거나 활용하지 못하는 한계가 관찰되었다.  

## Conclusion  
ReVPT는 강화학습 기반의 시각 도구 활용이 멀티모달 LLM의 시각 인지 및 추론 능력 향상에 효과적임을 입증하며, 도구 선택 및 활용 정책 학습의 새로운 가능성을 제시하였다.

# 6. [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)

## Introduction  
- Goal: 본 논문은 딥 리서치 시스템에서 강화학습(RL)의 기초와 역할을 체계적으로 조사하는 데 목적이 있다.  
- Motivation: 기존의 지도학습 기반 파인튜닝(SFT)과 선호도 정렬(DPO) 기법이 다단계, 도구 활용 연구 과제에서 한계를 보임에 따라, RL의 중요성이 부각되었다.  
- Contribution: 본 조사는 데이터 합성·선별, 에이전트 RL 기법, 학습 프레임워크, 아키텍처 설계 및 평가 벤치마크를 일관된 분류체계로 정리하여 RL 중심의 딥 리서치 연구 토대를 제시한다.  

## Method  
딥 리서치 시스템은 계획자(Planner), 조정자(Coordinator), 집행자(Executors)로 구성된 계층적 구조를 채택하며, 실제 학습은 주요 도구(검색, 탐색, 코딩)에 연동된 계획자 모델에 집중된다. 강화학습 기반 학습은 환경 상태와 도구 인터랙션을 포함하는 궤적 단위의 정책 최적화 방식을 활용하여, 장기 의사결정, 신뢰성 있는 보상 설계, 검색 및 도구 사용 전략 탐색을 가능하게 한다. 데이터는 점진적 난이도 및 다양한 멀티모달 요소를 포함하는 합성 질의로 구성하며, 복잡도에 따른 난이도 분류 및 학습 커리큘럼을 제안한다.  

## Results  
RL 기반 학습은 기존 SFT/DPO 기법 대비 장기적 목표 달성, 복잡한 다단계 추론 및 도구 활용에서 견고하고 효율적인 성능 향상을 보여주었으며, HotpotQA, MuSiQue, WebWatcher 등 다수 벤치마크에서 우수한 결과를 도출하였다.  

## Limitations  
본 연구는 RL 학습에 필요한 고품질 합성 데이터 생성과 안정적인 평가 지표 설계 등에서 여전히 해결해야 할 인프라 및 자동화 문제가 존재함을 지적하였다.  

## Conclusion  
딥 리서치 시스템에 특화된 강화학습 기법은 복잡다단한 정보탐색 및 다중 도구 협업 과제에서 효과적인 에이전트 학습 기반을 제공하며, 본 조사는 해당 분야 연구 진전에 실질적 방향성을 제시한다.

# 7. [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/abs/2509.06461)

## Introduction
- Goal: 본 논문은 시각-언어 모델(VLM)의 시각적 추론 성능을 높이기 위해 주의(attention) 메커니즘을 분석하고 개선하는 방법을 제안하는 데 목표가 있다.  
- Motivation: 기존 연구들은 복잡한 시각 환경에서 VLM의 성능 저하 문제를 충분히 해결하지 못하며 추가 학습이나 외부 도구에 의존하는 한계를 가진다.  
- Contribution: 본 연구는 VLM의 전반적 주의도(attention entropy)와 시각적 복잡성 간의 상관관계를 규명하고, 대비(attention contrasting)를 활용한 훈련 불필요한 시각 신호 추출 기법 CARVE를 제안하였다.  

## Method  
VLM의 일반 질의와 특정 질의에 대한 주의 맵의 대비를 통해 시멘틱 신호와 시각 노이즈를 분해하는 이론적 근거를 제시하였다. CARVE는 다중 층과 시점의 주의 맵을 융합하여 중요한 시멘틱 영역을 마스킹하고 확대함으로써 시각적 노이즈를 제거한다. 해당 과정은 모델 재학습 없이 픽셀 단위에서 수행되어 시각적 집중도를 향상시킨다.  

## Results  
CARVE는 QWEN과 LLAVA 계열 VLM에서 다양한 데이터셋(A-OKVQA, POPE, V*, TextVQA)에 대해 최대 75% 성능 향상을 일관되게 달성하였다.  

## Limitations  
다양한 외부 세분화 도구 대비 CARVE는 계산 비용이 상대적으로 높으나 실용적인 GPU 처리 시간을 유지한다.  

## Conclusion  
본 연구는 시각적 복잡성과 주의 메커니즘 간 상호작용에 대한 새로운 인사이트를 제공하며, 대비 기반 주의 정제 방식을 통해 VLM의 시각 추론 성능을 효과적으로 개선할 수 있음을 입증하였다.

# 8. [UniVerse-1: Unified Audio-Video Generation via Stitching of Experts](https://arxiv.org/abs/2509.06155)

## Introduction
- Goal: 본 논문은 UniVerse-1이라는 통합 오디오-비디오 생성 모델을 제안하여, 음성과 영상의 시간적 일치성을 갖춘 동시 생성이 가능하도록 하는 것을 목표로 한다.  
- Motivation: 기존 연구들이 주로 영상 단일 모달에 치중되어 음성-영상 동기화 문제를 해결하지 못했고, 공개된 대규모 동시 생성 모델이 부재한 상황에서 이를 극복하기 위함이다.  
- Contribution: 사전 학습된 영상 및 음악 생성 전문가 모델을 심층적으로 융합하는 ‘Stitching of Experts’ 기법, 온라인 데이터 주석 파이프라인 및 잡음 독립 샘플링 기법을 도입해 효율적 학습과 고품질 음성-영상 동시 생성을 가능하게 하였다.

## Method
UniVerse-1은 WAN2.1 영상 생성 모델과 Ace-step 음악 생성 모델을 트랜스포머 블록 단위로 깊게 융합하는 SoE(stitching of experts) 방법론을 적용하며, 각 모달 간 양방향 상호작용 구조를 도입하였다. 또한, 영상과 음성의 시·의미적 정렬을 위하여 학습 중 실시간으로 데이터를 주석 처리하는 온라인 주석 파이프라인을 개발하여 정렬 오류 문제를 해소하였다. 마지막으로, 오디오와 비디오 쪽 잡음을 독립적으로 샘플링하여 잡음의 상관관계로 인한 품질 저하 문제를 해결하였다.

## Results
약 7,600시간의 정렬된 오디오-비디오 데이터를 토대로 미세 조정한 UniVerse-1은 종합 벤치마크 Verse-Bench에서 뛰어난 음성-영상 동기화 성능과 우수한 영상 정체성 유지 능력을 보였다.

## Limitations
공개된 사전 학습 모델 두 개를 융합한 구조적 한계로, 단일 모달 최첨단 전문가 모델에 비해 음성 품질에서 다소 격차가 존재한다.

## Conclusion
본 연구는 공개 가능한 동시 음성-영상 생성 모델과 관련 기술을 제시하여, 연구 커뮤니티에서 다중 모달 생성 분야의 발전에 기여할 것이다.

# 9. [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI   Agents](https://arxiv.org/abs/2509.06917)

## Introduction
- Goal: 본 논문은 연구 논문을 자동으로 인터랙티브 AI 에이전트로 변환하는 프레임워크인 Paper2Agent를 제안한다.  
- Motivation: 기존 연구 논문은 정적인 자료로서 독자가 코드, 데이터 및 방법론을 직접 이해하고 적용해야 하는 높은 진입 장벽을 지닌다.  
- Contribution: Paper2Agent는 논문과 코드베이스를 분석해 Model Context Protocol(MCP) 서버를 구축하고 AI 에이전트와 연결하여 자연어 기반 복잡한 과학적 질의를 수행할 수 있게 한다.  

## Method  
Paper2Agent는 논문에서 핵심 기여를 추출하여 MCP 서버로 구현하며, 환경 설정과 도구 추출, 테스트를 반복 수행해 신뢰성과 재현성을 확보한다.  
MCP 도구, 자원, 프롬프트를 포함하는 MCP 서버는 원격 배포되어 어떠한 LLM 기반 에이전트와도 연결 가능하다.  
사용자는 AI 에이전트를 통해 논문의 방법론을 자연어로 질의하고 새로운 데이터에도 적용할 수 있다.  

## Results  
Paper2Agent는 AlphaGenome, TISSUE, Scanpy 논문을 AI 에이전트로 변환하여 각 분야의 복잡한 분석 작업을 정확하게 재현하고, 자연어 인터랙션으로 사용자 쿼리에 100% 정확한 결과를 제공하는 것을 확인하였다.  

## Limitations  
원본 코드베이스가 불완전하거나 문서화가 부족한 경우에는 Paper2Agent가 신뢰성 있는 에이전트를 생성하지 못하는 한계가 존재한다.  

## Conclusion  
Paper2Agent는 정적인 연구 논문을 동적이고 상호작용 가능한 AI 에이전트로 전환하여 과학적 지식 전달과 협업의 새로운 패러다임을 제시한다.

# 10. [DivMerge: A divergence-based model merging method for multi-tasking](https://arxiv.org/abs/2509.02108)

## Introduction
- Goal: 본 논문은 각기 다른 태스크에 대해 학습된 모델들을 병합하여 다중 태스크 처리 성능을 유지하는 모델 병합 방식을 제안하는 데 목적이 있다.  
- Motivation: 기존 다중 태스크 학습은 데이터셋 병합 후 미세조정을 통해 이루어지나, 개별 태스크별로 미세조정된 모델의 증가로 인해 모델 병합을 통한 새로운 접근법이 요구된다.  
- Contribution: 정보 이론 기반의 제이슨-샤논 발산을 활용하여 추가 라벨링 데이터 없이 태스크 간 간섭 문제를 완화하며, 태스크 중요도를 자동 조정하는 새로운 모델 병합 방법을 제안하였다.  

## Method  
제안된 방법은 기반 사전학습 모델과 다중 태스크별 미세조정 모델 간 파라미터 차이를 나타내는 태스크 벡터(task vectors)를 이용하여 모델 병합을 수행하며, 병합 계수를 제이슨-샤논 발산을 최소화하는 최적화 문제로 정의한다. 이 과정은 가중치 분리(weight disentanglement)를 목표로 하며, 고전적 다중 태스크 학습과 이론적으로도 연결된다. 또한, 병합 과정에서 데이터 드리븐 방식을 통해 태스크별 데이터를 사용하여 최적의 병합 파라미터를 학습한다.  

## Results  
제안된 방법은 GLUE 분류 태스크 및 T5 기반 생성 태스크를 포함한 쌍태스크 병합 설정에서 기존 최첨단 기법 대비 우수한 평균 정규화 성능(ANP)을 기록하였고, 병합 태스크 수가 증가할수록 성능 저하가 적어 확장성 또한 뛰어났다.  

## Limitations  
본 연구는 전수치 미세조정(full fine-tuning) 모델에 초점을 맞추었으며, 낮은 랭크 적응(LoRA) 방식이나 실제 입력 데이터 분포 불충분 시의 성능 연구는 미흡하다.  

## Conclusion  
본 연구는 정보 이론에 기반한 참조 비의존적 데이터 드리븐 모델 병합 기법을 통해 다중 태스크 학습 문제에서 태스크 간 간섭을 효과적으로 완화하며, 안정적이고 소량의 데이터로도 우수한 병합 성능을 보임을 입증하였다.

# 11. [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,   but Not Direct the Play?](https://arxiv.org/abs/2509.03516)

## Introduction
- Goal: 본 논문은 텍스트 기반 이미지 생성 모델(Text-to-Image, T2I)의 구성(composition) 및 추론(reasoning) 능력을 포괄적이고 복잡하게 평가하기 위한 새로운 벤치마크 T2I-COREBENCH를 제안하는 데 목적이 있다.  
- Motivation: 기존의 T2I 평가 벤치마크들이 구성과 추론 능력을 개별적이고 제한적으로 평가하며 실제 복잡한 장면을 반영하지 못하는 한계가 존재한다는 점에 착안하였다.  
- Contribution: 본 연구는 12차원 평가 체계를 도입해 구성과 추론 능력을 통합 평가하고, 고밀도 구성 요소와 다단계 추론을 포함하는 1,080개의 난이도 높은 프롬프트와 약 13,500개의 점검 질문을 포함하는 벤치마크를 개발하였다.  

## Method  
성능 평가를 위해 장면 그래프 요소(인스턴스, 속성, 관계)를 통한 구성 능력과 철학적 추론 틀(연역, 귀납, 가설 추론)을 통한 추론 능력의 12차원 분류 체계를 설계하였다.  
복잡성 증대를 위해 프롬프트 내 구성 요소를 평균 약 20개 이상으로 확대하고, 추론은 한 행동이 다중 결과를 낳거나 여러 전제로부터 결론을 도출하는 시나리오를 포함하였다.  
각 프롬프트에는 체크리스트 형태의 독립적인 예/아니오 질문을 부여하여 미세하고 신뢰성 있는 평가가 가능하게 하였다.  

## Results  
27개의 최신 T2I 모델을 평가한 결과, 고밀도 복잡한 구성 상황에서는 구성 능력이 제한적이며 암시적 요소 추론이 요구되는 추론 능력은 더욱 크게 부족함을 확인하였다.  

## Limitations  
현재 모델들은 복잡한 장면 구성과 다단계 추론에 있어 전반적으로 미흡하여 T2I 생성의 핵심 병목으로 작용하고 있다.  

## Conclusion  
T2I-COREBENCH는 텍스트-이미지 모델이 고도의 구성 및 추론 능력을 요구하는 실제 복잡한 상황을 정량적으로 평가할 수 있는 최초의 포괄적이고 고난도 벤치마크로서, 향후 T2I 연구 발전에 중요한 기반을 제공한다.

# 12. [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)

## Introduction
- 본 연구의 목표는 교차 텍스트-이미지 중첩 추론 방식을 통해 텍스트-이미지 변환(Text-to-Image, T2I) 생성의 품질을 향상하는 것이다.  
- 기존 통합 다중 모달 모델들이 이미지 생성에서 상세한 지시 준수와 미세한 디테일 보존 측면에서 한계가 존재함에 동기를 얻었다.  
- 본 논문은 텍스트 기반 추론과 이미지 생성을 교차 반복 수행하는 인터리빙 리즈닝 제너레이션(IRG) 프레임워크를 제안하였다.  

## Method  
IRG는 (1) 텍스트 기반 추론으로 초기 이미지를 생성하고 (2) 이를 반성하여 시각적 품질과 세부 묘사를 개선하는 두 단계로 구성된다. 학습을 위해 IRG-300K 데이터셋을 구축하고 여섯 가지 학습 모드로 모델을 계층적으로 강화하였다. 또한, 두 단계의 학습 파이프라인을 통해 초기 추론 강화와 고품질 이미지 생성 능력을 효과적으로 달성하였다.  

## Results  
제안된 IRG는 GenEval, WISE, TIIF, GenAI-Bench, OneIG-EN 등 여러 주류 벤치마크에서 기존 최첨단 모델 대비 5~10점의 절대 성능 향상을 보이며 T2I 생성에서 뛰어난 시각 품질과 미세 디테일 정확도를 달성하였다.  

## Limitations  
완전한 IRG 학습 데이터를 대규모로 구축하는 것이 이미지 품질 제약과 초기 이미지-개선 이미지 쌍 데이터 부족으로 인해 어려움이 있다.  

## Conclusion  
본 연구는 교차 중첩 추론 방식이 텍스트-이미지 생성 분야에서 우수한 성능 향상을 이루는 강력한 패러다임임을 선보였다.

# 13. [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)

## Introduction
- Goal: 본 연구는 Retrieval-Augmented Generation (RAG) 시스템에서 가이드 디코딩 기법이 구조화된 출력 생성에 미치는 영향을 분석하는 데 목적이 있다.  
- Motivation: 대규모 언어 모델(LLM)의 활용이 증가함에 따라, 사용자 요구에 맞는 형식적이고 신뢰할 수 있는 응답을 보장하는 것이 중요해졌다.  
- Contribution: 본 논문은 Outlines, XGrammar, LM Format Enforcer 세 가지 가이드 디코딩 방법의 성능을 다중 대화 턴 환경에서 평가하여 각 기법의 장단점과 적용성을 제시하였다.  

## Method  
본 연구는 vLLM 기반의 추론 엔진과 세 가지 가이드 디코딩 백엔드(Outlines, XGrammar, LM Format Enforcer)를 사용하여 여러 대화 턴(0, 1, 2턴) 시나리오에서 RAG 성능을 비교하였다.  
실험은 문서 검색, 모델 입력 설정, 가이드 디코딩 구성, 다중 턴 대화 평가의 네 단계로 수행되었으며, 알고리즘을 통해 참조 문서 정확성과 허위 정보 발생률을 측정하였다.  
각 가이드 디코딩 방법은 유한 상태 기계, 푸시다운 오토마타, 문자 수준 확률 필터링 등의 기법으로 출력의 구조적 유효성을 보장하였다.  

## Results  
다중 턴 대화 증가에 따라 세 가지 기법 모두 오류가 감소했으며, 특히 LM Format Enforcer가 전반적으로 허위 정보 발생률이 가장 낮았고 Outlines와 XGrammar는 다중 대화 맥락에서 큰 성능 향상을 보였다.  

## Limitations  
Outlines는 정규 표현식 지원이 제한적이고 비ASCII 문자 처리에 제약이 있으며, LM Format Enforcer는 엄격한 구조 준수 때문에 유연성이 떨어지고 XGrammar는 규칙 작성의 복잡성과 계산 비용 문제를 안고 있다.  

## Conclusion  
본 연구는 구조화된 출력 생성과 다중 턴 프롬프팅을 결합한 가이드 디코딩이 RAG 시스템에서 사실 정확성과 응답 신뢰도를 크게 향상시킨다는 점을 확인하였다.

# 14. [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers](https://arxiv.org/abs/2509.06493)

## Introduction
- Goal: 본 연구는 대규모 언어 모델(LLM)을 활용한 자동 정리 증명을 위한 훈련 및 추론 과정의 확장 문제를 해결하는 것이다.  
- Motivation: 기존 LLM 기반 정리 증명에서는 훈련 시 강화학습의 성능 정체 현상과 추론 시 탐색 공간이 급격히 증가하는 문제로 확장이 제한되었다.  
- Contribution: 이에 다중 단계 전문가 반복 학습과 계획자-다중 에이전트 탐색 구조를 도입한 BFS-Prover-V2 시스템을 제안하였다.  

## Method  
훈련 단계에서는 AlphaZero에서 영감을 받은 다중 턴 오프 정책 강화학습 프레임워크를 사용하며, 전술 단위 난이도 기반 적응형 데이터 필터링과 주기적 재학습을 도입하여 성능 정체를 극복하였다.  
추론 단계에서는 고수준의 계획자 역할을 하는 일반 추론 모델이 복잡한 정리를 하위 목표로 분해하고, 여러 병렬 증명자 에이전트가 공유된 하위 목표 캐시를 활용하여 효율적으로 협업하는 다중 에이전트 트리 탐색 아키텍처를 설계하였다.  
이러한 두 축의 확장 방식을 통합하여 훈련과 추론의 복합적인 문제를 동시에 해결하였다.  

## Results  
BFS-Prover-V2는 고등학교 수준 수학 문제를 다루는 MiniF2F 테스트셋에서 95.08%, 학부 수준 복합 문제를 대상으로 하는 ProofNet 테스트셋에서 41.4%의 최고 성능을 기록하였다.  

## Limitations  
본 논문에서는 제안한 방법의 적용이 형식적 수학 증명 분야에 집중되어 있으며, 다른 도메인에서의 일반화 가능성에 대해서는 추가 연구가 필요하다.  

## Conclusion  
본 연구는 다중 단계 강화학습과 계획자 기반 다중 에이전트 추론 아키텍처를 통해 LLM 기반 정리 증명기의 훈련 및 추론 확장 문제를 효과적으로 극복하고 최첨단 성능을 달성하였음을 입증하였다.

# 15. [Test-Time Scaling in Reasoning Models Is Not Effective for   Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)

## Introduction
- Goal: 본 연구는 대규모 추론 모델에서 테스트 시 연산량을 늘리는 test-time scaling 기법이 지식 집약적 과제에서 효과적인지 평가하는 것이다.  
- Motivation: 최근 모델들이 긴 사고 과정을 통해 성능을 개선하는 경향을 보이나, 실제 지식 정확도와 환각 방지 측면에서는 한계가 보고되고 있기 때문이다.  
- Contribution: 12개 추론 모델을 대상으로 두 개 지식 집약적 벤치마크에서 test-time scaling의 정확도와 환각률 변화를 종합 분석하였다.  

## Method  
두 벤치마크(SimpleQA, FRAMES)에서 모델별 reasoning 길이를 다양하게 조정하며 정확도와 환각률을 평가하였다.  
모델은 reasoning effort, thinking budget, budget forcing 세 가지 유형으로 구분하여 실험을 수행하였다.  
응답 평가는 자동 평가자를 이용해 정확성, 부적절 응답(환각)을 분류하였다.  

## Results  
대부분 모델에서 reasoning 시간을 늘려도 정확도가 일관되게 개선되지 않았으며, 오히려 일부 모델은 환각률이 증가하는 추세를 보였다.  

## Limitations  
본 연구는 짧은 형식의 정답에 한정된 벤치마크에만 적용되어, 장문 생성이나 개방형 과제에는 결과의 일반화가 어렵다.  

## Conclusion  
test-time scaling은 지식 집약적 과제에서 사실적 정확도 향상이나 환각 감소에 아직 효과적이지 않으며, 적절한 사고 활성화는 도움이 되지만 연산 시간 증가는 신뢰성 개선 전략으로 적합하지 않다.

# 16. [R^textbf{2AI}: Towards Resistant and Resilient AI in an   Evolving World](https://arxiv.org/abs/2509.06786)

## Introduction
- Goal: 본 논문은 급격히 발전하는 인공지능(AI) 기술과 안전성 확보 간의 격차를 해소하기 위해 내재적 안전성과 적응적 능력을 가진 AI 체계 설계를 목표로 한다.  
- Motivation: 기존 AI 안전 연구는 사후 대응 또는 고정된 안전 장치에 의존하여 빠르게 변화하는 위협과 예측 불가능한 위험에 효과적으로 대응하지 못하는 한계가 존재한다.  
- Contribution: 생물학적 면역체계에 영감을 받은 ‘공진화 기반 안전성(safe-by-coevolution)’ 원리를 제안하고, 이를 구현하는 실용적 프레임워크인 R2AI(저항성과 회복력을 갖춘 AI)를 소개하였다.  

## Method  
R2AI는 빠른 반응을 담당하는 Fast Safe Model과 깊이 있는 재귀적 판단을 수행하는 Slow Safe Model로 구성되며, 안전성 풍동(Safety Wind Tunnel)에서 적대적 공격을 시뮬레이션하고 검증한다. 이러한 구성요소들은 지속적 피드백과 자동 학습을 통해 알려진 위협에 대한 저항성과 예상치 못한 위험에 대한 회복력을 동시에 갖추도록 공진화한다. 또한, 안전보장 초기화, 반복 안전 업그레이드, 그리고 재설정-복구 메커니즘을 통한 연속 안전성을 확보하는 3단계 안전 진화 과정을 제안하였다.  

## Results  
R2AI 프레임워크는 AI 성능 향상과 안전성 유지가 동시 달성되는 45도 법칙(AI-45°Law)을 따르는 공진화적 안전성 확보의 가능성을 이론적으로 입증하였다.  

## Limitations  
본 논문에서는 이론적 및 구조적 틀을 제시하였으나, 구체적 구현 및 실험적 검증은 향후 연구 과제로 남겨두었다.  

## Conclusion  
공진화에 기반한 R2AI는 역동적인 환경에서 AI 안전성을 내재적으로 진화시키는 확장 가능하고 반응적인 안전 프레임워크로 제안되었다.

# 17. [D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning](https://arxiv.org/abs/2509.06771)

## Introduction
- 본 연구의 목표는 멀티모달 데이터에서 암흑 유머(dark humor)를 이해하고 탐지하는 프레임워크를 제안하는 것이다.  
- 암흑 유머는 문화적 맥락과 민감한 내용에 의존하여 멀티모달적으로 표현되므로, 이를 효과적으로 분석하기 위한 데이터 및 기법이 매우 부족한 실정이다.  
- 이에 본 연구는 Reddit으로부터 수집한 4,379개의 암흑 유머 밈 데이터셋과, 자체 생성·정제된 구조화된 설명을 활용하는 새로운 멀티모달 분류 모델을 함께 제안하였다.  

## Method  
본 연구는 Qwen-2.5-7B Vision-Language 모델을 통해 밈별로 구조화된 설명을 생성하고, Role-Reversal Self-Loop 기법으로 이 설명을 저자 관점에서 반복 정제한다. 정제된 설명과 OCR로부터 추출한 텍스트, 그리고 ViT 기반 시각 정보를 각각 임베딩하여 Tri-stream Cross-Reasoning Network(TCRNet)으로 세 스트림 간 쌍별 주의를 적용, 통합 표현을 학습한다. 최종 분류기를 통해 암흑 유머 존재, 대상 카테고리, 유머 강도를 예측한다.  

## Results  
제안된 TCRNet은 암흑 유머 탐지 정확도 75.00%, 유머 강도 예측 정확도 62.72%로 기존 텍스트, 이미지 단일 모달리티 모델 및 제로샷 VLM 대비 우수한 성능을 기록하였다.  

## Limitations  
데이터셋은 성별 관련 주제가 다수 포함되어 있어 편향 가능성이 있으며, 향후 세분화된 타겟 클래스 추가 및 적응 기법 연구가 필요하다.  

## Conclusion  
멀티모달 밈 데이터에서 암흑 유머를 정확히 분류하기 위해서는 명시적이며 반복적으로 정제된 구조화된 설명과 텍스트·이미지 통합 기법이 필수적이며, 본 연구의 D-Humor 데이터셋과 TCRNet 모델이 이에 효과적으로 기여함을 확인하였다.

# 18. [Llama-GENBA-10B: A Trilingual Large Language Model for German, English   and Bavarian](https://arxiv.org/abs/2509.05668)

## Introduction
- Goal: 본 연구는 영어 중심 편향을 해소하고 독일어, 영어, 바이에른어 세 언어를 대상으로 하는 10B 파라미터 규모의 삼중언어 기반 모델 Llama-GENBA-10B를 제안하는데 그 목적이 있다.  
- Motivation: 기존 대형 언어 모델은 영어 데이터에 편중되어 있어 저자원 언어 및 방언에 대한 성능이 저조하며, 이를 개선하기 위한 균형 잡힌 다국어 모델 개발이 필요하다.  
- Contribution: 본 연구는 균형 잡힌 언어 자원 수집, 통합 토크나이저 설계, 최적 아키텍처 및 언어 비율 탐색, 및 독일어 벤치마크의 바이에른어 번역을 통한 최초의 삼중언어 평가 도구 개발 등을 수행하였다.  

## Method  
독일어, 영어, 바이에른어의 1640억 토큰 규모의 균형 잡힌 삼중언어 말뭉치를 구축하였으며, 바이에른어 데이터는 희소성을 고려해 학습 후반부에 추가하였다.  
Llama 3.1-8B 모델을 10B 규모로 확장하고, 20% 어휘 확장을 적용한 통합 토크나이저를 개발하여 세 언어를 효율적으로 처리하도록 하였다.  
Cerebras CS-2 AI 가속기를 활용해 대규모 다국어 사전학습과 감독 학습 기반의 미세 조정을 수행하였다.  

## Results  
Llama-GENBA-10B는 기본 모델에서 영어와 바이에른어에서 우수한 성능을 보였고, 미세 조정 후 바이에른어에서는 경쟁 모델을 앞서는 최고 성능을 기록하였다.  

## Limitations  
본 연구는 학습 데이터의 바이에른어 자원 부족으로 인해 바이에른어 데이터를 후반 학습에만 투입하는 등 일부 자원의 한계를 극복하는 데 제약이 있었다.  

## Conclusion  
Llama-GENBA-10B는 다국어 균형성을 강화하고 저자원 지역어 지원이 가능한 효율적인 대형 언어 모델의 설계 및 학습 방안을 제시하며, 소규모 연구팀의 대규모 다국어 모델 개발 가능성을 입증하였다.

# 19. [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI   Agents](https://arxiv.org/abs/2509.06477)

## Introduction
- Goal: 본 연구는 모바일 GUI 에이전트의 효율성을 높이기 위해 GUI 조작과 단축키를 결합한 하이브리드 에이전트를 체계적으로 평가할 수 있는 최초의 벤치마크 MAS-Bench를 제안하는 것이다.  
- Motivation: 기존 GUI 전용 에이전트는 유연성을 제공하나 다단계 조작에 따른 비효율성 문제를 가지며, 하이브리드 GUI-단축키 에이전트 평가를 위한 체계적인 프레임워크가 부족하였다.  
- Contribution: MAS-Bench는 11개 실제 앱 기반 139개 복잡 과제, 88개의 미리 정의된 단축키, 그리고 에이전트가 단축키를 자율 생성할 수 있는 능력을 평가하는 신규 프레임워크를 제공한다.  

## Method  
MAS-Bench는 GUI 조작과 API, 딥링크, RPA 스크립트 등을 포함하는 단축키를 결합한 하이브리드 액션 공간에서 동작하는 모바일 에이전트를 대상으로 한다.  
에이전트는 GUI 전용 조작뿐 아니라 단축키를 효율적으로 선택하고, 반복 작업에 대해 자율적으로 단축키를 생성하여 수행 효율성을 개선한다.  
평가는 임의 환경에서 실시간 수행 능력과 단축키 생성 및 활용 능력을 7가지 다차원 지표로 측정하는 방식으로 진행된다.  

## Results  
MAS-Bench 실험 결과, 하이브리드 GUI-단축키 에이전트는 GUI 전용 모델 대비 성공률이 최대 64.1%로 44.6%에 비해 크게 증가하고, 실행 효율성도 40% 이상 향상되었다.  

## Limitations  
현재까지 비교군이 제한적이며, 모바일 GUI-단축키 하이브리드 에이전트 연구가 미비하여 연구 확장과 다양한 벤치마크가 요구된다.  

## Conclusion  
본 연구는 GUI-단축키 하이브리드 모바일 에이전트의 수행 효율성과 자율 단축키 생성 능력을 종합적으로 평가하는 MAS-Bench를 제안하여 해당 분야 연구 발전의 기초 플랫폼을 마련하였다.

# 20. [SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)

## Introduction
- Goal: 본 논문은 강화학습을 통해 자율적으로 추론하며 도구를 활용하는 단일 에이전트 모델을 개발하는 것을 목표로 한다.  
- Motivation: 복잡한 심층 연구 과제 수행 시 다중 에이전트 시스템의 고정된 역할 제약을 벗어나 단일 에이전트가 동적으로 행동을 결정하는 유연한 접근법이 필요하기 때문이다.  
- Contribution: 합성 데이터 기반의 단순한 강화학습 레시피를 제안하고 이를 다양한 오픈소스 대형언어모델에 적용하여 에이전트 능력과 추론 능력을 동시에 향상시켰다.  

## Method  
도구 사용은 웹 검색, 웹 크롤링, Python 코드 실행의 최소한의 기능만 제공하여 에이전트가 효율적으로 탐색하고 도구를 전략적으로 사용하도록 유도한다.  
에이전트는 다중 턴 대화를 단일 턴의 맥락적 질문 응답으로 재구성하고 메모리 관리 도구를 이용해 긴 롤아웃 동안 중요 정보만 유지하도록 설계되었다.  
강화학습은 REINFORCE 기반 알고리즘에 경로 길이 정규화와 전략적 경로 필터링을 도입해 긴 에피소드의 불안정성을 완화한다.  

## Results  
SFR-DR-20B 모델은 Humanity’s Last Exam 벤치마크에서 28.7%의 성과를 기록하며 유사 크기 및 일부 비공개 모델을 능가하는 성능을 보여주었다.  

## Limitations  
모델의 다중 턴 추론 능력 및 긴 맥락 관리 능력에 여전히 한계가 존재하며 복잡한 도구 사용 경로에서 비효율적 행동이 나타날 수 있다.  

## Conclusion  
본 연구는 강화학습과 합성 학습 데이터 활용을 통해 추론 최적화 대형언어모델을 자율 단일 에이전트 딥리서치 시스템으로 성공적으로 전환시켰음을 입증하였다.

# 21. [Mechanistic interpretability for steering vision-language-action models](https://arxiv.org/abs/2509.00328)

## Introduction
- Goal: 본 연구는 Vision-Language-Action (VLA) 모델의 내부 표현을 해석하고 이를 통해 추론 시 행동을 직접 조작하는 메커니즘적 해석 가능성과 조종 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 기존 로봇공학에서 사용하는 운동학, 역학 및 제어의 명시적 모델에 기반한 해석과 달리, VLA 모델은 해석과 제어의 투명성이 부족하여 실제 환경에서의 안전성과 신뢰성 확보에 어려움이 존재한다.  
- Contribution: 본 논문은 대형 언어 모델에서 발전한 메커니즘적 해석 기법을 VLA에 최초로 적용하여, FFN(feed-forward network) 활성화 벡터를 의미 기반 군집화하고 이를 실시간 조종에 활용하는 방법론을 제안하였다.  

## Method  
VLA 모델의 트랜스포머 내부 FFN 활성화 벡터를 토큰 임베딩 공간으로 사영하여 의미적 방향을 탐색하였다.  
의미가 부여된 뉴런 클러스터를 찾아 활성화 값을 조작함으로써 별도의 학습 없이 추론 시 행동 조정을 가능하게 하였다.  
실시간 조종은 OPENVLA와 π0 모델에 적용되어 시뮬레이션과 실제 로봇(UR5)에서 평가되었다.  

## Results  
실험 결과, 제안한 의미 기반 신경 활성화 조작은 시뮬레이션 LIBERO 및 UR5 로봇의 장기 조작 및 이진 행동 대조 과제에서 효과적인 제로샷 행동 조종을 입증하였다.  

## Limitations  
의미 클러스터링의 애매성과 표현의 변화 문제, 미세조정에 따른 조종 가능성 변화, 그리고 평가가 제한적인 조작 작업에 국한된 점이 주요 한계로 지적되었다.  

## Conclusion  
본 연구는 VLA 모델 내에 의미론적 개념이 보존되고, 소수 의미 뉴런의 활성화 조작을 통해 재학습 없이도 로봇 행동을 투명하고 효과적으로 조종할 수 있음을 실증하였다.

# 22. [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in   the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)

## Introduction
- Goal: 본 연구의 목표는 수학적 추론 능력을 향상시키기 위한 고품질의 논리적으로 타당한 데이터셋을 자동 정리 기반으로 대규모 생성하는 프레임워크를 제시하는 것이다.  
- Motivation: 기존 대규모 언어 모델(LLM)의 다단계 논리적 추론 성능이 한정적인 이유로서, 고품질 수학적 훈련 데이터의 심각한 부족 문제가 존재한다.  
- Contribution: 본 연구는 포화 기반 증명 엔진인 E-prover와 TPTP 공준 라이브러리를 활용하여, 오류 없는 수학 정리를 대규모로 생성하고 세 가지 난이도 조절 가능한 추론 태스크로 전환하는 체계적인 데이터 생성 및 평가 프레임워크를 개발하였다.

## Method  
본 프레임워크는 TPTP 라이브러리의 공리 집합을 E-prover의 포화(saturation) 기법으로 포괄적으로 추론하여 증명 그래프를 생성한다.  
이후 AGInTRater를 활용한 "흥미로운" 정리 선별과 Vampire 증명기를 통한 타당성 검증 과정을 거쳐, 진위 판단, 최소 전제 선택, 증명 그래프 재구성의 세 가지 태스크로 변환한다.  
이 과정은 완전한 기호적 접근으로 LLM의 사실 오류를 제거하고, 난이도 조절 파라미터를 통해 점진적 논리 난제를 생성 가능하게 한다.

## Results  
최신 LLM을 대상으로 한 제로샷 실험에서 다단계 구조적 추론이 요구되는 증명 그래프 재구성에서 성능이 급격히 저하되며, 모델 크기에 따른 성능 차이가 존재하지만 구조적 추론 자체는 여전히 극복되지 못함이 드러났다.

## Limitations  
본 프레임워크는 자연어 처리 복잡성을 배제한 완전한 절대화된 논리 형태(CNF)에 집중하며, 현재는 GPT 계열 모델만 평가하고 대규모 데이터 생성은 향후 확장 과제로 남아 있다.

## Conclusion  
기호적 자동화 증명 도구를 활용한 체계적 데이터 생성 프레임워크는 LLM의 수학적 추론 한계를 진단하고, 복잡한 논리 태스크의 학습을 위한 지속 가능한 데이터 기반을 제공하는 중요한 진전을 이룩하였다.

# 23. [DCReg: Decoupled Characterization for Efficient Degenerate LiDAR   Registration](https://arxiv.org/abs/2509.06285)

## Introduction
- Goal: 본 연구는 기하학적으로 축퇴된 LiDAR 환경에서 로봇 위치 추정 문제의 수치적 불안정성(ill-conditioning)을 정확히 검출, 해석 및 해결하는 통합적 프레임워크인 DCReg를 제안하는 것이다.  
- Motivation: 기존 LiDAR 등록 기법은 축퇴 환경에서의 회전-병진 간 결합효과와 크기 불균형 문제로 인해 불안정성을 정확히 탐지하거나 정량적으로 해석하지 못하며, 복원 단계에서 불필요한 정보 손실이나 왜곡이 발생하는 한계를 지닌다.  
- Contribution: 본 연구는 슈어 보완 분해를 통한 회전과 병진의 결합 분리, 물리적으로 해석 가능한 방향별 축퇴 정량화, 그리고 선별적 사전조건화를 통한 최적화 안정화 기법을 통합한 새로운 해결책을 제안한다.  

## Method  
DCReg는 헤시안 행렬에 슈어 보완 분해를 적용하여 회전 및 병진 파라미터 공간을 독립적이고 규모에 일관된 서브스페이스로 분리함으로써 축퇴 탐지를 개선한다. 이후 내적분석 및 그램-슈미트 정규 직교화 기법으로 각 서브스페이스의 물리적 운동 방향과 축퇴 성분 간 명확한 대응 관계를 수립하고, 이를 바탕으로 클러스터별 고유값 조절이 가능한 선별적 사전조건자를 설계하여 의도한 축퇴 방향만 안정화한다. PCG 기법과의 결합으로 단일 물리적 해석 매개변수로 기존 정보 보존과 함께 효율적이고 견고한 최적화를 달성한다.  

## Results  
광범위한 실험 결과, DCReg는 기존 최첨단 방법 대비 위치 추정 정확도가 20%~50% 향상되고 연산 속도는 5~100배 빠름을 보였으며 다양한 축퇴 및 환경 조건에서의 강인성을 입증하였다.  

## Limitations  
정보 부족  

## Conclusion  
DCReg는 축퇴된 LiDAR 등록 문제에 대해 투명한 물리적 해석과 정보 손실 없는 안정화를 동시에 제공함으로써 자율 로봇 시스템의 내비게이션 신뢰성을 획기적으로 향상시킨다.

# 24. [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing   via Bidirectional Warping](https://arxiv.org/abs/2509.04582)

## Introduction
- 본 논문은 픽셀 공간에서 양방향 워핑과 이미지 인페인팅을 결합하여 직관적인 드래그 기반 이미지 편집을 실시간으로 구현하는 Inpaint4Drag 프레임워크를 제안하는 데 목적이 있다.  
- 기존 드래그 기반 편집법은 생성 모델의 잠재 공간 조작에 의존함에 따라 정밀도 저하, 피드백 지연, 모델 특화 제약 등의 한계를 보였다.  
- 제안하는 방법은 탄성체 변형에서 영감을 받은 픽셀 단위 양방향 워핑과 표준 인페인팅 모델 연동을 통해 높은 정밀도와 실시간 인터랙티브 편집을 달성한다.  

## Method  
Inpaint4Drag는 사용자가 지정한 변형 영역을 마스크화하고 이를 다중 제어점의 드래그 입력으로 양방향 픽셀 워핑하여 자연스러운 변형을 수행한다. Forward warping으로 초기 변형을 정의하고 backward mapping으로 픽셀 공백을 채워 디테일을 보존하며, 변형된 영역에 대한 인페인팅 마스크를 산출해 기존 인페인팅 모델에 입력한다. SAM 기반 마스크 정제 모듈과 결합된 모듈형 파이프라인으로 변형과 생성 과정을 분리해 효율성과 정확성을 동시에 만족시킨다.  

## Results  
DragBench-S와 DragBench-D 벤치마크에서 Inpaint4Drag는 평균 거리(MD) 및 인지적 유사도(LPIPS)에서 최고 또는 경쟁력 있는 성능을 보이며, 기존 기법 대비 최대 600배 빠른 처리 속도(약 0.3초)를 달성하였다.  

## Limitations  
정보 부족.  

## Conclusion  
Inpaint4Drag는 양방향 픽셀 워핑과 인페인팅 모델의 결합을 통해 정밀하고 실시간성 높은 드래그 기반 이미지 편집을 가능하게 하며, 모든 인페인팅 모델과 호환되어 향후 기술 발전을 즉시 수용할 수 있는 범용 솔루션임을 입증하였다.

# 25. [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)

## Introduction
- Goal: 본 연구는 Singular Value Decomposition(SVD)을 활용하여 CLIP 비전-언어 모델을 효율적으로 적응시키는 새로운 파라미터 절약형 적응 기법인 CLIP-SVD를 제안하는 것이다.  
- Motivation: 기존의 VLM 적응 방법들은 프롬프트 엔지니어링에 의존하거나 모델 전체 미세조정에 높은 계산 비용이 드는 문제, 추가 모듈 삽입으로 인한 모델 불안정 및 학습된 지식 손실 우려가 존재하였다.  
- Contribution: CLIP-SVD는 기존 학습된 파라미터를 보존하면서 SVD를 통해 파라미터 행렬의 특잇값만 미세조정하여 자연 및 생의학 영역에서 소수샘플 응용에 탁월한 성능과 해석력을 동시에 달성하였다.

## Method  
CLIP-SVD는 CLIP의 이미지 및 텍스트 인코더 내 Transformer 층의 다중 헤드 어텐션과 피드포워드 네트워크 가중치를 SVD로 분해하여, 특잇값만 미세조정하는 방식을 취한다. 이 방법은 전체 파라미터의 0.04%만을 학습시켜 효율성을 극대화하고, 기존의 특잇벡터 방향은 고정함으로써 프리트레인된 지식 유지가 가능하다. 또한, TextSpan을 활용해 적응 과정에서 어텐션 헤드의 의미론적 변화를 자연어 기반으로 해석할 수 있도록 하였다.

## Results  
자연 및 생의학 분야 총 21개 데이터셋에서 1~16 샷 실험 결과, CLIP-SVD는 기존 최고 기법 대비 최대 +4.28%의 정확도 향상을 달성하며 강력한 일반화 및 크로스-도메인 전이 성능을 보였다.

## Limitations  
본 연구에서 구축한 생의학 이미지 설명 말뭉치 기반의 자연어 해석 방법은 전문가 검증과 광범위한 적용에서 추가적인 확인이 필요하다.

## Conclusion  
CLIP-SVD는 특잇값 미세조정만으로 효율적이고 효과적인 VLM 도메인 적응을 실현함으로써 자연어 기반 해석 가능성과 함께 자연·생의학 분야 모두에서 최첨단 성능을 입증하였다.
