---
layout: post
title: "Daily Papers — 2025-09-29"
date: 2025-09-29 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22622)

## Introduction
- Goal: 본 연구는 실시간으로 상호작용할 수 있는 장시간 영상 생성 프레임워크인 LONGLIVE를 제안하는 데 목적이 있다.  
- Motivation: 장시간 영상 생성은 효율성 및 품질 유지에 어려움이 있으며, 특히 사용자 프롬프트 전환 시 시각적 일관성과 의미적 연속성 확보가 중요하다.  
- Contribution: LONGLIVE는 프롬프트 전환 시 캐시 상태를 새롭게 갱신하는 KV-recach 기술과 긴 영상 학습을 위한 스트리밍 롱 튜닝, 그리고 고속 생성을 위한 짧은 윈도우 주의와 프레임 싱크를 결합하여 실시간 인터랙티브 장영상 생성을 실현하였다.  

## Method  
LONGLIVE는 인과적 프레임 단위 자기회귀 모델로 KV-recach를 통해 프롬프트 전환 시 자연스러운 전환과 의미 일치를 유지한다. 스트리밍 롱 튜닝은 긴 시퀀스 생성 과정에서 발생하는 누적 오류를 완화하고 학습과 추론을 일치시킨다. 또한 짧은 윈도우 주의와 프레임 싱크를 활용하여 계산 비용과 메모리 사용량을 줄이면서도 장시간 영상의 시각적 일관성을 유지한다.  

## Results  
LONGLIVE는 단일 NVIDIA H100 GPU에서 20.7 FPS의 실시간 추론 속도를 달성하며, 최대 240초 길이의 영상을 생성하고, VBench 벤치마크에서 기존 최첨단 기법들보다 우수한 품질과 의미 일치도를 기록하였다.  

## Limitations  
본 연구에서는 KV-recach 비용과 긴 영상 학습 시 메모리 제한 등 일부 효율성 관련 제약이 존재하며, 복잡한 멀티 프롬프트 상황에서 추가 연구가 필요하다.  

## Conclusion  
LONGLIVE는 실시간 상호작용이 가능한 장시간 영상 생성에 최적화된 프레임워크로서, 효율성과 고품질을 동시에 구현하며 INT8 양자화로 메모리 효율까지 확보하였다.

# 2. [MinerU2.5: A Decoupled Vision-Language Model for Efficient   High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22186)

## Introduction
- Goal: MinerU2.5는 고해상도 문서 파싱을 위해 글로벌 레이아웃 분석과 로컬 콘텐츠 인식을 분리한 효율적인 비전-언어 모델을 제안하는 것이다.  
- Motivation: 기존 문서 파싱 모델들은 고해상도 입력 처리 시 연산 효율성 저하와 긴 문서 내의 중복 토큰 문제에 직면한다는 한계가 존재한다.  
- Contribution: MinerU2.5는 1.2억 파라미터 경량 모델로 두 단계(저해상도 레이아웃 분석, 고해상도 주요 영역 인식)를 분리 적용하여 최첨단 정확도와 높은 계산 효율성을 동시에 달성하였다.  

## Method  
MinerU2.5는 다운샘플링된 이미지를 활용해 빠르고 전체적인 레이아웃을 분석한 후, 검출한 영역을 원본 해상도에서 부분적으로 크롭하여 세밀한 콘텐츠 인식을 수행한다. 이러한 coarse-to-fine 이중 단계 파싱 전략을 통해 불필요한 전역 토큰 처리 비용을 낮추고, 각 단계별 최적화를 가능케 했다. 데이터 엔진을 통해 다양한 대규모 문서 코퍼스를 구축하고, 이를 기반으로 다단계 사전학습과 미세조정을 진행하였다.  

## Results  
MinerU2.5는 OmniDocBench를 포함한 여러 벤치마크에서 일반 목적 및 도메인 특화 모델을 능가하는 텍스트, 수식, 표 인식 및 읽기 순서 예측 정확도를 기록함과 동시에, 페이지 처리 속도 및 토큰 처리량 측면에서도 최고 수준의 연산 효율을 나타냈다.  

## Limitations  
복잡한 문서 구조나 희귀한 문서 유형에 대해선 추가 데이터 보강과 미세조정이 필요하다는 제한점이 존재한다.  

## Conclusion  
MinerU2.5는 고해상도 문서 파싱에서 정확도와 효율성을 균형 있게 향상시키는 분리형 비전-언어 모델로서, 실제 응용에서의 확장 가능성과 실용성을 크게 제고하였다.

# 3. [ReviewScore: Misinformed Peer Review Detection with Large Language   Models](https://arxiv.org/abs/2509.21679)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21679)

## Introduction
- Goal: 본 연구는 대형 언어 모델을 활용하여 학술 논문 심사에서 잘못된 정보에 기반한 평가(미스인포메이션)가 포함된 피어 리뷰 포인트를 자동으로 탐지하는 방법을 제안하는 데 있다.  
- Motivation: AI 학술대회 제출 논문 수의 급증에 따라 심사 품질이 저하되고 있으며, 잘못된 정보에 기반한 리뷰가 상당 부분 존재한다는 점에서 이를 신뢰성 있게 감별하는 기준 및 자동화가 요구된다.  
- Contribution: 본 연구는 질문의 논문 내 답변 가능성 및 약점의 사실성 판단이라는 구체적이고 적용 가능한 리뷰 품질 기준인 REVIEWSCORE를 정의하고, 자동 근거 재구성 엔진과 인간 전문가가 주석한 데이터셋을 구축하여 대형 언어모델의 평가 신뢰성을 검증하였다.  

## Method  
REVIEWSCORE는 각 리뷰 포인트를 질문과 약점으로 구분하며, 질문이 논문으로 답변 가능하거나 약점이 사실에 부합하지 않을 경우 미스인포메이션으로 정의된다.  
약점 내 명시적·암묵적 근거를 자동으로 추출하는 논리적 근거 재구성 엔진을 개발하여, 근거 단위 사실성 점수를 집계하는 고도화된 평가 방식을 도입하였다.  
또한, 15명의 석사급 연구자가 참여한 인간 전문가 주석 데이터셋을 기반으로 여덟 개 최첨단 LLM을 활용하여 자동 평가의 신뢰도를 측정하였다.  

## Results  
여덟 개 LLM 모델은 인간 평가자와의 REVIEWSCORE 측정에서 중간 수준의 일치도를 보였으며, 특히 근거 단위 사실성 평가를 활용한 고도화된 REVIEWSCORE 방식이 기본 방식 대비 약 2배 이상 높은 평가 일치를 달성하였다.  

## Limitations  
본 연구는 PDF 파싱 오류와 도메인별 제한된 데이터, 인간 주석의 주관성 등으로 인해 완전한 자동 평가 시스템 구현에는 기술적·실용적 한계가 존재한다.  

## Conclusion  
본 연구는 대형 언어모델을 활용해 학술 리뷰 내 잘못된 정보 포함 여부를 효과적으로 탐지할 수 있음을 보이며, AI 학회 심사 품질 향상을 위한 자동화 평가 체계 구축 가능성을 제시하였다.

# 4. [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary   Learning](https://arxiv.org/abs/2509.22075)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22075)

## Introduction
- Goal: 본 논문은 후처리 없이 대형 언어 모델(LLM)을 효과적으로 압축하기 위해 교정 데이터 기반 희소 사전 학습 기법인 CoSpaDi를 제안하는 데 목적이 있다.  
- Motivation: 기존의 저랭크 근사 방식은 단일 저차원 부분공간이라는 엄격한 구조적 제약으로 인해 모델 정확도 저하가 발생하는 한계를 가진다.  
- Contribution: CoSpaDi는 단일 기저 대신 희소 조합으로 각 가중치 열을 표현하는 유연한 다중 부분공간 모델을 도입하고, 교정 데이터를 활용해 출력 활성화 복원을 최적화함으로써 낮은 손실률의 압축을 달성한다.  

## Method  
CoSpaDi는 저랭크 근사를 대체하는 희소 사전 학습 방식으로, 각 가중치 행렬을 밀집 사전 행렬과 각 열마다 희소한 계수 행렬의 곱으로 근사한다. 교정 데이터로부터 입력 활성화의 저차원 공간을 추정하고 이를 활용해 사전과 계수를 학습하여 기능적 재구성 오차를 최소화한다. 또한, 여러 층에 공통 사전을 공유하는 교차 층 압축과 계수 행렬 양자화 기법을 적용하여 효율성을 극대화한다.  

## Results  
다양한 LLaMA 및 Qwen 계열 모델에 대해 20~50% 압축 비율에서 기존 최첨단 저랭크 및 프루닝 기법 대비 정확도와 퍼플렉서티 모두에서 일관되게 우수한 성능을 보였다.  

## Limitations  
주요 한계로는 K-SVD 알고리즘의 OMP 기반 희소 코딩과 순차적 원자 업데이트 과정의 느린 계산 속도가 있으며, 이를 개선하기 위한 보다 효율적인 기법 연구가 필요하다.  

## Conclusion  
CoSpaDi는 기존 저랭크 접근법을 뛰어넘는 희소 사전 학습 기반 데이터 인식 압축 기법으로서 대형 언어 모델의 후처리 압축에서 실질적인 성능 향상을 달성함을 확인하였다.

# 5. [LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale   Diffusion Transformer](https://arxiv.org/abs/2509.22414)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22414)

## Introduction
- Goal: 본 논문은 캡션 없이 대규모 확산(transformer) 모델을 활용하여 다양한 저화질 이미지의 보편적 복원을 달성하는 프레임워크 LucidFlux를 제안하는 것을 목표로 한다.  
- Motivation: 기존의 CNN 및 UNet 기반 확산 모델들은 복잡한 저하 환경에서 과잉 평활화, 인위적 구조 생성 및 의미 왜곡 문제를 겪으며, 텍스트 캡션 의존 방식은 지연과 의미 불일치 문제를 초래한다.  
- Contribution: LucidFlux는 경량의 이중 분기 조절기와 시점·계층 적응형 모듈을 통합하고, 캡션 없는 시맨틱 정합성 유지 기법 및 대규모 고품질 데이터 큐레이션 파이프라인을 통해 대규모 Flux.1 확산 트랜스포머를 효과적으로 적용한다.  

## Method  
LucidFlux는 원본 저화질 입력과 경량 복원된 프록시 이미지를 각각 처리하는 이중 분기 조절기를 갖추어 기하학적 구조 고정과 잡음 억제를 분리한다.  
이러한 조절기 출력을 시점 및 계층 정보에 따라 가변적으로 조절하여 백본 모델 내에서 조화롭고 단계적인 복원 업데이트를 수행한다.  
또한 시맨틱 신호를 경량 복원 프록시에서 추출한 SigLIP 특징으로 대체하여, 텍스트 캡션 없이 의미 일관성을 유지하는 방식을 채택한다.  

## Results  
LucidFlux는 여러 합성 및 실세계 벤치마크에서 최첨단 확산 기반 공개 및 상용 모델들을 능가하며, 주관적 품질과 의미적 정합성 측면에서 최고 성능을 기록하였다.  

## Limitations  
본 연구는 모델 크기와 계산 자원 요구량이 높으며, 시점 및 계층 적응 모듈 설계가 복잡하여 구현 부담이 존재한다는 한계가 있다.  

## Conclusion  
LucidFlux는 캡션 없이 언제, 어디서, 무엇을 조건화할지에 대한 체계적 설계로 대규모 확산 트랜스포머 기반 보편적 이미지 복원을 실현하며, 고품질 데이터와 시맨틱 정합성 유지를 통한 복원 성능 향상을 입증하였다.

# 6. [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in   Subject-Driven Generation](https://arxiv.org/abs/2509.21989)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21989)

## Introduction
- 본 논문은 사전학습된 확산모델의 백본에서 시각적 및 의미적 특징을 분리하여, 주제 주도 이미지 생성에서 시각적 불일치를 검출하는 방법을 제안한다.  
- 주제의 시각적 일관성을 판단하기 위한 기존 평가 지표가 부족하고, 특히 자세 및 환경 변화에도 불구하고 세밀한 시각적 불일치를 정량화 및 위치화하는 문제의 어려움에 착안하였다.  
- 자동화된 데이터셋 생성 파이프라인과 대조학습 기반 특징 분리 아키텍처를 통해 시각적-의미적 대응을 가능케 하고, 이를 활용한 새로운 시각-의미 매칭(VSM) 평가 지표를 도입하였다.  

## Method  
- Subjects200k 데이터셋을 활용해 의미 및 시각 대응 관계가 주석 처리된 이미지 쌍을 자동 생성하는 파이프라인을 설계하였다.  
- 분리된 의미 및 시각 특징을 추출하기 위해 두 개의 별도 집계 네트워크를 이용한 대조학습 구조를 개발하였으며, 시각적으로 불일치하는 영역과 일치하는 영역 간 특징 차이를 명확히 학습시켰다.  
- 학습된 특징을 활용해 의미적으로 대응하는 지점들 중 시각적으로 일치하는 비율을 측정하는 VSM 지표를 정의하여 시각적 불일치를 정량화하고 공간적으로 위치화하였다.  

## Results  
- VSM 지표는 CLIP, DINO, 챗GPT-4o 등 기존 전역 특징 기반 및 비전-언어 모델 평가 지표 대비 통계적 상관관계와 불일치 위치화 측면 모두에서 우수한 성능을 보였다.  

## Limitations  
- 시각 특징과 의미 특징 간 완전한 분리는 이루어지지 않았으며, 데이터셋 품질 및 낮은 공간 해상도 등으로 인해 세밀한 불일치 탐지가 제한적이다.  

## Conclusion  
- 본 연구는 확산 모델 백본 특성 분리를 통해 주제 주도 이미지 생성에서 시각적 일관성을 평가 및 위치화할 수 있는 최초의 실험적 틀과 지표를 제시하였다.

# 7. [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22072)

## Introduction
- Goal: 본 논문은 대형 언어 모델(LLM) 편집에서 널리 약점으로 여겨졌던 파인튜닝 기법의 문제점을 규명하고, 이를 개선하여 효과적인 모델 편집 방법을 제안하는 것이다.  
- Motivation: 기존 연구들이 파인튜닝의 실패를 기법의 근본적 한계 때문이라고 보았으나, 실제로는 편집 작업에 부적절한 훈련 파이프라인 설계 때문임을 밝히고자 한다.  
- Contribution: 깊이 우선(Depth-First) 파이프라인의 문제점을 지적하고, 폭 넓은(Breadth-First) 파이프라인과 미니배치 최적화를 도입한 LocFT-BF 방법을 제안하여 기존 최첨단 대비 월등한 성능과 대규모 편집 및 대형 모델 확장성을 입증하였다.  

## Method  
먼저, 기존 파인튜닝 기반 편집이 적용한 깊이 우선 방식 대신 표준 폭 넓은 파이프라인을 채택하여 편집 간 상호 간섭과 망각 문제를 완화하였다.  
다음으로, 샘플 별 단일 업데이트를 미니배치 업데이트로 개선하여 모델 능력 저하를 줄였다.  
마지막으로, LLM 내에서 편집 효과성이 높은 층과 모듈(주로 후반부 MLP 하강 투영층)만 선택하여 국부적 파인튜닝을 수행하는 체계적인 위치 선정법을 제안하였다.  

## Results  
제안된 LocFT-BF는 다양한 LLM과 편집 데이터셋에서 안정적인 신뢰도, 우수한 일반화 성능, 높은 모델 능력 보존률을 달성하였으며, 특히 10만 건 편집 및 최대 720억 매개변수 모델에서도 기존 방법 대비 뛰어난 확장성을 입증하였다.  

## Limitations  
일부 편집 데이터셋에서 일반화 성능이 데이터 특성에 영향을 받으며, 최적의 튜닝 위치 탐색은 여전히 비용이 많이 드는 작업이다.  

## Conclusion  
본 연구는 기존의 잘못된 파인튜닝 적용 방식을 바로잡아 파인튜닝 기반 모델 편집의 가능성을 재조명하고, LocFT-BF를 통해 효과적이고 확장 가능한 편집 기법으로 자리매김하였으며 향후 연구에 견고한 기초를 제공한다.

# 8. [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21574)

## Introduction
- Goal: 본 연구는 단일 초상화 이미지로부터 텍스트, 음성, 비디오를 통합하여 무한한 대화형 멀티모달 인간 에이전트를 실시간으로 생성하는 X-Streamer 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 기존 모듈식 시스템들은 멀티모달 상호작용에서 지연, 맥락 유실, 정합성 문제를 겪으며, 특히 장시간 대화에서 정체성 유지와 일관성 확보가 어렵다는 한계가 존재하였다.  
- Contribution: Thinker–Actor 이중 트랜스포머 구조와 청크 단위 확산 강제(chuck-wise diffusion forcing) 및 글로벌 정체성 참조를 결합하여 실시간, 장시간 대화를 유지하면서 음성-입술 싱크와 시각적 일관성을 보장하는 통합 인간 세계 모델링 방식을 제안하였다.

## Method  
X-Streamer는 사용자 입력(텍스트 및 음성)을 이해하고 추론하는 Thinker 모듈과 이를 토대로 텍스트, 음성, 비디오를 시간 정렬하여 생성하는 Actor 모듈로 구성된다.  
비디오 생성은 고도로 압축된 VAE 잠재공간에서 청크 단위로 확산 모델을 사용하여 자동회귀적으로 이루어지며, 시-청각 간 타이밍 정렬과 문맥 유지가 교차주의(cross-attention)와 3D 다중모달 위치 임베딩으로 구현된다.  
실시간 성능을 위해 두 개의 A100 GPU에 모듈들을 분산 배치하고, 키-값 캐시와 피라미드형 노이즈 스케줄러를 도입하여 연산 비용을 최적화하였다.

## Results  
X-Streamer는 실제 평가에서 기존 최첨단 멀티모달 인물 애니메이션 및 영상 확산 기법 대비 시각적 선명도, 정체성 유지, 음성-입술 싱크 및 자연스러운 움직임 측면에서 우수한 성능을 보이며, 장시간(수십 분 이상) 일관된 실시간 멀티턴 대화를 안정적으로 수행하였다.

## Limitations  
본 모델은 오직 실제 사람의 토킹헤드 영상만으로 학습되어 광범위한 시나리오에 대한 일반화가 제한적이다.

## Conclusion  
X-Streamer는 텍스트, 음성, 비디오를 하나의 통합 아키텍처에서 실시간으로 상호작용하며 생성하는 최초의 인간 세계 모델링 프레임워크로서, 지속적이고 지능적인 디지털 휴먼 개발에 크게 기여하였다.

# 9. [Scale-Wise VAR is Secretly Discrete Diffusion](https://arxiv.org/abs/2509.22636)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22636)

## Introduction
- 본 연구의 목표는 Visual Autoregressive Generation(VAR) 모델이 Markovian attention mask를 적용할 경우 이산 확산(discrete diffusion) 모델과 수학적으로 동등함을 밝히는 것이다.  
- 기존 VAR 모델은 높은 효율성을 보이나, 초기 생성된 픽셀이나 영역이 전체 이미지의 분포나 의미론 정보를 충분히 반영하지 못하는 한계가 존재한다.  
- 본 논문은 Markovian VAR 변형을 이산 확산 모델 관점에서 새롭게 해석하는 Scalable Visual Refinement with Discrete Diffusion(SRDD)를 제안하여, VAR의 성능과 효율성을 이론적으로 설명하고 실험적으로 향상시켰다.  

## Method  
Markovian attention mask를 적용한 VAR 변형은 이전 스케일만 조건으로 하여 다음 스케일을 예측하며, 이는 이산 확산 모델에서의 상태 전이와 복원 과정과 동일한 확률적 모델링이 된다.  
이를 기반으로 SRDD는 이산 확산 기법의 반복적 정제(iterative refinement) 및 샘플링 전략(예: classifier-free guidance, token resampling, distillation)을 도입하여 VAR의 효율성과 성능을 개선한다.  
이와 함께, 모델의 입력에 스케일에 따른 신호 대 잡음비(SNR)를 내포하는 점과 교차 엔트로피 손실 함수를 활용하는 점이 확산 모델의 특성과 부합함을 이론적으로 입증하였다.  

## Results  
SRDD는 MiniImageNet, FFHQ, SUN 및 AFHQ 데이터셋에서 VAR 대비 최대 20.2% FID 개선과 31.1% IS 향상을 기록하며, 기존 최첨단 생성 모델 대비 우수한 생성 품질과 샘플링 효율성을 보였다.  

## Limitations  
본 연구는 Markovian VAR 변형에 기반하기 때문에 다중 스케일에 대한 종속성 감소로 인한 정보 손실 가능성과 이에 따른 일부 복잡한 이미지 구조 반영 한계가 존재한다.  

## Conclusion  
본 연구는 VAR 모델이 구조화된 이산 확산 과정과 수학적으로 등가임을 규명하고, 이 관점에서 확산 기법을 결합하여 생성 품질과 효율성을 크게 향상시킨 새로운 시각적 생성 패러다임 SRDD를 제시하였다.

# 10. [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion   Models](https://arxiv.org/abs/2509.22300)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22300)

## Introduction
- Goal: 본 논문은 확산 모델의 샘플링 과정에서 예측 이력을 활용하여 생성 이미지의 품질과 효율성을 향상시키는 새로운 비학습 기반 모멘텀 기법인 History-Guided Sampling(HiGS)을 제안하는 것을 목표로 한다.  
- Motivation: 확산 모델은 적은 신경망 평가 횟수(NFE)나 낮은 가이던스 스케일을 사용할 때 출력 이미지가 흐릿하거나 세부 표현이 부족한 문제점이 존재한다.  
- Contribution: HiGS는 과거 예측값들의 가중 평균과 현재 예측값의 차이를 반영하여 보다 사실적이고 세밀한 이미지를 생성하며, 추가 훈련 없이 기존 확산 샘플러에 쉽게 통합 가능함을 보인다.  

## Method  
HiGS는 샘플링 각 단계에서 이전 예측 이력을 지수 가중 이동평균으로 통합하여 현재 예측과의 차이를 가이드로 사용한다. 이 때 가중치는 시간에 따라 조절되며, 색상 과포화를 줄이기 위해 직교 사영과 주파수 영역 고주파 필터링 기법을 적용한다. 이 방식은 기존 샘플링에 추가 계산 부담 없이 적용 가능하다.  

## Results  
HiGS는 Stable Diffusion 및 ImageNet 조건부 생성 등 다양한 모델 및 설정에서 표준 샘플링 대비 적은 샘플링 단계와 낮은 가이드 스케일에서 이미지 품질 및 지표(FID, HPSv2 등)를 일관되게 개선하였으며, 특히 SiT 기반 모델에서는 30단계 샘플링 만으로 unguided ImageNet 생성에서 1.61의 신규 최첨단 FID를 달성하였다.  

## Limitations  
HiGS는 기반 확산 모델의 편향과 한계를 일정 정도 계승하며, 이로 인한 문제 완화는 후속 연구 과제로 남아 있다.  

## Conclusion  
본 연구는 예측 이력 정보를 활용한 HiGS 방법을 통해 훈련 없이 확산 모델 샘플링의 품질과 효율을 현저히 향상시키는 플러그-앤-플레이 기법을 제안하였다.

# 11. [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22630)

## Introduction
- Goal: 본 논문은 사전 학습된 순환 신경망(RNN)의 상태 크기를 사후 학습(post-training)을 통해 효율적으로 확장하여 기억(recall) 능력을 향상시키는 StateX 방법을 제안하는 데 목적이 있다.  
- Motivation: Transformer 기반 모델은 긴 문맥 처리 시 높은 연산 비용이 발생하는 반면, RNN은 일정한 토큰당 연산복잡도로 효율적이나 기억 능력이 제한적이기 때문에 상태 크기 확장을 통한 성능 개선이 필요하였다.  
- Contribution: StateX는 선형 어텐션과 상태 공간 모델의 구조적 변형을 설계하여 추가 파라미터 증가를 최소화하면서도 사전 학습된 RNN 상태를 확장하고, 이를 통해 기억력과 문맥 학습 능력을 개선하는 최초의 사후 학습 기반 상태 확장 방식을 제시한다.  

## Method  
StateX는 GLA와 Mamba2 두 가지 대표 RNN 구조에 맞춰 각각 멀티헤드를 단일 대형 헤드로 합치거나, 상태 차원 확장을 위해 키와 쿼리 프로젝션 층을 확장하는 구조적 수정을 적용한다. 파라미터 초기화는 토큰 혼합과 관련된 부분만 재초기화하여 성능 저하를 방지하며, 전체 계층 중 일부 계층만을 선택적으로 확장하여 학습 효율과 적응성을 균형 있게 유지한다. 이러한 구조 변경은 사후 학습 단계에서 수행되어 재학습 비용을 최소화한다.  

## Results  
1.3억 파라미터 규모의 GLA와 Mamba2 공개 체크포인트에 StateX를 적용한 결과, 기존 방식 대비 기억 집중 태스크, 문맥 내 학습 및 장문 검색(최대 64K 토큰)에서 유의미한 성능 향상을 보였다.  

## Limitations  
기존 파라미터를 상속하는 초기화 방법이 성능 저하를 유발하여 추가적인 재초기화 및 세밀한 확장 계층 선택이 필요하다는 제약이 존재한다.  

## Conclusion  
StateX는 사전 학습된 RNN의 상태 크기 확장을 사후 학습으로 효율적으로 수행하여 긴 문맥 처리 시 기억 능력 향상에 기여하며, RNN이 Transformer 대비 효율성을 유지하면서 장문 처리 성능을 개선하는 중요한 진전으로 평가된다.

# 12. [CHURRO: Making History Readable with an Open-Weight Large   Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.19768)

## Introduction
- Goal: 본 논문은 다양한 언어와 스크립트, 복잡한 레이아웃 및 물리적 열화가 나타나는 역사적 문서의 고정확도 저비용 텍스트 인식을 위한 오픈 웨이트 대형 비전-언어 모델 CHURRO를 제안하는 것이다.  
- Motivation: 기존 비전-언어 모델은 현대의 표준화된 텍스트에 최적화되어 있어, 역사적 자료의 언어적 다양성과 문서 특성을 효과적으로 처리하지 못한다는 한계가 존재한다.  
- Contribution: CHURRO는 22세기에 걸친 46개 언어군의 99,491페이지로 구성된 최대 규모의 역사적 텍스트 인식 데이터셋 CHURRO-DS로 학습되었으며, 기존 모델보다 우수한 정확도와 비용 효율성을 보인다.  

## Method  
CHURRO는 3억 매개변수의 Qwen 2.5 VL 모델을 선택하여 CHURRO-DS 데이터셋으로 특화 미세조정했다. 데이터셋은 서로 다른 포맷을 갖는 155개의 역사적 코퍼스를 통합하고, 적절한 읽기 순서와 정확성을 보장하기 위해 자동화 및 수동 검증을 병행하여 선별, 정제하였다. 평가를 위해 다양한 오픈 및 폐쇄형 VLM과 OCR 시스템과 비교 실험을 수행하였다.  

## Results  
CHURRO는 CHURRO-DS 테스트셋에서 인쇄문서 82.3%, 필사문서 70.1%의 정규화된 Levenshtein 유사도를 기록하며, 두 번째로 높은 모델 Gemini 2.5 Pro 대비 각각 1.4%, 6.5% 우수한 성능을 보였고, 비용 효율성은 15.5배 뛰어났다.  

## Limitations  
CHURRO-DS 데이터셋은 다수의 저자원 언어와 아프리카 대륙 원주민 언어를 포함하지 못해 일부 언어군이 부족하게 대표된다.  

## Conclusion  
역사 텍스트 인식에 특화된 CHURRO 모델과 CHURRO-DS 데이터셋은 역사 문서 디지털화 및 학술 연구 가속화를 위한 효과적인 자원임이 입증되었다.

# 13. [Instruction-Following Evaluation in Function Calling for Large Language   Models](https://arxiv.org/abs/2509.18420)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.18420)

## Introduction  
- Goal: 본 논문은 대형 언어 모델의 함수 호출 시 파라미터 형식 명령 준수 능력을 평가하는 벤치마크 IFEval-FC를 제안하는 데 목적이 있다.  
- Motivation: 기존 벤치마크가 함수 인자의 기능적 정확성만을 평가할 뿐 파라미터 설명 내 형식 지침 준수 여부를 평가하지 않아 실제 에이전트 시스템에서 발생하는 포맷 오류 문제를 해결할 필요가 있다.  
- Contribution: IFEval-FC는 JSON 스키마 내 명확히 검증 가능한 형식 지침을 포함한 750개의 테스트 케이스와 사용자 쿼리를 제공하며, 완전 알고리즘적 평가 방식을 도입하여 객관적이고 재현 가능한 평가를 가능하게 하였다.  

## Method  
IFEval-FC는 JSON 형식 파라미터 설명에 포함된 19종의 다양한 형식 지침을 기반으로 대형 언어 모델의 명령 준수 능력을 평가한다. 함수 스키마는 실제 및 GPT-5를 통한 합성 데이터를 혼합하여 생성되었으며, 각 함수에 대해 다섯 개의 자연어 사용자 쿼리를 제공한다. 평가 과정에서 모든 모델은 반드시 함수를 호출하도록 설계되어 형식 준수 여부를 정량적으로 산출한다.  

## Results  
최신 대형 언어 모델인 GPT-5와 Claude Opus 4.1조차도 기본적인 형식 규칙 준수 정확도가 80%를 넘지 못하여 함수 호출 시 명령 준수가 여전히 어려운 문제임을 확인하였다.  

## Limitations  
본 벤치마크는 영어권 함수 호출에 집중하여 다국어 지원과 복수 함수 선택 상황 등 더 복잡한 시나리오는 포함하지 않았다.  

## Conclusion  
본 연구는 함수 호출 업무에서 대형 언어 모델의 세밀한 명령 준수 능력을 평가하는 새로운 방향을 제시하며, 향후 다국어 확장 및 난이도 고도화 연구가 필요함을 강조한다.
