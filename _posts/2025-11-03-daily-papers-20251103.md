---
layout: post
title: "Daily Papers — 2025-11-03"
date: 2025-11-03 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization   Formats](https://arxiv.org/abs/2510.25602)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.25602)

## Introduction
- Goal: 본 연구는 정밀도와 하드웨어 효율성 측면에서 정수(INT)와 부동소수점(FP) 저비트 미세 양자화 기법을 통합적으로 비교하는 데 목적이 있다.  
- Motivation: 대규모 언어 모델(LLM)에서 활성화 이상치 현상 대응을 위해 AI 하드웨어가 저정밀도 FP 포맷으로 전환하는 추세임에도 불구하고, 다양한 양자화 세분화 수준에서 INT와 FP 간의 체계적 비교가 부족하였다.  
- Contribution: MXINT8이 MXFP8보다 알고리즘 정확도와 하드웨어 효율성에서 우수하며, 4비트 포맷에서는 FP가 일반적으로 우위를 점하나 이상치 완화 기법 적용 시 NVINT4가 NVFP4를 능가함을 밝히고, 대칭 클리핑을 통한 INT 저비트 학습의 그래디언트 편향 해소 기법을 제안하였다.  

## Method  
양자화 신호대잡음비(QSNR)를 이론적으로 모델링하여 INT와 FP 저비트 블록 단위 양자화의 성능 교차점을 규명하였다.  
MX, NV 포맷의 FP와 대응하는 INT 변형 포맷을 도입하여 다양한 블록 크기와 비트 폭 조건에서 정량적 비교를 수행하였다.  
랜덤 하다마드 회전 기법을 적용하여 데이터 분포의 크레스트 팩터를 완화하고 양자화 정확도에 미치는 영향을 평가하였다.  

## Results  
MXINT8은 8비트 미세 양자화에서 MXFP8 대비 전이 모델의 직접 캐스트 추론 및 저비트 학습 모두에서 우수한 QSNR과 정확도를 보였으며, 하드웨어 분석 결과 INT 포맷의 에너지 및 면적 효율성이 FP 대비 20~40% 이상 향상됨이 증명되었다.  

## Limitations  
4비트 이하 저비트 양자화에서는 FP 포맷이 대체로 우위를 보이나, 해당 수준에서 INT가 FP보다 항상 우수하지는 않음을 확인하였다.  

## Conclusion  
본 연구는 미세 양자화 수준에서 INT와 FP 간의 정밀한 성능 교차 현상을 밝히고, 미래 AI 가속기 설계에서 정확도와 효율성의 최적 균형을 위해 미세 양자화 INT 포맷, 특히 MXINT8의 중요성을 강조하며 FP 중심 하드웨어 방향성에 도전하였다.

# 2. [Phased DMD: Few-step Distribution Matching Distillation via Score   Matching within Subintervals](https://arxiv.org/abs/2510.27684)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.27684)

## Introduction
- Goal: 본 논문은 복잡한 생성 과제를 효율적으로 수행하는 다단계(score-based) 분포 매칭 지식 증류 기법인 Phased DMD를 제안하는 것이다.  
- Motivation: 기존의 한 단계(DMD) 지식 증류는 제한된 모델 용량으로 인해 복잡한 텍스트-비디오 생성 등 고난이도 과제에서 성능이 저하되고, 다단계 확장은 메모리와 계산량 증가로 안정성과 효율성이 악화된다.  
- Contribution: 본 연구는 신호대잡음비(SNR) 구간을 하위 구간으로 나누어 단계별로 점진적 분포 매칭 및 점수 매칭을 수행하는 MoE 기반 다단계 증류 프레임워크 Phased DMD를 제안하여, 기존 방법 대비 다양성과 생성 능력을 향상시켰다.  

## Method  
Phased DMD는 SNR 범위를 여러 하위 구간으로 나누어 각 구간 내에서 점수 매칭을 통해 독립적으로 학습하며, 이를 통해 복잡한 데이터 분포를 점진적으로 학습한다. 각 단계별 생성자는 해당 하위 구간에 특화된 전문가 역할을 수행하여 MoE 구조로 모델 용량을 효과적으로 확장한다. 또한, 시간 구간 내의 점수 매칭 목적함수를 엄밀하게 유도하여 중간 단계에서도 안정적인 학습을 가능하게 하였다.  

## Results  
제안 방법은 대규모 이미지 및 비디오 생성 모델(Qwen-Image, Wan2.2 등)에서 기존 DMD 및 SGTS 기반 증류 대비 생성 다양성을 더 잘 보존하면서 핵심 생성 능력인 텍스트 정확성 및 동적 장면 재현을 유지함을 실험적으로 입증하였다.  

## Limitations  
기반 모델 자체의 출력 다양성이 매우 제한적인 경우, Phased DMD에 의한 다양성 개선 효과가 미미하게 나타났다.  

## Conclusion  
Phased DMD는 점진적 단계별 학습과 MoE를 결합한 데이터-프리 다단계 지식 증류 방법으로, 복잡한 생성 모델의 효율적 가속과 성능 향상을 달성하였다.

# 3. [Revisiting Multimodal Positional Encoding in Vision-Language Models](https://arxiv.org/abs/2510.23095)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23095)

## Introduction
- Goal: 본 연구의 목적은 비전-언어 모델에서 다중모달 위치 인코딩 방식인 Rotary Positional Embedding(RoPE)의 설계 원리를 체계적으로 분석하고 개선하는 것이다.  
- Motivation: 기존 다중모달 위치 인코딩 방법들은 시각적 3차원 구조 보존 실패, 주파수 할당 불균형, 그리고 텍스트 RoPE와의 비호환성 문제로 인해 모델 성능이 저하되는 한계를 가진다.  
- Contribution: 본 논문에서는 위치 설계, 주파수 할당, 텍스트 RoPE 호환성의 세 가지 설계 지침을 도출하고, 이를 기반으로 구조 변경 없이 적용 가능한 Multi-Head RoPE(MHRoPE)와 MRoPE-Interleave(MRoPE-I) 두 가지 간단한 개선안을 제안하였다.  

## Method  
본 연구는 기존 MRoPE를 바탕으로 3D 시각 구조를 유지하고, 시각 위치 값 초기화를 통해 모달리티 혼동을 방지하는 spatial-reset 기법을 도입하였다.  
주파수 할당 분할의 단점을 극복하기 위해, MHRoPE는 서로 다른 어텐션 헤드에 축별 포지셔널 인코딩을 분배하고, MRoPE-I는 채널을 층별로 교차 배치하여 각 축이 전체 주파수 스펙트럼을 활용하도록 하였다.  
두 방법은 모두 텍스트 RoPE와 완전한 호환성을 유지하며, 사전 학습된 LLM 지식의 효과적인 전이 학습을 지원한다.  

## Results  
제안한 MHRoPE와 MRoPE-I는 이미지, 비디오, 시각적 그라운딩 등 20개 이상의 다양한 벤치마크에서 기존 방법군을 일관되게 능가하며, 특히 미세 공간 추론과 장기 시퀀스 이해에서 유의미한 성능 향상을 기록하였다.  

## Limitations  
본 연구는 제안한 위치 인코딩 설계가 전반적으로 우수함을 보였으나, 일부 특화된 비디오 및 문서 중심 작업에서는 추가적인 주파수 조정이나 위치 할당 개선이 필요하다는 점이 남아있다.  

## Conclusion  
본 연구는 비전-언어 모델의 다중모달 위치 인코딩에 대한 최초의 체계적 분석과 함께, 성능과 확장성을 모두 만족하는 두 가지 실용적인 RoPE 개선안을 제시하여 향후 연구 방향에 중요한 설계 가이드를 제공하였다.

# 4. [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.27258)

## Introduction
- Goal: 본 논문은 긴 문맥의 자기회귀 언어 모델에서 계산 비용 문제를 해결하기 위해 고차원 선형 주의 메커니즘인 Higher-order Linear Attention(HLA)를 제안하는 것이다.  
- Motivation: 기존의 선형 시간 주의 및 상태 공간 모델은 1차 또는 커널 기반 근사에 제한되어 표현력이 낮다는 문제점이 존재한다.  
- Contribution: HLA는 컴팩트한 접두사 요약통계를 통해 고차 상호작용을 선형 시간 내에서 엄격한 인과성을 유지하며 계산하고, 병렬 학습을 위한 집합적 스캔 방식을 제공한다.  

## Method  
HLA는 2차 이상의 차원에서 키-쿼리의 낮은 차수 모멘트를 이용해 정확한 인과적 스트리밍을 가능하게 하는 접두사 요약을 유지한다.  
엄격한 인과성 보장을 위해 두 개의 추가 요약을 도입하였으며, 청크별 병렬 학습을 위한 결합 가능한 연산자를 정의하여 직렬 순환과 정확히 일치하는 계산을 구현한다.  
2차 이상의 확장과 비대칭 변형(AHLA)을 포함하여 다양한 고차원 주의 연산을 제안하였으며, 각 토큰별 상태 업데이트 복잡도를 O(d² + d dv)로 제한한다.  

## Results  
제안된 HLA는 표준 선형 주의 모델 대비 엄격한 인과성을 갖춘 고차원 상호작용을 선형 시간 내에 효율적으로 계산하며, 직렬 반복과 동일한 결과를 내는 병렬 학습 방식을 성공적으로 개발하였다.  

## Limitations  
고차원 확장 시 계산 및 메모리 요구가 증가하며, 3차 이상 완전 병렬 학습 연산자의 구현은 후속 연구가 필요하다.  

## Conclusion  
HLA는 데이터 의존적 가중치를 유지하면서도 효율적인 선형-복잡도의 고차원 인과 주의 메커니즘으로, 긴 문맥 모델을 위한 확장 가능하고 원칙적인 새로운 구성 요소임을 입증하였다.

# 5. [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.26707)

## Introduction
- Goal: 본 연구는 대규모 언어 모델(LLM)의 사후학습(post-training) 과정 중에 가치 정렬(value alignment)이 어떻게 발생하는지 추적하고 분석하는 것을 목표로 한다.  
- Motivation: LLM이 사회에서 점점 중요한 역할을 하면서 인간 가치 체계와의 정렬 문제가 대두되지만, 기존 연구는 완성된 모델의 평가에 치중하여 가치 학습 과정의 역동성은 충분히 이해되지 못하였다.  
- Contribution: 본 연구는 SFT(감독학습 미세조정)와 선호 최적화(preference optimization)를 구분하여 가치 표류(value drifts)의 크기와 시점을 정량화하고, 다양한 알고리즘과 데이터셋이 모델의 가치 표현에 미치는 영향을 체계적으로 밝혀냈다.  

## Method  
모델의 가치는 가치 탐색형(value-probing) 프롬프트에 대한 반응에서 추출한 입장(찬성, 중립, 반대) 분포로 정량화하였다.  
Llama-3와 Qwen-3 계열 모델을 대상으로 각 사후학습 단계별 중간 체크포인트에서 응답을 수집하고 GPT-4o를 활용하여 입장을 분류하였다.  
이후 두 시점 간 가치 변화량인 드리프트 크기와 드리프트에 소요된 시간을 측정하여 가치 표류를 분석하였다.  

## Results  
SFT 단계에서는 모델 가치가 빠르게 정립되며, 이후 선호 최적화 과정은 일반적인 데이터셋 환경에서 가치 변화를 거의 일으키지 않으나, 인위적으로 조작된 가치 간극을 가진 합성 데이터셋에서는 알고리즘별로 가치 변동 양상이 크게 달라진다는 점을 확인하였다.  

## Limitations  
가치 정량화에 사용된 입장 기준은 복잡한 가치 내역을 단순화한 proxy이며, 문화적으로 제한된 주제와 데이터셋에 기반해 있어 일반화에 제약이 있을 수 있다.  

## Conclusion  
본 연구는 LLM 사후학습 과정 중 가치 학습의 동적 과정을 최초로 체계적으로 분석하여, 가치 정착 및 변화에 영향을 미치는 데이터 및 알고리즘 선택에 대한 실질적 인사이트를 제공하였다.

# 6. [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained   Classification](https://arxiv.org/abs/2510.24078)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.24078)

## Introduction
- Goal: 본 연구는 텍스트-이미지(T2I) 모델을 활용하여 세밀한 분류(fine-grained classification)를 위한 효과적인 합성 훈련 데이터를 생성하는 방법을 제안하는 데 목적이 있다.  
- Motivation: 기존 T2I 모델의 미세 조정 과정에서 과적합과 생성 샘플 다양성 저하 문제가 존재하며, 이는 합성 데이터 기반 분류 성능 향상을 제한한다.  
- Contribution: 클래스에 무관한 배경 및 자세 정보 추출 후 이를 조건으로 T2I 모델을 미세 조정하고, 합성 데이터 생성 시 이들 속성을 마지널라이즈하여 과적합을 완화하고 분류 성능을 향상시키는 BOB 전략을 제안하였다.  

## Method  
BOB은 세밀한 분류를 위해 이미지의 배경과 자세 속성을 캡셔닝 모델로 추출하여 텍스트 조건에 포함시켜 T2I 모델을 미세 조정하는 '컨텍스트 보존(Context Preservation)' 단계와, 합성 데이터 생성 시 다른 클래스의 배경과 자세를 무작위로 샘플링하여 클래스-컨텍스트 간의 불필요한 연관을 제거하는 '컨텍스트 마지널라이제이션(Context Marginalization)' 단계를 포함한다.  

## Results  
여러 데이터셋과 백본, T2I 모델에서 평가한 결과, BOB은 5장의 실제 이미지와 100장의 합성 이미지만으로 학습시  항공기 데이터셋에서 기존 최고 성능인 DataDream 대비 정확도가 7.4%p 향상되는 등 총 24개 실험 환경 중 18개에서 기존 방법을 능가하였다.  

## Limitations  
Pets 데이터셋 등 일부 환경에서는 BOB가 기존 방법과 비슷한 성능을 보였으며, 모든 상황에서 절대적 우위를 점하지는 못하였다.  

## Conclusion  
BOB은 세밀한 분류용 합성 데이터 생성을 위해 T2I 모델 미세 조정 시 클래스 비의존적 컨텍스트 정보를 활용하고 이를 데이터 생성 시 효과적으로 마지널라이즈함으로써 과적합 문제를 완화하고 분류 성능을 크게 향상시키는 효과적인 방법임이 입증되었다.
