---
layout: post
title: "Daily Papers — 2025-09-19"
date: 2025-09-19 08:15:00
tags: [papers, hugginface]
categories: []
---

# 1. [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/abs/2509.15185)

## Introduction

- Goal: 본 연구는 자기회귀적 이미지 생성 모델의 고수준 시각 의미 학습을 향상시키고자 하였다.
- Motivation: 기존 자기회귀 모델은 국소적 의존성, 단계 간 의미 불일치, 공간 불변성 결여 등의 문제로 인해 이미지 이해 능력에 한계가 존재하였다.
- Contribution: 시각적 자기지도 학습 기법을 통합한 새로운 학습 프레임워크 ST-AR(Self-guided Training for AutoRegressive models)을 제안하였다.

## Method

ST-AR은 주의(attention) 맵에 무작위 마스킹을 적용하여 국소적 의존성을 완화하고, 대조 학습으로 단계별 및 뷰 간 의미 일관성을 유지한다. 또한, EMA 기반의 비학습 교사 모델을 활용하여 자기회귀 모델을 자기지도 방식으로 효과적으로 지도한다. 이를 통해 전통적 토큰 예측 손실과 함께 마스킹 이미지 모델링 및 대조 학습 손실을 통합하였다.

## Results

ST-AR을 적용한 LlamaGen-XL 모델은 ImageNet-256×256 조건부 생성에서 50 에폭 만에 FID 점수를 49% 개선하였으며, 기존 보다 적은 파라미터 규모에서 경쟁력 있는 성능을 보였다.

## Limitations

본 연구의 한계는 자기지도 학습 도입으로 인해 훈련 비용이 증가한다는 점이다.

## Conclusion

자기회귀 이미지 생성 모델의 시각적 이해도를 향상시키기 위한 ST-AR 학습법은 이미지 생성 품질을 효과적으로 개선하는 것으로 검증되었다.

# 2. [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)

## Introduction

- Goal: 본 논문은 이미지, 비디오, 3D 자산을 아우르는 최초의 통합된 시각 토크나이저인 ATOKEN을 제안하는 것을 목표로 한다.
- Motivation: 기존 시각 토크나이저들은 재구성 또는 의미 이해 중 한 가지 목표와 단일 모달리티에서만 전문화되어 있어 범용적인 시각 표현 학습에 한계가 있었다.
- Contribution: ATOKEN은 4차원 공간에서 모든 시각 입력을 공통적으로 인코딩하며 적대적 학습 없이 안정적인 학습이 가능하고, 다양한 시각 모달리티와 태스크를 하나의 프레임워크로 통합하여 최첨단 성능을 달성하였다.

## Method

ATOKEN은 이미지, 비디오, 3D 자산을 각각 4차원 좌표계의 희소 표현으로 통합하고, 4D rotary position embedding을 활용하는 순수 트랜스포머 구조를 사용한다. 재구성 및 의미 이해 두 가지 태스크를 위한 라티언트 투영과 태스크별 디코더를 통해 높은 표현력을 갖춘 멀티모달 토크나이저를 구현하였다. 학습은 사전 훈련된 이미지 이해 모델 기반 위에 이미지 재구성부터 비디오, 3D로 점진적 커리큘럼을 적용하고, 적대적 손실 대신 Gram 행렬 손실과 지각 손실을 결합하여 안정성을 확보하였다.

## Results

ATOKEN은 이미지에서 0.21 rFID와 82.2% ImageNet 정확도를, 비디오에서 3.01 rFVD와 40.2% MSRVTT 영상 텍스트 검색 정확도를, 3D에서 28.28 PSNR과 90.9% 분류 정확도를 달성하며, 모든 모달리티와 태스크를 아우르는 최초의 통합 시각 토크나이저로서 최고 또는 경쟁력 있는 성능을 기록하였다.

## Limitations

정보 부족

## Conclusion

ATOKEN은 다양한 시각 모달리티와 태스크를 하나의 통합된 토크나이저 구조로 효율적이고 안정적으로 처리함으로써 차세대 다중모달 AI 시스템 구축에 중요한 기반을 제공한다.

# 3. [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/abs/2509.14638)

## Introduction

- 본 연구의 목표는 다양한 복잡한 편집 작업에 대응 가능한 지시어 기반 이미지 편집(instruction-based image editing, IBIE) 성능 향상을 위한 대규모 데이터셋과 학습 방식을 제안하는 것이다.
- 기존 IBIE 방법들은 제한된 편집 유형과 소규모 데이터셋으로 인해 복잡한 편집 작업의 수행이 어렵고, 전통적인 데이터셋은 이미지와 캡션 간의 잡음 문제로 인해 모델 성능에 제약이 있었다.
- 본 논문에서는 10만 건 이상의 고품질 이미지 편집 샘플을 포함하는 MultiEdit 데이터셋과 이를 활용한 멀티태스크 학습 전략을 제시한다.

## Method

- MultiEdit는 6개의 도전적인 편집 작업과 56개의 세분화된 편집 유형(비스타일 변환 18종, 스타일 변환 38종)을 포함하며, 원본 이미지, 편집 지시어, 편집 결과 이미지의 삼중 구성을 갖춘다.
- 데이터셋 구축은 최신 멀티모달 대형언어모델(MLLM) 기반 파이프라인을 활용하여 캡션 의존 없이 원본 이미지에서 직접 시각적 적응형 편집 지시어를 생성하고, SOTA 이미지 생성 모델(GPT-Image-1)을 통해 고품질 편집 이미지를 생산한다.
- 편집 지시어와 편집 이미지의 품질 확보를 위해 규칙 기반 필터링과 다단계 검수 과정을 거쳐 데이터 정제 및 평가를 수행한다.

## Results

- 공개된 다수의 IBIE 모델에 MultiEdit-Train 데이터셋으로 파인튜닝을 수행한 결과, 복잡한 편집 작업에 대한 정확도 및 정합도가 크게 향상되어 기존 최고 성능 모델과 경쟁하거나 이를 능가하는 성과를 보였다.

## Limitations

- 본 데이터셋과 모델 개선에도 불구하고 얼굴과 세밀한 디테일 편집 등 일부 세부 영역에서는 여전히 개선 여지가 남아 있다.

## Conclusion

- MultiEdit 데이터셋과 제안된 학습 방법은 보다 다양하고 도전적인 지시어 기반 이미지 편집 연구 발전에 중요한 기반 자원으로 활용될 수 있다.

# 4. [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)

## Introduction

- Goal: Apertus는 데이터 준수와 다국어 표현 문제를 해결한 완전 개방형 대형 언어 모델(LLM)을 개발하는 것이다.
- Motivation: 기존 공개 모델들이 데이터 합법성 문제와 제한된 다국어 지원으로 인해 글로벌 사용자 요구를 충족하지 못하는 한계가 존재하였다.
- Contribution: 1811개 언어를 포함한 15조 토큰 규모의 개방 데이터로 사전학습하고, 금지된 데이터 제거 및 기억 억제 기법을 적용하여 8B 및 70B 크기의 최첨단 성능의 모델을 공개하였다.

## Method

Apertus는 xIELU 활성화 함수, AdEMAMix 최적화기, QK-Norm 등 아키텍처 혁신과 Goldfish loss를 활용하여 데이터 반복 출현 억제를 동시에 달성하였다.  
학습 데이터는 robots.txt에 따른 사후 옵트아웃 처리와 독성 및 개인정보 필터링으로 엄격히 준수되었으며, 사전학습은 4096 토큰 길이, 장맥락 확장을 위해 최대 65,536 토큰까지 확장 가능하게 설계되었다.  
다국어 모델링 향상을 위해 약 40%의 데이터가 비영어권 언어로 구성되었고, 토크나이저는 다언어 공평성을 고려해 선정되었다.

## Results

Apertus 모델은 동급 공개 모델 중에서 다국어 벤치마크에서 최고 수준의 성능을 달성하며, 동일 규모의 공개 가중치 모델들을 뛰어넘거나 경쟁하였다.

## Limitations

Apertus는 텍스트 전용 모델로 Hallucination, 독성 생성 및 안전성 문제 등 현실적 사용 전에 추가 검증과 조율이 요구된다.

## Conclusion

Apertus는 투명한 아키텍처와 데이터 정책을 바탕으로 글로벌 사용자에게 적합한 신뢰성 있고 데이터 준수적인 최첨단 완전 공개 LLM을 구현하였다.

# 5. [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)

## Introduction

- Goal: 다중 회차 지시 기반 이미지 편집의 자동화되고 확장 가능하며 세밀한 평가를 위한 객체 중심 프레임워크인 EdiVal-Agent를 제안하는 것이다.
- Motivation: 기존 평가 방법이 참조 이미지 의존과 VLM 단독 사용의 한계로 인해 신뢰성과 해석력이 떨어지는 문제를 해결하고자 한다.
- Contribution: 객체 분해, 지시 생성, 다양한 평가 도구 통합을 통한 완전 자동화 평가 파이프라인과 이를 활용한 다중 회차 편집 벤치마크 EdiVal-Bench를 구축하였다.

## Method

EdiVal-Agent는 이미지에서 의미 있는 객체를 분해하고, 이를 토대로 9가지 편집 유형에 해당하는 다중 회차 지시어를 생성한다. 편집 결과는 개체 인식기와 VLM을 결합해 지시 이행도를 평가하고, DINOv3 기반 의미적 일관성 및 픽셀 수준 일관성, 사람 선호 모델로 시각적 품질을 정량화한다. 이 파이프라인은 모듈화되어 추후 도구의 통합과 정확도 향상이 가능하다.

## Results

EdiVal-Agent는 인간 평가자와 81.3% 일치율을 보이며, 기존 VLM 단독 및 CLIP 기반 평가보다 높은 신뢰도와 해석력을 달성하였고, 11개 최첨단 편집 모델을 평가한 결과, Nano Banana가 전체적으로 가장 균형 잡힌 성능을 나타냈다.

## Limitations

강조된 도메인에 대해 스타일 변화 평가 기능이 부재하며, 자연 이미지용 객체 인식기의 한계로 인해 강한 스타일화 영역에서 안정적인 검증이 어렵다.

## Conclusion

객체 중심 접근과 전문가 도구 통합을 통한 EdiVal-Agent는 다중 회차 이미지 편집 평가의 실용적 발전을 촉진하는 효과적인 자동화 프레임워크임을 입증하였다.
