---
layout: post
title: "Daily Papers — 2025-12-22"
date: 2025-12-22 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing](https://arxiv.org/abs/2512.17909)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.17909)

## Introduction
- 본 연구의 목표는 표현 인코더의 특징을 재구성 및 의미적 정보 보존을 동시에 고려하는 잠재 공간으로 변환하여 텍스트-이미지 생성과 편집에 적합하도록 만드는 것이다.  
- 동기부여는 기존의 표현 인코더가 고차원 의미적 특징 공간에서 off-manifold latent 생성 및 빈약한 픽셀 재구성으로 인해 생성 작업에 직접 적용하기에 한계가 있기 때문이다.  
- 본 논문은 의미-픽셀 복원 손실을 도입한 PS-VAE를 제안하여 압축된 의미 공간과 고해상도 구조 및 질감 정보를 동시에 보존함으로써 생성과 편집 작업에서의 성능을 크게 향상시켰다.  

## Method  
표현 인코더에서 추출된 고차원 특징을 KL-규제된 압축 잠재 공간으로 매핑하는 S-VAE를 설계하여 off-manifold 문제를 개선했다.  
이어 픽셀 수준의 복원 손실과 의미 복원 손실을 결합하여 인코더를 미세 조정하는 PS-VAE를 개발함으로써 고해상도 세부 묘사와 의미 보존을 동시에 달성하였다.  
이를 기반으로 텍스트-이미지 생성과 이미지 편집을 통합하는 딥퓨전 아키텍처를 적용하여 실험하였다.  

## Results  
PS-VAE(96채널)는 기존 RAE 대비 재구성 품질(rFID 0.619→0.203, PSNR 19.20→28.79)과 텍스트-이미지 생성(GenEval 71.27%→76.56%) 및 편집 과제에서 우수한 성능과 빠른 수렴 속도를 달성하였다.  

## Limitations  
고해상도나 다양한 데이터셋에 대한 일반화 및 연산 효율성 최적화에 관해서는 추가 연구가 요구된다.  

## Conclusion  
PS-VAE는 의미적 구조와 미세한 픽셀 세부를 통합하여 표현 인코더를 강력한 생성 및 편집 모델의 공통 기반으로 성공적으로 변환시켰음을 검증하였다.

# 2. [HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering](https://arxiv.org/abs/2512.14870)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.14870)

## Introduction
- Goal: 본 논문은 비디오 질문응답(VideoQA)에서 여러 시점에 분산된 복수의 증거를 통합하는 능력을 평가하기 위한 벤치마크인 HERBench를 제안하는 데 목적이 있다.  
- Motivation: 기존 VideoQA 벤치마크들은 단일 프레임이나 언어적 편향에 의존해 고차원적 시공간적 추론 능력을 충분히 측정하지 못한다는 한계가 존재하였다.  
- Contribution: HERBench는 최소 세 개 이상의 비중복 증거를 요구하는 2만 6천여 개의 다중선택 질문과 12개의 구성적 과제로 구성되며, 증거 요구량을 수치화하는 MRFS 지표를 통해 기존 데이터셋 대비 높은 다중증거 종합 난이도를 입증하였다.  

## Method  
HERBench는 객체 추적, 장면 샷 분할, 인간 검증 내러티브 통합의 세 가지 데이터 처리 스트림을 활용한 엄격한 데이터 구축 파이프라인으로 구성되며, 각 문제는 3개 이상의 분리된 증거 프레임을 통합해야만 정답을 찾을 수 있도록 설계되었다.  
질문 유형은 시간적 순서 추론, 개체 추적, 전역 일관성 검증, 다중 개체 집계 및 수치화 네 가지 범주로 체계화되어 있다.  
MRFS(Minimum Required Frame-Set) 지표는 정답 도출에 필요한 최소 프레임 수를 계산하여 모델의 다중 증거 통합 여부를 정량적으로 평가한다.  

## Results  
최신 13개 Video-LLM 모델을 HERBench에서 평가한 결과, 평균 정확도는 31~42% 수준으로 20%의 무작위 추측을 앞서긴 하나 근본적인 다중 증거 종합 능력 부족을 드러냈다.  

## Limitations  
모델 성능 저하는 핵심 증거 프레임을 놓치는 증거 검색 부족과 증거를 효과적으로 융합하지 못하는 융합 부족이라는 두 가지 병목 현상에 기인한다.  

## Conclusion  
HERBench는 다중 시점 증거 통합을 강제하고 이를 수치화하여 현존 Video-LLM의 근본적 한계를 드러내고, 향후 영상 이해 및 복합 추론 능력 발전을 위한 명확한 연구 방향을 제시하는 중요한 평가 도구로 자리매김한다.

# 3. [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16483)

## Introduction
- 본 논문의 목표는 시각적 자기회귀(Visual Autoregressive, VAR) 모델의 계산 효율성을 개선하여 고품질 이미지 생성을 가속화하는 것이다.  
- 전통적인 AR 모델의 차세대 토큰 예측 방식에서 벗어난 VAR 모델은 고품질 이미지 생성에서 우수하나, 대규모 스케일 단계에서 계산 복잡도와 실행 시간이 크게 증가한다는 문제점을 가진다.  
- 이를 해결하기 위해 VAR 모델의 추론 과정을 체계적으로 분석하고, 단계별 특성을 반영한 StageVAR라는 플러그인 가속 프레임워크를 제안하였다.  

## Method
- VAR 추론 과정을 의미 구축 단계, 구조 구축 단계, 그리고 정밀 세부 조정 단계의 3단계로 구분하고, 초기 단계는 원본 방식을 유지하며 정밀 세부 조정 단계에 대해 의미 무관성 및 저랭크 특성을 활용한 가속 전략을 적용하였다.  
- 구체적으로 의미 조정 단계 이후부터는 텍스트 조건 입력을 생략하고, 중간 특징 맵의 저랭크 근사를 통해 계산량을 줄였으며, 이 과정에서 랜덤 프로젝션 및 대표 토큰 복원 기법을 활용하였다.  
- 별도의 모델 재훈련 없이 기존 VAR 모델에 쉽게 적용 가능한 점진적 구조의 플러그앤플레이 방식으로 제안되었다.  

## Results
- 제안된 StageVAR는 GenEval과 DPG 벤치마크에서 최대 3.4배의 속도 향상을 달성하면서 성능 저하는 극히 미미하여 기존 가속 기법들보다 우수한 효율-성능 균형을 보였다.  

## Limitations
- 본 연구에서는 주로 텍스트-기반 이미지 생성에 초점을 맞추었으며, 다른 유형의 VAR 모델이나 다양한 도메인에의 일반화에 관한 추가 연구가 필요하다.  

## Conclusion
- 본 연구는 VAR 모델의 단계별 특징을 체계적으로 분석하여 의미 무관성 및 저랭크 구조를 활용한 StageVAR를 통해 효율적인 고품질 이미지 생성을 가능하게 함으로써 VAR 추론 가속화에 기여하였다.

# 4. [Bolmo: Byteifying the Next Generation of Language Models](https://arxiv.org/abs/2512.15586)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.15586)

## Introduction
- 본 논문은 기존의 서브워드 단위 언어 모델을 바이트 단위로 변환(byteifying)하여 1B와 7B 파라미터 규모에서 경쟁력 있는 완전 공개 바이트 단위 언어 모델 계열인 Bolmo를 제안한다.  
- 서브워드 토크나이제이션의 문자 인식 한계와 고정 어휘 크기로 인한 효율성 제약 문제를 극복할 필요성이 존재한다.  
- Bolmo는 비약적인 사전학습 토큰 자원 투입 없이도 기존 서브워드 모델 성능을 모방 및 초과할 수 있도록 설계된 아키텍처와 효율적인 2단계 학습 절차로 byteifying을 실현하였다.

## Method
- Bolmo 아키텍처는 로컬 인코더, 프로비저닝 경계 예측기, 글로벌 모델, 로컬 디코더로 구성되며, 경계 예측 시 미래 한 글자를 참조하는 비인과적 방식으로 서브워드 토크나이저의 경계 생성 특성과 표현력을 맞췄다.  
- 학습은 (1) 고정된 글로벌 모델을 유지하며 서브워드 모델을 엄밀히 모방하는 경계 예측기 및 인코더-디코더를 학습하는 증류 단계, (2) 전체 모델을 최적화하며 바이트 단위 정보 활용과 바이트당 패치 압축비 조정을 수행하는 본훈련 단계로 이루어진다.  
- 출력 경계 예측을 로컬 디코더 출력 어휘에 결합하여 경계 예측 오버헤드를 최소화하고, 서브워드 임베딩 정보를 바이트 임베딩에 잔류적으로 더하여 표현력을 향상시켰다.

## Results
- Bolmo 7B는 기존 바이트 단위 모델 대비 STEM 과제에서 16.5% 절대 성능 향상을 달성하였으며, 서브워드 원본 모델보다 문자 이해 및 일부 코딩 과제에서 우수한 성능을 보였다.

## Limitations
- 비인과적 경계 예측으로 인한 완전한 엔드-투-엔드 경계 예측 학습은 본 연구에서 다루지 못했으며, 주로 외부 겨례 지도에 의존하였다.

## Conclusion
- Bolmo는 기존 서브워드 언어 모델의 뛰어난 성능을 바이트 단위로 거의 동일하게 모방하면서 효율성과 범용성을 갖춘 현실적인 바이트 단위 언어 모델의 실현 가능성을 제시하였다.

# 5. [3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework](https://arxiv.org/abs/2512.17459)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.17459)

## Introduction
- Goal: 본 연구는 단일 이미지로부터 실내 3D 장면을 텍스처가 입혀진 개별 3D 객체와 배경으로 구성하여 재구성하는 프레임워크인 3D-RE-GEN을 제안한다.  
- Motivation: 기존 3D 장면 재구성 기법들은 객체 분해 오류, 공간 관계의 부정확성, 배경 누락 등의 문제로 제작자 친화적인 결과물을 제공하지 못했다.  
- Contribution: 본 논문은 객체 검출, 재구성, 배치 모델들을 통합하고 현장 수준의 4-DoF 최적화 기법으로 물리적으로 타당한 배치와 완전한 배경을 포함하는 장면을 생성한다.

## Method  
3D-RE-GEN은 단일 RGB 이미지를 입력받아 객체 분할 및 문맥 인식을 활용한 인페인팅으로 개별 객체와 배경을 분리한다. 생성된 개별 객체 이미지는 2D-3D 변환 모델에 투입되어 텍스처가 있는 3D 메시를 생성하며, 배경 점군과 카메라 파라미터를 추정한다. 마지막으로, 4-DoF 제약 기반 차별화 렌더링 최적화 과정을 통해 객체를 실제 물리 바닥에 정렬하여 일관된 3D 장면을 조립한다.

## Results  
3D-RE-GEN은 합성 및 실제 데이터셋에서 기존 최신 기법 대비 챔퍼 거리 0.011 및 F-스코어 0.85 등 주요 3D 재구성 지표에서 우수한 성능을 보여준다.

## Limitations  
정보 부족.

## Conclusion  
3D-RE-GEN은 단일 이미지로부터 고품질의 텍스처드 3D 실내 장면을 완전하고 물리적으로 타당하게 자동 재구성하여 VFX 및 게임 개발에 즉시 활용 가능한 환경을 제공한다.
