---
layout: post
title: "Daily Papers — 2025-10-03"
date: 2025-10-03 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.00446)

## Introduction  
- 본 연구의 목표는 코드 언어 모델의 긴 문맥을 효과적으로 압축하여 처리 비용과 지연 시간을 줄이는 것이다.  
- 긴 코드 문맥 처리 시 높은 계산 복잡도와 코드 특유의 구조적 의존성 문제로 인해 기존 방법들이 성능 한계를 보이는 상황이 동기를 제공한다.  
- 본문에서는 코드 특화된 이중 단계 압축 전략과 적응적 토큰 배분을 활용한 LongCodeZip 프레임워크를 제안한다.  

## Method  
LongCodeZip은 함수 단위의 조대 압축과 펄플렉시티 기반 세밀 압축의 두 단계로 이루어진다. 먼저, 함수별 문맥을 작업 명령과의 조건부 펄플렉시티로 중요도 순위를 매겨 관련 함수만 선별하는 조대 압축을 수행한다. 그 후, 선택된 함수 내에서 의미적 경계를 펄플렉시티 기준으로 분할하고, 0/1 배낭 문제 최적화 기법으로 토큰 예산 내에서 가장 중요한 코드 블록을 선별하는 세밀 압축을 진행한다.  

## Results  
LongCodeZip은 코드 완성, 요약, 질문응답 등 다양한 코드 작업에서 최대 5.6배의 높은 압축률을 기록하면서도 성능 저하 없이 기존 최첨단 압축 및 검색 기반 방법들을 일관되게 능가하였다.  

## Limitations  
긴 코드 문맥 압축 시 일부 매우 복잡하거나 비정형적 의존성은 완벽히 보존하지 못하는 한계가 존재한다.  

## Conclusion  
LongCodeZip은 코드 언어 모델의 긴 문맥 문제를 효율적이고 효과적으로 해결하여 대규모 실제 코드 환경에서의 응용 가능성을 크게 확장한다.

# 2. [ModernVBERT: Towards Smaller Visual Document Retrievers](https://arxiv.org/abs/2510.01149)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.01149)

## Introduction
- Goal: 본 연구의 목표는 시각 문서 검색기의 성능 향상을 위한 효율적이고 소형화된 시각-언어 인코더 모델을 개발하는 것이다.  
- Motivation: 기존 대형 생성형 비전-언어 모델(VLM)을 재활용하는 접근법은 비용 효율적이나 문서 검색 성능에 병목 현상이 발생하는 문제점이 존재한다.  
- Contribution: 본 논문은 주의(attention) 마스킹, 이미지 해상도, 모달리티 정렬 데이터, 후반 상호작용 중심 대조 학습 등 핵심 설계 요소들의 영향을 체계적으로 분석하고, 이를 기반으로 2.5억 파라미터의 소형 인코더 ModernVBERT를 제안하였다.  

## Method  
모델은 Masked Language Modeling(MLM) 기반 양방향 주의 메커니즘을 사용하는 조기 융합(early fusion) 아키텍처로 설계되었으며, 시각 인코더와 텍스트 인코더를 고해상도 문서 이미지와 다국적 문서-텍스트 데이터로 모달리티 정렬하였다.  
정렬된 모델은 문서-쿼리 및 텍스트-텍스트 대조 데이터의 혼합을 활용하는 후속 대조학습 단계를 거쳐 문서 검색에 특화된 표현력을 학습하였다.  
철저한 통제 실험을 통해 인코더 및 디코더 기반 모델과 주의 마스크 유형, 해상도 증대 및 데이터 구성의 영향을 독립적으로 평가하였다.  

## Results  
ColModernVBERT는 2.5억 파라미터의 소형 모델임에도 불구하고 최대 10배 큰 기존 모델들과 동등하거나 우수한 문서 검색 성능을 보였으며, CPU 기반 추론에서 유의미한 속도 이점을 가졌다.  

## Limitations  
본 연구는 소형 및 중형 모델에 집중하였으며, 대규모 다국어 확장성 및 자연 이미지 검색과 같은 다른 도메인에 대한 일반화 여부는 추가 검증이 필요하다.  

## Conclusion  
시각 문서 검색 성능 향상을 위해서는 양방향 주의 기반 모델과 후반 상호작용 대조학습의 결합이 핵심이며, ModernVBERT는 작은 모델 크기로 고성능을 달성할 수 있음을 실험적으로 입증하였다.

# 3. [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.01284)

## Introduction
- 본 논문은 음성과 영상 두 모달리티를 단일 생성 과정으로 모델링하는 통합 오디오-비디오(AV) 생성 모델 OVI를 제안한다.  
- 기존의 AV 생성은 복잡한 다단계 아키텍처나 순차적 합성을 필요로 하였으나, 실제 영화 콘텐츠는 음성과 영상의 자연스러운 동기화를 요구한다.  
- OVI는 대규모 데이터와 대칭적 트윈-디퓨전 트랜스포머 구조를 이용해 블록 단위 양방향 교차 주의(attention)를 통해 정밀한 시공간적 융합 및 동기화를 달성한다.  

## Method  
OVI는 동일한 아키텍처를 공유하는 음성 및 영상 디퓨전 트랜스포머 백본을 대칭적으로 구성하고, 각 블록마다 양방향 교차 주의 메커니즘을 삽입하여 두 모달 간 시공간 정보를 교환한다.  
오디오 백본은 고품질 음성 및 다양한 효과음을 생성하도록 대규모 음성 데이터셋으로부터 처음부터 학습하며, 영상 백본과 함께 파인튜닝하여 통합 음성-영상 생성 능력을 획득한다.  
또한, 동일한 T5 인코더를 사용한 결합된 자연어 프롬프트로 제어함으로써 모달리티 간 의미적 일관성과 학습 안정성을 극대화한다.  

## Results  
OVI는 공개된 최첨단 오픈 소스 동시 생성 모델 대비 음질, 영상 품질, 음성-영상 동기화 측면에서 우수한 성능을 보이며, 인간 평가에서 일관된 선호도를 얻었다.  

## Limitations  
OVI는 현재 5초 길이의 720p 해상도 영상에 최적화되어 있어, 장편 영상 내러티브 및 글로벌 스토리 일관성 측면에서는 한계가 존재한다.  

## Conclusion  
OVI는 대칭적 트윈 백본과 블록 단위 교차 융합을 통해 음성과 영상을 단일 생성 과정으로 통합함으로써, 선행 연구 대비 간결하면서도 고품질의 동시 오디오-비디오 생성 가능성을 실증하였다.

# 4. [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag   Editing](https://arxiv.org/abs/2510.02253)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.02253)

## Introduction
- Goal: DragFlow는 Diffusion Transformer(DiT) 기반 모델의 강력한 생성 prior를 활용하여 드래그 기반 이미지 편집의 왜곡 문제를 해결하는 프레임워크이다.  
- Motivation: 기존 Stable Diffusion 기반 드래그 편집 방식은 생성 prior가 약해 복잡한 영역에서 기형적 변형을 유발하는 한계가 존재하였다.  
- Contribution: DragFlow는 점기반(point-based) 감독 대신 영역 기반(region-based) Affine 변환 감독과 배경 강제 보존, 사전학습된 어댑터를 활용한 주제 일관성 강화 기법을 도입하여 DiT의 prior를 효과적으로 이용한다.  

## Method  
DragFlow는 사용자가 지정한 소스 영역과 목표 지점을 기반으로 영역 단위 Affine 변환을 통하여 피처를 비교하는 영역 수준 감독을 수행한다. 배경 영역은 하드 제약으로 보존하며, CFG-distilled 특성으로 인한 도메인 역변환(Inversion) 드리프트 문제를 사전학습된 개인화 어댑터 삽입으로 완화한다. 추가로 대형 다중모달 언어모델(MLLM)을 활용해 편집 의도를 명확히 하여 세밀한 제어가 가능하도록 설계되었다.  

## Results  
ReD Bench와 DragBench-DR 벤치마크 실험에서 DragFlow는 최저 왜곡(MD)과 우수한 영역 정합성(IFs2s, IFs2t)을 포함해 모든 평가 지표에서 기존 최첨단 기법들을 능가하였다.  

## Limitations  
FLUX 모델의 CFG-distilled 구조상 도메인 역변환의 드리프트가 커서 복잡한 이미지 구조에서 일부 세부 정보 손실 및 편집 품질 저하가 발생하였다.  

## Conclusion  
DragFlow는 DiT 백본의 강력한 생성 prior를 활용하여 영역 단위 감독과 배경 보존, 어댑터 강화 역변환을 통해 드래그 기반 이미지 편집의 왜곡을 크게 줄이고 편집 충실도를 높인 실질적이고 새로운 접근법임을 입증하였다.

# 5. [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.02295)

## Introduction
- Goal: 본 연구의 목표는 Native Sparse Attention(NSA)를 활용하여 멀티모달 비디오-언어 모델의 긴 컨텍스트 처리 문제를 해결하고 비디오 이해 성능을 향상하는 것이다.  
- Motivation: 기존의 비디오 언어 모델은 제한된 컨텍스트 길이로 인해 핵심 전환 프레임을 놓치거나 긴 시간 축에서의 일관성을 유지하는 데 어려움이 있다.  
- Contribution: VideoNSA라는 하드웨어 인지형 하이브리드 희소 어텐션 방식을 제안하고, 이를 통해 128K 토큰에 이르는 초장기 비디오-텍스트 컨텍스트를 효율적으로 처리할 수 있음을 실험적으로 입증하였다.  

## Method  
VideoNSA는 Qwen2.5-VL-7B 모델을 기반으로 하며, 비디오 토큰에 대해 토큰 압축, 선택, 슬라이딩 윈도우의 세 가지 희소 어텐션 분기를 학습 가능한 게이트를 통해 동적으로 결합하는 하이브리드 희소 어텐션을 적용한다. 텍스트 토큰에는 그룹 쿼리 어텐션(Grouped Query Attention)을 유지하여 지시문 수행 능력을 보존한다. 최대 36K 토큰 훈련 데이터로 엔드-투-엔드 학습을 진행하여 시간적 중복성을 줄이고 고효율 대용량 비디오 이해를 가능하게 하였다.

## Results  
VideoNSA는 LongVideoBench, MLVU, TimeScope, Tomato, VSIBench 등 다양한 벤치마크에서 기존의 토큰 압축 및 무학습 희소 어텐션 기법 대비 우수한 성능을 보여, 특히 초장기 비디오와 시간적 추론 과제에서 탁월한 성과를 기록하였다.

## Limitations  
본 연구에서 여전히 프리필 단계가 계산 병목점으로 남아 있으며, 토큰 압축 분기의 커널 및 메모리 효율 최적화가 추가로 요구된다.

## Conclusion  
VideoNSA는 하드웨어 인지형 동적 희소 어텐션을 통해 초장기 비디오 이해 성능을 향상시키며, 확장 가능하고 효율적인 비디오 기초 모델 개발에 중요한 기반을 마련하였다.

# 6. [Visual Multi-Agent System: Mitigating Hallucination Snowballing via   Visual Flow](https://arxiv.org/abs/2509.21789)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21789)

## Introduction
- Goal: 본 연구는 다중 에이전트 시스템(Multi-Agent System, MAS)에서 시각 정보 전달 과정에서 발생하는 환각 효과인 ‘다중 에이전트 시각 환각 눈덩이 현상(multi-agent visual hallucination snowballing)’을 완화하는 방법을 제안하는 것이다.  
- Motivation: 기존 MAS는 시각 정보를 텍스트 흐름으로만 전달하여 초기 에이전트의 시각적 오류가 이후 에이전트들에 의해 증폭되는 한계가 존재한다.  
- Contribution: 본 논문은 시각 토큰의 주목(attention) 감소가 환각 눈덩이 현상의 본질임을 규명하고, 선택된 시각 중계 토큰과 주목 재분배(attention reallocation)를 활용한 플러그앤플레이 방식의 ‘ViF’ 기법을 제안하였다.  

## Method  
ViF는 중간 계층에서 주목의 단봉형(unimodal) 패턴을 보이는 시각 토큰을 선택해 이전 에이전트로부터 시각 정보를 직접 전달하는 ‘시각 흐름(visual flow)’을 생성한다. 이어서 중간 및 깊은 계층에서 주목 재분배를 통해 시각 토큰의 영향력을 증폭시킨다. 또한 주목 점수 미획득 모델을 위한 키-노름 기반 대체 방식을 제공한다.  

## Results  
ViF는 4가지 MAS 구조와 10개의 기본 VLM 모델을 대상으로 8개의 장르별 벤치마크에서 평균 2.4~4.4% 성능 향상을 달성하고, 환각 눈덩이 현상을 최대 40% 이상 감소시키는 효과를 보였다.  

## Limitations  
제안 방법은 단일 에이전트 환경에서는 상대적으로 미미한 개선을 보이며, 주로 다중 에이전트 협업 환경에서 유의미한 성능 향상을 나타낸다.  

## Conclusion  
ViF는 기존 텍스트 흐름 의존 방식을 넘어서 시각 정보를 효과적으로 유지, 전달하여 다중 에이전트 시스템에서 발생하는 시각 환각 증폭 문제를 실질적으로 완화한다.

# 7. [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject   Fidelity](https://arxiv.org/abs/2510.02315)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.02315)

## Introduction
- Goal: 본 연구는 텍스트-이미지 변환 모델에서 다중 객체 표현의 충실도를 향상시키기 위한 이론적 최적 제어 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 기존 텍스트-이미지 생성 모델은 다중 대상 프롬프트에 대해 속성 누수, 정체성 얽힘, 대상 누락 등 문제로 정확한 구성이 어려웠다.  
- Contribution: 확률적 최적 제어 문제로서 흐름 매칭(Flow Matching)을 통해 다중 객체 분리 문제를 공식화하고, 테스트 시 제어기와 경량 미세조정 알고리즘을 개발하였다.  

## Method  
흐름 매칭 샘플러를 기반으로 다중 객체 얽힘을 최소화하는 최적 제어 문제를 정의하였다.  
테스트 시 단일 경로에서 제어 신호를 근사 계산하는 온더플라이 제어기와, 역방향 어조인트 신호를 활용해 미세조정을 수행하는 Adjoint Matching 방식을 제안하였다.  
또한 객체별 교차 어텐션 분포의 분산과 상호 분리를 동시에 최소화하는 확률적 어텐션 손실 FOCUS를 도입하였다.  

## Results  
Stable Diffusion 3.5, FLUX 및 Stable Diffusion XL 모델에서 제안된 알고리즘들이 다중 객체 정합도를 일관되게 개선하며 기본 스타일과 생성 속도를 유지하는 성과를 보였다.  

## Limitations  
테스트 시 제어 방식은 추론 시간이 약 2배 증가하며, 미세조정은 주제 토큰 주석이 필요하고 훈련 중에만 적용 가능하다.  

## Conclusion  
본 연구는 기존의 다중 객체 생성 문제에 대해 최적 제어 이론을 접목한 통합적 접근법을 제안하며 다중 객체 충실도 향상에 효과적인 실험적 증거를 제공하였다.

# 8. [Sparse Query Attention (SQA): A Computationally Efficient Attention   Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.01817)

## Introduction
- Goal: 본 논문은 계산 복잡도를 줄이기 위해 쿼리 헤드 수를 감소시키는 새로운 효율적 어텐션 메커니즘인 Sparse Query Attention(SQA)를 제안하는 데 목적이 있다.  
- Motivation: 기존의 Multi-Query Attention(MQA) 및 Grouped-Query Attention(GQA) 방식은 메모리 대역폭 병목을 완화하지만, 훈련과 전체 시퀀스 처리에 주요한 계산량(FLOPs) 감소에는 한계가 존재한다.  
- Contribution: SQA는 쿼리 헤드 수를 줄여 어텐션 계산 비용을 줄이며, 이론적 근거와 여러 변형 아키텍처를 제시하고, 긴 시퀀스에서 최대 3배의 처리량 향상을 실험적으로 입증하였다.  

## Method  
Sparse Query Attention는 쿼리 헤드 수 Hq를 기존 MHA의 전체 헤드 수 H보다 적게 설정하여 QKT 연산에 필요한 계산량을 비례적으로 감소시키는 방식이다.  
키/밸류 헤드 수 Hkv는 Hq 이하로 설정하며, 키/밸류 텐서들을 쿼리 헤드 수에 맞게 복제해 계산한다.  
이러한 구조적 변경을 통해 쿼리 헤드 감소 비율(H/Hq)에 따라 FLOPs를 줄임으로써 계산 효율성을 확보한다.  

## Results  
긴 시퀀스(32k~200k 토큰) 처리 시 SQA는 GQA 대비 최대 3.4배 빠른 처리 속도를 달성하며, 초소규모 실험에서 모델 품질 저하는 미미한 수준임을 보였다.  

## Limitations  
SQA는 계산 비용 절감에 초점을 두어 메모리 대역폭 병목 문제에는 제한적인 개선만을 제공한다는 한계가 존재한다.  

## Conclusion  
SQA는 쿼리 헤드를 줄이는 혁신적 접근으로 계산 병목 문제를 완화하며, 기존 메모리 중심 최적화와 상호 보완이 가능해 Transformer 모델의 효율성과 확장성을 높이는 유망한 방법임을 제시한다.

# 9. [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.01796)

## Introduction
- Goal: 본 논문은 전통적인 MLP 구조의 narrow-wide-narrow 형태를 재고하여 wide-narrow-wide (Hourglass) 형태의 MLP 블록을 제안하는 것을 목표로 한다.  
- Motivation: 기존 narrow-wide-narrow MLP는 스킵 연결이 좁은 차원에서 이루어져 잔차 업데이트가 저차원에 제한되나, 고차원에서 점진적 개선을 수행하는 것이 더 효과적일 수 있다는 가설에 기반한다.  
- Contribution: 본 연구는 고차원에서 스킵 연결을 유지하고 좁은 병목을 통해 연산이 이루어지도록 설계된 wide-narrow-wide MLP 구조를 제안하며, 학습 중 무작위 고정 입력 사영을 활용하여 효율적 구현이 가능함을 보인다.  

## Method  
Hourglass MLP는 입력을 고차원 잠재 공간으로 선형 투영하는 고정 무작위 사영을 사용하며, 이후 넓은 차원의 스킵 연결과 좁은 병목 경로를 갖는 잔차 MLP 블록들을 층으로 쌓아 구성된다.  
각 블록은 고차원 스킵 연결로 정보 보존하면서 좁은 병목에서 점진적 잔차 보정을 수행하며, 출력은 추가 사영층을 통해 원하는 형식으로 변환된다.  
매개변수 수가 유사하도록 설계된 Hourglass 구조는 깊이, 병목 폭 및 잠재 차원 간 균형을 맞추어 최적의 성능을 도출한다.  

## Results  
다양한 이미지 생성 과제(MNIST, ImageNet-32)에서 Hourglass MLP는 전통적 narrow-wide-narrow MLP 대비 파라미터 효율과 성능 측면에서 우수한 파레토 전선을 일관되게 달성하였다.  

## Limitations  
본 연구는 계산 자원 한계로 비교적 저차원 이미지 데이터셋에 집중하여 고해상도 응용에 대한 확장성 및 실용성은 추가 연구가 필요하다.  

## Conclusion  
제안된 wide-narrow-wide MLP는 잔차 학습에서 스킵 연결 위치 재고를 통해 더 우수한 표현력과 효율성을 달성하며, 향후 Transformer 및 기타 잔차 네트워크에도 적용 가능성을 시사한다.

# 10. [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/abs/2510.00523)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.00523)

## Introduction
- Goal: 본 연구는 시각적 인터랙티브 기능을 갖춘 텍스트-이미지 통합 임베딩 모델 VIRTUE를 제안하는 데 목적이 있다.  
- Motivation: 기존 임베딩 모델은 시각적 상호작용, 즉 사용자가 지정한 관심 영역(포인트, 바운딩 박스, 마스크)을 처리하는 기능이 부족하여, 객체 수준 정보 학습 및 정교한 지역 기반 검색에 한계를 보인다.  
- Contribution: VIRTUE는 세분화(segmentation) 모델과 비전-언어 모델을 결합하여 시각적 프롬프트를 처리하고, 100만 개 샘플 규모의 SCaR 벤치마크를 새로 구축하여 시각적 인터랙션 임베딩 성능을 평가한다.  

## Method  
VIRTUE는 사전 학습된 세분화 모델 SAM-2와 비전-언어 모델(Qwen2-VL)을 통합하여, 사용자가 제공하는 시각적 프롬프트를 세분화 임베딩으로 변환 후 이를 전역 이미지 임베딩과 결합하여 엔티티 및 씬 레벨 정보를 동시에 포착한다. 세분화-언어 커넥터는 세분화 피쳐를 LLM 차원으로 변환하며, 이를 LLM에 입력해 단일 임베딩으로 학습한다. 학습은 대조 학습(contrastive learning)을 사용하며, 시각적 상호작용 유무에 따라 자동 샘플링 또는 명시적 시각 프롬프트를 적용한다.  

## Results  
VIRTUE는 기존 최첨단 모델 대비 MMEB 벤치마크 36개 작업에서 3.1%~8.5%, SCaR 벤치마크 5개 작업에서 15.2%~20.3%의 성능 향상을 기록하였다.  

## Limitations  
논문에서는 VIRTUE의 제한점에 대해 구체적으로 기술하지 않았으며, 관련 내용은 부록에 상세히 서술되었다.  

## Conclusion  
VIRTUE는 시각적 인터랙션을 통합하여 엔티티 인지 및 전역 씬 정보를 결합하는 범용 텍스트-이미지 임베더로서, 기존 임베딩 모델의 한계를 극복하고 새로운 인간-인공지능 상호작용 가능성을 제시한다.

# 11. [Spectral Scaling Laws in Language Models: How Effectively Do   Feed-Forward Networks Use Their Latent Space?](https://arxiv.org/abs/2510.00537)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.00537)

## Introduction
- 본 연구의 목표는 FFN(Feed-Forward Network) 너비가 언어 모델의 잠재 공간 활용에 미치는 영향을 스펙트럼적 관점에서 규명하는 것이다.  
- 기존 확장 법칙들은 모델 크기와 성능 관계에 집중하며 구성 요소가 잠재 공간을 어떻게 활용하는지에 대한 이해가 부족하였다.  
- 본 논문은 FFN 너비 선택 문제를 스펙트럼 활용 최적화 문제로 재정의하고, LLaMA, GPT-2 및 nGPT 계열 모델을 대상으로 비대칭 스펙트럼 확장 법칙을 도출하였다.  

## Method  
- FFN 내부 표현의 층별 후활성화 공분산 행렬의 고유 분해를 수행하여 고유값 스펙트럼을 분석하였다.  
- 하드 랭크, 소프트 랭크, 스펙트럼 집중도, 그리고 하드·소프트 랭크를 조화평균한 스펙트럼 활용 지수(SUI) 등 네 가지 경량 지표를 제안하여 잠재 방향들의 활용도를 정량화하였다.  
- 이를 통해 FFN 너비에 따른 효과적 활용 차원 수와 활성화 패턴을 수치적으로 평가하였다.  

## Results  
- FFN 너비 확장 시 소프트 랭크는 너비에 거의 선형적으로 비례하는 반면 하드 랭크는 약한 아원함수적 증가를 보이며, 이는 너비 확장이 주로 낮은 에너지 꼬리 방향을 추가해 우세 모드의 잠재 공간 활용이 조기 포화됨을 시사한다.  

## Limitations  
- 본 연구는 FFN 너비에 국한되어 있으며, 깊이나 기타 아키텍처 요소의 영향력에 대한 분석은 부족하였다.  

## Conclusion  
- FFN 너비 선택을 스펙트럼 활용 관점에서 재고하고, 효과적인 잠재 공간 활용과 너비 확장 간 트레이드오프를 규명하여 효율적인 LLM 설계에 실질적 방향을 제시하였다.

# 12. [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22582)

## Introduction
- Goal: 본 연구의 목표는 대형 언어 모델(LLM)을 활용하여 문맥 기반 환각(hallucinations)의 미세 단위 검출을 수행하는 것이다.  
- Motivation: 기존의 복잡한 평가 파이프라인 대신 LLM을 통한 실용적이고 효과적인 오류 국지화 방법의 필요성이 제기되었기 때문이다.  
- Contribution: 본 논문은 LLM 맞춤형 신규 벤치마크를 구축하고 자유 형식 오류 서술 방식을 도입하며 LLM 기반 평가 프로토콜을 설계하여, 다양한 모델의 오류 검출 성능을 종합적으로 분석하였다.  

## Method
LLM이 사실 불일치 오류를 자유 서술 형태로 설명하도록 유도하는 방식을 제안하였다. DeFacto 데이터셋을 바탕으로 전문 연구진의 엄격한 인간 검수를 거쳐 1,405개의 문맥-요약 쌍에 대해 상세 오류 주석을 구축하였다. 오류 검출 결과의 평가를 위해 LLM을 평가자(judge)로 활용하는 자동화된 매칭 프로토콜을 개발하고 인간 평가를 통해 신뢰성을 검증하였다.  

## Results  
GPT-4o, Claude-Sonnet, Gemini-Pro, Llama-3 등 4개 대형 모델을 대상으로 실험한 결과, 최고 성능의 모델도 F1 점수 0.67에 불과하여 높은 난이도를 확인하였다.  

## Limitations  
LLM은 (1) 누락된 상세 정보를 일관성 오류로 과대 분류하는 경향과 (2) 원문에 없으나 LLM 내부 지식과 부합하는 사실을 오히려 일관성 있는 정보로 잘못 판단하는 문제를 드러냈다.  

## Conclusion  
본 연구는 LLM을 활용한 미세 단위 문맥 환각 검출의 가능성과 한계를 명확히 밝히며, 향후 모델 개선 및 평가 방법론 발전에 중요한 방향을 제시한다.

# 13. [SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking   for Training-free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2509.26330)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26330)

## Introduction
- Composed Image Retrieval(CIR)는 참조 이미지의 시각적 내용을 유지하면서 텍스트 수정 내용을 반영한 목표 이미지를 검색하는 과제이다.  
- 기존의 학습 기반 접근법은 주석 데이터가 필요하여 확장이 어렵고, 학습 없는 제로샷 CIR 방법의 정확한 사용자 의도 파악은 여전히 어려운 문제이다.  
- 본 논문에서는 Multimodal Large Language Models(MLLMs)를 활용하여 학습 없이 제로샷 CIR의 성능을 향상시키는 두 단계의 새로운 프레임워크 SQUARE를 제안한다.  

## Method  
SQUARE는 Semantic Query-Augmented Fusion(SQAF)과 Efficient Batch Reranking(EBR)의 두 단계로 구성된다.  
SQAF 단계에서는 참조 이미지와 텍스트 수정의 임베딩에 MLLM이 생성한 목표 이미지 설명을 융합하여 쿼리를 의미론적으로 풍부하게 만든다.  
EBR 단계에서는 상위 후보 이미지들을 격자 형태로 제시하여 MLLM의 다중 모달 추론으로 일괄 재정렬을 수행함으로써 더 정확한 순위를 산출한다.  

## Results  
SQUARE는 네 가지 공개 CIR 벤치마크에서 기존 학습 없는 방법보다 우수한 성능을 일관되게 달성하였으며, 경량화된 백본에서도 강력한 실용성과 확장성을 보여준다.  

## Limitations  
객관 대상 변경과 같이 일부 특정 작업에서는 MLLM이 관련 없는 배경 요소를 과도하게 강조하여 성능 저하를 보이는 경우가 존재한다.  

## Conclusion  
SQUARE는 학습 없이도 MLLM의 의미론적 풍부함과 효율적 배치 재정렬 기법을 결합하여 제로샷 구성 이미지 검색에서 뛰어난 정확도와 효율성을 달성하는 효과적인 프레임워크이다.
