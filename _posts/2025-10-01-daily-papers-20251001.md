---
layout: post
title: "Daily Papers — 2025-10-01"
date: 2025-10-01 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token   Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.23873)

## Introduction
- Goal: 본 연구는 대규모 언어 모델(Large Language Models, LLMs)의 감독 학습 미세조정(Supervised Fine-Tuning, SFT)에서 샘플과 토큰 수준의 동시 가지치기(pruning)를 통해 데이터 효율성을 극대화하는 통합적인 방법을 제안하는 것이다.  
- Motivation: 기존의 데이터 가지치기 방법은 샘플 또는 토큰 수준에서 개별적으로 작동하여 두 차원의 정보를 함께 최적화하지 못함으로써 중요한 데이터 손실이나 불필요한 중복이 발생하는 문제점이 존재한다.  
- Contribution: 오류-불확실성(Error-Uncertainty) 평면이라는 진단 도구를 도입하고, 이를 기반으로 샘플과 토큰 가지치기를 전략적으로 조율하는 Q-Tuning 프레임워크를 제안하여 예산 제한 환경에서도 SFT 성능을 향상시키는 동작 원리를 증명하였다.  

## Method  
본 연구에서는 오류(Perplexity)와 불확실성(Entropy)을 기준으로 훈련 데이터를 네 구간(Q1~Q4)으로 분류하는 EU 평면을 개발하였다. Q-Tuning은 우선 유해 노이즈(Q1)와 중복 지식(Q3) 샘플을 샘플 수준에서 제거하고, 가치 있는 오개념(Q2) 샘플 내에서 문맥을 고려한 비대칭 토큰 가지치기를 수행하며, 보정 데이터(Q4)는 온전히 보존한다. 이러한 두 단계의 동적 가지치기 전략은 샘플과 토큰의 상호 의존적 중요도를 통합하여 훈련 효율성과 모델 성능을 동시에 개선한다.  

## Results  
Q-Tuning은 LLaMA3-8B, Mistral-7B, SmolLM2-1.7B 등 다양한 모델과 다섯 개의 벤치마크에서 전체 데이터 사용 대비 최대 38% 이상의 성능 향상을 보이며, 12.5%의 데이터만으로도 완전한 데이터 기반 미세조정보다 우수한 결과를 달성하였다.  

## Limitations  
본 연구의 분석 및 실험은 주로 언어 모델의 특정 크기와 데이터셋에 한정되어 있어, 보다 대규모 모델이나 다양한 태스크에 대한 일반화 가능성은 추가 검증이 필요하다.  

## Conclusion  
Q-Tuning은 오류-불확실성 평면 분석을 활용하여 샘플과 토큰 수준의 가지치기를 통합·최적화함으로써 제한된 예산 내에서 데이터 활용도를 극대화하며, 대형 언어 모델의 감독 학습 미세조정 효율화를 위한 실용적이고 확장 가능한 전략을 제시한다.

# 2. [dParallel: Learnable Parallel Decoding for dLLMs](https://arxiv.org/abs/2509.26488)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26488)

## Introduction
- 목표는 diffusion large language models(dLLMs)의 병렬 디코딩 성능을 효과적으로 향상시키는 방법을 제안하는 것이다.  
- 기존 dLLMs는 이론적으로 병렬 토큰 예측 능력을 가지나, 실제 공개 모델은 순차적 확실성 수렴 문제로 인해 디코딩 단계가 시퀀스 길이에 비례하여 시간이 오래 걸진다.  
- 본 연구는 dParallel이라는 학습 기반 병렬 디코딩 방법과 확실성 강제 증류(certainty-forcing distillation) 훈련 전략을 제안하여 dLLMs의 병렬성 한계를 극복하였다.  

## Method  
제안된 확실성 강제 증류는 사전학습된 dLLM의 원래 생성 궤적을 유지하면서 마스킹된 토큰의 예측 확실성을 빠르고 병렬적으로 높이도록 학습함으로써, 확실성의 순차적 전파를 병렬 수렴으로 전환하였다.  
이를 위해 블록 단위 반자동회귀적 마스킹을 적용하고, 정답 토큰에 대한 교차 엔트로피 손실과 함께 예측 분포의 엔트로피 감소 손실을 동시에 최적화하였다.  
이 과정은 기존 교사 모델에서 생성한 궤적에 자기 증류하는 형태로 이루어지며, LoRA를 활용해 효율적으로 수행되었다.  

## Results  
LLaDA-8B-Instruct 모델에 본 방법을 적용한 결과, GSM8K 벤치마크에서 디코딩 단계를 256에서 30으로 88% 줄임과 동시에 8.5배 속도 향상을 보였으며, MBPP에서는 256단계에서 24단계로 91% 감축과 10.5배 가속을 달성하여 정확도를 유지하였다.  

## Limitations  
Dream 모델 등 일부 dLLM 구조에서는 반자동회귀식 마스킹 훈련 시 원래 AR 모델로 회귀하는 경향이 관찰되어, 완전 무작위 마스킹을 사용해야 하는 제약이 존재한다.  

## Conclusion  
dParallel은 dLLMs 디코딩에서 확실성의 순차적 수렴 병목을 극복하고 병렬 디코딩을 효과적으로 활성화하는 새로운 학습 패러다임을 제시함으로써 병렬 디코딩 연구의 기준선을 확립하였다.

# 3. [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal   LLMs](https://arxiv.org/abs/2509.22646)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22646)

## Introduction
- Goal: 본 논문은 인간이 AI가 생성한 영상에서 인지하는 가짜 흔적을 식별하고 근거를 제공할 수 있는지를 연구하는 것을 목표로 한다.  
- Motivation: 기존 영상 생성 모델 평가들은 영상의 겉모습이나 사전 정의된 기준에만 집중하여, 인간이 인지하는 깊은 가짜 단서를 간과해왔다.  
- Contribution: 인간이 인지하는 깊은 가짜 흔적에 대해 시공간적으로 정밀 주석된 DEEPTRACEREWARD 데이터셋과 이를 활용한 멀티모달 대형언어모델 기반 보상 모델을 제안하였다.  

## Method  
DEEPTRACEREWARD 데이터셋은 7개의 최첨단 텍스트-투-비디오 모델이 생성한 3,318개 고품질 영상에 대해 전문가가 프레임별 바운딩 박스, 가짜 단서의 시작/종료 시점, 자연어 설명을 정밀 주석하였다.  
주석된 가짜 단서는 객체 왜곡, 갑작스러운 블러 처리, 객체 합병 등 총 9가지 주요 카테고리로 분류되었으며, 이를 기반으로 멀티모달 언어모델을 학습하여 인간의 판단과 위치 지정을 모방하도록 하였다.  
최신 13종 멀티모달 모델을 평가하고, VideoLLaMA 3 기반의 7B 파라미터 보상 모델이 DEEPTRACEREWARD에서 가장 우수한 성능을 내는 것을 확인하였다.  

## Results  
최고 성능의 보상 모델은 GPT-5 대비 가짜 단서 식별 및 위치 지정에서 평균 34.7% 향상된 종합 성능 70.2%를 기록하였고, 이진 가짜-진짜 분류보다 미세한 가짜 단서 검출 과제가 훨씬 어렵다는 난이도 경사를 관찰하였다.  

## Limitations  
멀티모달 언어모델의 미세한 가짜 단서 시공간적 위치 추정 성능은 아직 인간 수준에는 미치지 못하며, 특히 시간적 위치 지정에서 큰 어려움이 존재한다.  

## Conclusion  
DEEPTRACEREWARD는 인간 인지 기반의 정밀하고 시공간적으로 주석된 가짜 단서 데이터셋과 이를 활용한 보상 모델을 통해 AI 생성 영상의 신뢰성 평가와 생성 품질 향상 연구에 중요한 자원으로 작용할 것으로 기대된다.

# 4. [Humanline: Online Alignment as Perceptual Loss](https://arxiv.org/abs/2509.24207)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24207)

## Introduction
- Goal: 본 연구의 목표는 온라인 온폴리시(alignment) 방법이 오프라인 오프폴리시 방법보다 더 우수한 이유를 인간 중심의 지각 손실(perceptual loss) 관점에서 설명하는 것이다.  
- Motivation: 온라인 정렬은 계산 비용과 학습 시간이 크고 불안정함에도 불구하고 더 높은 성능 한계를 보이는 반면, 왜 그런 성능 차이가 발생하는지는 명확하지 않다.  
- Contribution: 본 논문은 전망 이론(prospect theory)을 통해 인간이 지각하는 확률 분포에 기반한 학습 방식을 제안하고, 이를 반영한 휴먼라인(humanline) 설계 패턴을 통해 오프라인 데이터로도 온라인 방식의 성능을 재현할 수 있음을 보였다.  

## Method  
본 연구는 정책 대비 참조 모델의 토큰 단위 우도 비율을 인간의 확률 왜곡에 맞게 비대칭 클리핑하는 휴먼라인 클리핑 방식을 도입하였다. 또한 정책과 참조 모델 동기화 주기를 두어 참조 모델이 정책 변화를 반영하게 하였으며, 이로써 휴먼라인 변종을 기존 DPO, KTO, GRPO 정렬 목표에 확장 적용하였다. 제안된 방법은 온라인/오프라인 데이터 모두에 적용 가능하며 온라인 온폴리시 데이터에만 의존하지 않는다.  

## Results  
휴먼라인 변종은 Llama3-8B-Instruct 모델을 사용한 지시 수행 과제에서 오프라인 데이터임에도 온라인 방식과 동등한 성능(1.3~1.6배 향상)을 달성하였고, 수학 문제 해결 과제에서는 온라인 데이터 샘플링 빈도를 최대 64배 줄여도 성능 손실 없이 학습할 수 있음을 확인하였다.  

## Limitations  
휴먼라인 변종이 모든 종류의 오프라인 데이터에 대해 온라인 방식과 같은 성능을 보장하지 않으며, 데이터 품질과 분포의 차이가 결과에 영향을 미칠 수 있다.  

## Conclusion  
인간의 확률 지각 왜곡을 반영한 휴먼라인 설계는 온라인/오프라인 데이터 출처의 구분을 불필요하게 만들며, 이를 통해 정렬 학습을 더 빠르고 저렴하게 수행할 수 있음을 시사한다.

# 5. [VisualOverload: Probing Visual Understanding of VLMs in Really Dense   Scenes](https://arxiv.org/abs/2509.25339)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.25339)

## Introduction
- Goal: 본 연구는 시각적 복잡도가 높은 장면에서 대형 비전-언어 모델(VLM)의 기본적인 시각 이해 능력을 평가할 수 있는 VisualOverload 벤치마크를 제안하는 데 목적이 있다.  
- Motivation: 기존 VQA 데이터셋이 전반적인 이미지 이해에만 집중하여, 밀집되고 고해상도의 복잡한 장면에서의 세밀한 인지 및 추론 능력을 과대평가한다고 판단하였다.  
- Contribution: VisualOverload는 150점의 고해상도 공공 도메인 회화 이미지와 6개 핵심 시각 인식 과제를 포함하는 2,720개의 수작업 기반 질문-응답 쌍을 제공하며, 37개 모델 평가 결과 현존 최고 모델도 어려운 문제에서 낮은 정확도를 보임을 입증하였다.  

## Method  
VisualOverload 데이터셋은 고해상도(최소 Full HD 이상)의 시각적으로 과부하된 회화 이미지 150점으로 구성되었으며, 여섯 가지 핵심 과제(활동 인식, 속성 인식, 개수 세기, OCR, 추론, 장면 분류)에 대한 사람이 직접 작성한 질문과 정답을 포함한다.  
모든 질문은 모호성 없이 이미지 내 관찰 가능한 정보에 기반하여 작성하였고, 일부는 다중선택, 일부는 자유서술 형식을 채택하였다.  
평가 시에는 사전에 공개하지 않은 정답을 보유하며, 이를 통해 모델의 편향과 과대적합 문제를 방지하고 제3자가 직접 서버에 제출하여 정확도를 측정하도록 하였다.  

## Results  
VisualOverload에서 최고 성능 모델(o3)은 어려운 문제 영역에서 19.6% 정확도에 그쳤으며, 전체 질문에 대해 평균 69.5% 정확도를 기록하여 세밀한 시각 인지 및 추론 능력에서 현 수준 VLM의 한계를 명확히 드러냈다.  

## Limitations  
본 연구는 평가 대상 모델들이 고해상도 장면 인식 능력이 제한적임을 보여주었으나, 제한된 예술작품 데이터 및 사전학습에 따른 편향이 완전히 배제되지 않았다는 점에서 한계가 존재한다.  

## Conclusion  
VisualOverload 벤치마크는 고도로 복잡하고 밀집된 시각 환경에서 VLM의 근본적인 시각 이해 능력이 여전히 미비함을 밝히고, 보다 견고하고 정교한 모델 개발을 위한 필수적인 평가 자원을 제공한다.

# 6. [Regression Language Models for Code](https://arxiv.org/abs/2509.26476)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26476)

## Introduction
- 본 연구의 목표는 다양한 프로그래밍 언어로 작성된 코드에서 실행 지표(메모리 사용량, 지연 시간, 신경망 정확도 등)를 직접 예측하는 통합 회귀 언어 모델(Regression Language Model, RLM)을 개발하는 것이다.  
- 기존의 연구들은 복잡하고 도메인 특화된 특징 공학에 의존하는 반면, 본 연구는 텍스트 기반 단일 모델로 여러 다양한 코드 실행 지표를 동시에 예측하는 방식을 제안하였다.  
- 본 연구의 주요 기여는 사전학습된 T5Gemma 기반의 RLM을 활용하여 17개 프로그래밍 언어와 여러 NAS(Neural Architecture Search) 디자인 공간에서 경쟁력 있는 성능을 달성하고, 다중 목표 예측도 가능함을 실험적으로 입증한 것이다.  

## Method  
RLM은 입력 코드와 출력 지표를 모두 텍스트로 표현하여 인코더-디코더 구조로 구성되었으며, 수치 예측을 토큰 단위 수치 토크나이제이션으로 처리한다.  
다중 태스크 및 다중 목표 학습이 가능하여 다양한 코드 유형과 지표를 한 모델에서 처리할 수 있고, 사전학습과 미세조정을 통한 효율적인 적응이 가능하다.  
디코더의 자기회귀적 특성으로 지연 시간과 정확도 같은 상호 연관된 지표 간의 조건부 예측도 수행할 수 있다.  

## Results  
RLM은 APPS 데이터셋에서 0.9 이상의 스피어만 상관계수를 기록하며, CodeNet의 17개 언어 평균에서 0.5 이상의 성능을 보였고, NAS 벤치마크에서는 기존 그래프 신경망 기반 기법을 상회하는 평균 켄달 타우 0.46을 달성하였다.  

## Limitations  
모델 크기가 커질수록 성능 향상이 관찰되나, 대규모 모델의 하이퍼파라미터 튜닝과 계산 비용 문제로 인해 확장성 연구가 제한적이었다.  

## Conclusion  
텍스트 기반 회귀 언어 모델은 복잡한 특징 추출 과정 없이 다양한 프로그래밍 언어 및 코드 표현을 대상으로 정량적 실행 지표를 정확히 예측할 수 있으며, 향후 자동화된 머신러닝 및 컴퓨터 아키텍처 분야에서 폭넓게 활용될 가능성이 있다.

# 7. [TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics](https://arxiv.org/abs/2509.26329)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26329)

## Introduction
- Goal: 본 논문은 지역사회에서 즉시 인지할 수 있으나 외부인은 인식하기 어려운 문화적 비언어 음향을 이해하는 대규모 오디오-언어 모델의 한계를 평가하는 벤치마크인 TAU(Taiwan Audio Understanding)를 제안하는 것이다.  
- Motivation: 기존 대규모 오디오-언어 모델 평가가 전 세계적으로 흔한 음향 및 언어 의미 중심으로 이루어져 지역적·문화적 특성을 반영하지 못하는 문제점이 존재한다.  
- Contribution: TAU는 대만 일상에서 지역적으로 특색 있는 702개의 음원과 1,794개의 비의미적 선택형 문제를 포함하는 데이터셋을 구축하여 문화적 음향 해석능력의 격차를 탐색하고, 인간 대비 모델 성능 저하를 보임으로써 지역화 평가의 필요성을 규명하였다.  

## Method  
TAU 구축은 지역적 식별이 가능한 대만의 소리표식(soundmarks)을 전문가 큐레이션과 인간 편집, 대형언어모델(LLM)을 활용한 문항 생성 과정으로 진행되었다.  
데이터는 라이선스가 허용된 공개 소스 및 자체 녹음본에서 수집되어, 음운적 의미 정보 없이 음향 특성만으로 문제 해결이 가능하도록 출제되었다.  
추가로 ASR과 LLM을 이용해 텍스트 정보만으로 풀 수 있는 문제를 제거하는 자동 필터링 절차를 거쳤다.  

## Results  
최첨단 대규모 오디오-언어 모델 Gemini 2.5 Pro가 최고 성능을 보였으나 인간(84% 수준)에 크게 미치지 못하는 70~74% 정확도를 기록하였으며, 지역 특화 프롬프트 적용 시 일부 모델은 개선되었으나 전반적인 격차는 여전하였다.  

## Limitations  
TAU는 대만 특유의 도시 중심 환경과 제한된 음향 유형에 편중되어 있으며, 음향 환경 변화에 따른 데이터 분포 변화 문제 및 다른 지역 일반화 가능성에 한계가 존재한다.  

## Conclusion  
문화적 음향 이해를 평가하는 TAU 벤치마크는 현 대규모 오디오-언어 모델들이 지역적 문화 배경 지식을 통합하는 데 한계가 있음을 보여주어, 향후 지역 맞춤형 데이터와 평가 지표 개발의 중요성을 강조한다.

# 8. [EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series   Forecasting](https://arxiv.org/abs/2509.26157)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26157)

## Introduction
- 본 연구의 목표는 시계열 데이터의 시간적 연속성을 보존하면서 효율적이고 정확한 예측을 가능하게 하는 동적 패치 인코더인 EntroPE를 제안하는 것이다.  
- 기존의 고정 길이, 임의 경계 기반 패치 기법은 시간적 일관성을 훼손하고 학습 시와 추론 시 패치 분할 차이로 인해 예측 성능 저하 문제가 존재한다.  
- 본 논문은 조건부 엔트로피를 활용하여 자연스러운 시간적 전환점을 탐지하고, 이를 바탕으로 동적으로 패치 경계를 설정하는 새로운 정보 이론적 기반 패칭 방식을 제안한다.  

## Method  
- EntroPE는 경량 causal transformer로 시계열 예측 불확실성을 엔트로피로 측정해 패치 경계를 동적으로 결정하는 Entropy-based Dynamic Patcher(EDP)를 포함한다.  
- Adaptive Patch Encoder(APE)는 가변 길이 패치를 풀링 및 크로스 어텐션을 통해 고정 크기 임베딩으로 변환해 시계열 내 의존성을 보존한다.  
- 이후 전역 트랜스포머가 패치 간 장기 의존성을 학습하고, Fusion Decoder가 전역 문맥과 세부 시계열 정보를 결합해 정확한 예측을 수행한다.  

## Results  
- EntroPE는 7개 장기 다변량 시계열 예측 벤치마크에서 다양한 최신 모델 대비 최대 약 20% 정확도 향상과 효율성 개선을 달성하였다.  

## Limitations  
- 정보 부족  

## Conclusion  
- 본 연구는 엔트로피 기반 동적 패치 경계 탐지 및 적응형 인코딩을 통해 시계열 구조를 효과적으로 반영하면서 예측 성능과 효율성을 동시에 향상시키는 새로운 패치 인코딩 패러다임을 제시하였다.

# 9. [d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.23094)

## Introduction
- Goal: 본 논문은 확산 기반 대형 언어 모델(dLLMs)의 추론 속도를 개선하기 위한 훈련 불필요한 근사 키-값 캐시 프레임워크인 Dual aDaptive Cache(d2Cache)를 제안하는 것이다.  
- Motivation: dLLMs는 양방향 어텐션을 사용하여 기존의 오토리그레시브 모델이 활용하는 표준 키-값 캐시를 직접적으로 이용할 수 없어 추론 효율성이 낮다는 문제를 가진다.  
- Contribution: d2Cache는 두 단계의 미세 조정된 토큰 선택 전략을 통해 각 디코딩 단계마다 핵심 토큰의 키-값 상태를 갱신하고 나머지 토큰은 캐시하여, 효율적인 추론 가속과 품질 향상을 동시에 달성하였다.  

## Method  
d2Cache는 마스킹된 토큰의 빠른 변화 단계에서 키-값 상태를 갱신하고, 나머지 단계에서는 상태를 재사용하는 세 단계 동역학을 분석하였다. 이어서, 주위에 알려진 토큰 밀도와 예측 확신도를 곱한 '확신 우선도'를 기반으로 마스킹된 토큰을 선택하며, 나머지 토큰들에 대해서는 어텐션 롤아웃 기법으로 영향력이 큰 토큰만 갱신한다. 이러한 이중 선택 과정으로 불필요한 계산을 줄이면서도 성능 저하를 최소화하였다.  

## Results  
LLaDA와 Dream 두 dLLM 모델에서 d2Cache는 기존 방법 대비 최대 4.7배 추론 속도 개선과 일관된 생성 품질 향상을 동시에 달성하였다.  

## Limitations  
본 연구는 미세 단위 토큰 선택 전략과 하이퍼파라미터 설정에 따른 튜닝 필요성에 대한 제한점이 존재한다.  

## Conclusion  
d2Cache는 dLLMs의 키-값 상태 변화 특성과 어텐션 분포를 활용한 두 단계 적응형 캐시 기법으로, 실용적인 추론 가속과 향상된 생성 품질을 달성하는 효과적인 방법임을 입증하였다.

# 10. [Context Is What You Need: The Maximum Effective Context Window for Real   World Limits of LLMs](https://arxiv.org/abs/2509.21361)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21361)

## Introduction
- Goal: 본 논문은 대형 언어 모델(LLM)의 최대 맥락 창 크기와 실제 효과적 맥락 창 크기(MECW)의 차이를 규명하고, 이를 측정하기 위한 표준화된 테스트 방법론을 제안하는 데 목적이 있다.  
- Motivation: LLM 제공자들이 제시하는 최대 맥락 창 크기(MCW)가 실제 모델의 유효 맥락 용량과 크게 다르며, 이로 인해 실사용 환경에서 성능 저하 및 잘못된 기대가 발생하는 문제를 해결하기 위함이다.  
- Contribution: MECW 정의, 다양한 모델 및 문제 유형에 따른 MECW 실험 분석 결과, 그리고 이를 기반으로 한 모델 설계 및 활용 가이드라인을 제시하였다.

## Method  
본 연구는 무작위화된 데이터셋과 네 가지 문제 유형(단일 정보 검색, 다중 정보 검색 및 합산, 요약, 정렬 후 다중 검색)에 대해 11개 최첨단 LLM을 API로 호출해 입력 토큰 수를 점진적으로 증가시키며 수행하였다.  
66,000건 이상의 데이터 포인트를 수집하여 각 모델의 성능 변화와 정답률을 통계적으로 검증하였다.  
토큰 수 구간별 정답률 변화를 분석해 MECW의 수치화 및 문제 유형별 차이를 평가하였다.

## Results  
실험 결과 모든 모델에서 MCW 대비 MECW가 크게 낮았으며, 문제 유형에 따라 MECW가 달라진다는 사실과 1000토큰 내외부터 성능 저하 및 환각 발생률 증가가 뚜렷하게 관찰되었다.

## Limitations  
단일 변수인 토큰 수만 고려하였고 실제 복잡한 문제, 다양한 데이터 형식 및 프롬프트 기법에 따른 MECW 영향은 별도로 분석하지 않았다.

## Conclusion  
최대 맥락 창과 실제 효과적 맥락 창은 모델과 문제 유형에 따라 크게 달라지며, MECW에 기반한 맥락 관리가 LLM의 정확도 향상과 환각 감소에 가장 중요한 요소임을 확인하였다.

# 11. [Swift: An Autoregressive Consistency Model for Efficient Weather   Forecasting](https://arxiv.org/abs/2509.25631)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.25631)

## Introduction
- 본 연구의 목표는 기존 확산 모델 대비 계산 효율성을 획기적으로 개선한 자기회귀 일관성 모델인 Swift를 제안하여 중기에서 계절 간격에 이르는 기상 예측을 수행하는 것이다.  
- 동기부여는 확산 모델이 물리적으로 안정적인 확률적 예측을 가능케 하지만 반복적 추론 과정에서 과도한 계산량이 요구되어 장기 예측에 부적합하다는 점이다.  
- 본 연구에서는 for the first time, CRPS 목표함수를 이용한 확률 흐름 모델의 자기회귀 파인튜닝을 단일 단계에서 가능케 하여 다중 모델 앙상블이나 파라미터 변동 없이도 안정적이고 숙련된 예측을 제공함을 밝혔다.  

## Method  
Swift는 트리고노메트릭 시간 매개변수를 사용하는 TrigFlow 변수를 기반으로 하는 확산 모델과 일관성 모델을 동시에 사전학습한 후, CRPS 손실함수를 통한 다단계 자기회귀 파인튜닝으로 최종 모델을 완성한다.  
확률적 예측의 불확실성을 정량화하는 CRPS를 이용하여 앙상블의 교정성을 보장하며, 효율적인 스윈 트랜스포머 기반 네트워크 아키텍처를 활용하여 1회 평가로 예측을 생성한다.  
6시간 단위 간격의 동적 시간 구간을 도입함으로써 일중 및 시노프틱 규모 변화까지 포착하여 시간적 충실도를 유지한다.  

## Results  
Swift는 기존 최고 확산 모델 대비 최대 39배 빠른 추론 속도를 보이면서, 운영 중인 수치모델 IFS ENS와 경쟁력 있는 예측 능력을 중·장기(최대 75일) 예측에서 지속적으로 나타냈다.  

## Limitations  
본 연구에서는 계산 및 저장 제약으로 인해 초기 조건과 앙상블 수가 제한적이었으며, 결과적으로 예측이 다소 언더디스퍼스되는 한계가 존재한다.  

## Conclusion  
Swift는 중기부터 계절 간격까지 안정적이고 숙련된 확률적 기상예측을 가능케 하여 기계학습 기반 앙상블 예측의 효율성과 신뢰성 측면에서 중요한 발전을 나타낸다.

# 12. [LayerD: Decomposing Raster Graphic Designs into Layers](https://arxiv.org/abs/2509.25134)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.25134)

## Introduction
- Goal: 본 연구는 래스터 그래픽 디자인 이미지를 재편집 가능한 레이어로 분해하는 자동화된 방법인 LayerD를 제안하는 것이다.  
- Motivation: 그래픽 디자인은 기본적으로 레이어 단위로 제작되고 편집되지만 최종적으로 래스터 이미지로 합성되면 레이어 정보가 소실되어 재편집이 어렵다.  
- Contribution: LayerD는 반복적으로 전경 레이어를 추출하고 도메인 특성을 활용한 정제 기법을 적용하여 기존 방법보다 높은 품질의 레이어 분해를 달성하고, 불확실한 정답 레이어 구조에 대응한 정량 평가 방법도 제안한다.  

## Method  
레이어 분해를 전경 최상위 레이어 추출과 배경 완성 작업으로 반복 수행하며, top-layer matting 모델과 배경 인페인팅 모델을 이용한다.  
디자인 도메인 특성인 평평하고 단일 색상 영역을 가정한 색상팔레트 기반 전경 및 배경 정제 기법으로 분해 품질을 향상시킨다.  
레이어 간 알파 블렌딩의 역연산을 통해 전경의 색상도 정확히 추정한다.  

## Results  
Crello 데이터셋 기반 실험에서 LayerD는 여러 베이스라인 대비 정성적·정량적으로 우수한 레이어 분해 품질을 보였으며, 생성 이미지를 포함한 다양한 그래픽 디자인 편집에 활용 가능함을 보였다.  

## Limitations  
레이어 분해는 본질적으로 해석이 모호한 문제로, 여러 올바른 해답이 존재하며 평가를 위한 정답 레이어 간 일관성이 낮은 점은 제한점으로 남았다.  

## Conclusion  
LayerD는 래스터 그래픽 디자인을 효과적으로 레이어 단위로 분해하여 편집 가능성을 확장하였으며, 향후 벡터화 및 자동 설계 생성 등 다양한 응용 가능성이 기대된다.

# 13. [LLM Watermark Evasion via Bias Inversion](https://arxiv.org/abs/2509.23019)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.23019)

## Introduction
- Goal: 본 연구는 대형 언어 모델(LLM)에서 적용되는 워터마크의 취약성을 분석하고, 이를 회피하는 새로운 공격 기법을 제안하는 것이다.  
- Motivation: 기존 워터마크 기법은 정상 환경에서의 효과가 입증되었으나 적대적 회피 공격에 대한 내구성이 충분히 검증되지 않았다.  
- Contribution: 이론적 분석에 기반하여 워터마크 신호를 약화시키는 Bias-Inversion Rewriting Attack (BIRA)을 제안하고, 다양한 최신 워터마크 기법에 대해 99% 이상의 공격 성공률을 보임을 실험적으로 입증하였다.  

## Method  
BIRA는 워터마크가 적용된 텍스트의 특정 고확률 '초록토큰' 집합을 사전 지식 없이 추정하고, 이들의 로짓 값에 음의 편향을 적용하여 재작성한다. 이 과정에서 언어 모델의 자기정보량을 활용하여 고엔트로피 토큰을 식별하며, 텍스트 의미를 보존하면서 워터마크 신호를 효과적으로 제거한다. 어댑티브 편향 조절 기법을 통해 텍스트 반복 문제를 최소화하며, 공격 성공률과 의미 보존 간 균형을 유지한다.  

## Results  
BIRA는 Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini 등 여러 모델과 KGW, Unigram, SIR 등 7개의 최신 워터마킹 기법에 대해 기존 공격법 대비 월등히 높은 99% 이상의 공격 성공률과 의미 보존 능력을 동시 달성하였다.  

## Limitations  
본 연구는 워터마크 알고리즘의 내부 정보를 전혀 알지 못하는 블랙박스 환경을 가정하였으나, 일부 극단적 편향 적용 시 텍스트 의미가 다소 훼손될 수 있다.  

## Conclusion  
BIRA는 LLM 워터마크의 본질적 취약성을 이론적으로 규명하고 실험적으로 검증하여, 워터마크의 신뢰성과 방어 전략 마련의 필요성을 강조하였다.

# 14. [ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency   Estimation](https://arxiv.org/abs/2509.26278)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.26278)

## Introduction
- 목표: ProfVLM이라는 경량화된 비디오-언어 모델을 통해 다중 시점에서 수행 능력 평가와 전문적 피드백 생성을 동시에 수행하는 방법을 제안하는 것이다.  
- 동기: 기존 수행 능력 평가는 주로 단일 시점 분류 방식과 점수 산출에 의존해 설명 가능성이 낮고 다중 시점 정보를 효과적으로 활용하지 못하는 한계가 존재하였다.  
- 기여: 본 논문은 시공간 트랜스포머 기반 영상 인코더와 새로운 AttentiveGatedProjector를 활용하여 다중 시점 특징을 언어 모델 입력 공간에 투사하고, 수행 등급 분류와 자연어 피드백 생성을 통합하는 경량 비전-언어 구조를 제시하였다.  

## Method  
ProfVLM은 사전학습된 TimeSformer 백본을 고정하여 비디오에서 시공간 특징을 추출하고, AttentiveGatedProjector 구조로 다중 시점 영상 특징을 어텐션 및 게이트 기법으로 융합한 뒤, LoRA로 미세 조정된 SmolLM2 언어 모델에 맞춰 정규화 및 투사한다.  
입력으로 8프레임의 동기화된 1인칭 및 최대 4개의 3인칭 시점을 사용하며, 모델은 분리된 분류 헤드 없이 자연어 생성 과정에서 수행 등급과 피드백을 동시에 산출한다.  
모델 훈련은 챗 기반 프롬프트 구조를 활용하여 텍스트와 레이블 정보를 통합 학습하였다.  

## Results  
ProfVLM은 EgoExo4D 데이터셋의 다중 시점 평가에서 기존 최첨단 모델 대비 최대 20배 적은 파라미터와 60% 단축된 학습 시간으로 정확도 48.2%를 달성하며, 다섯 개 도메인에서 오차율을 낮추고 자연어 피드백의 의미적 유사도를 높였다.  

## Limitations  
농구 시나리오에서는 공간적, 시간적 해상도의 부족과 게이트 어텐션의 전역적 처리 한계로 인해 일부 전통적 방법에 비해 성능이 떨어지는 문제점이 확인되었다.  

## Conclusion  
ProfVLM은 경량화된 비전-언어 모델을 활용해 다중 시점 수행 능력 평가와 전문가 수준 피드백 생성을 통합함으로써, 기존 분류 중심 평가 방식을 넘어선 해석 가능하고 효율적인 새로운 평가 패러다임을 제시하였다.

# 15. [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.18538)

## Introduction
- 본 연구는 이미지 내 목표 객체와 그에 기인한 시각적 인공물(그림자, 반사 등)을 제거하는 지오메트리 인지 객체 제거 방법을 제안한다.  
- 기존 엄격한 마스크 정렬 학습법은 인공물 제거에 실패하고, 느슨한 정렬법은 제어력 저하 및 의도치 않은 영역 삭제 문제를 야기하는 한계를 극복하고자 하였다.  
- 객체의 기하학적 존재와 시각적 인과 효과 간 인과관계를 모델링하는 두 단계 프레임워크를 개발하여 기하학적 제거와 외관 렌더링을 분리하였다.  

## Method  
입력 이미지의 깊이 맵을 추정한 후, 엄격한 마스크 감독 하에 확산 모델로 기하학적 제거를 수행하여 객체를 제거하고 불필요한 구조 삽입을 방지하기 위해 선호도 기반 손실을 도입하였다.  
이어, 수정된 깊이 정보를 조건으로 하여 객체와 연관된 인과적 시각 효과가 암묵적으로 제거된 사실적 RGB 이미지를 생성하는 외관 렌더링 단계를 수행하였다.  
렌더링 단계는 깊이 맵 변화를 반영하는 조건부 이미지 변환 문제로 정의하였으며, 객체 제거 및 삽입을 상호 보완적으로 학습하였다.  

## Results  
RemovalBench와 RORD-Val 벤치마크에서 기존 최첨단 기법 대비 FID, CMMD, LPIPS, PSNR 등 모든 주요 지표에서 우수한 성능을 보이며 객체와 인과적 시각적 인공물을 효과적으로 제거하였다.  

## Limitations  
동작 흐림, 투명 및 반사 표면, 폐색 및 저질감 조건 등에서 기하학적 제거 단계가 실패할 경우 전체 제거 과정이 부정확해지는 제약이 존재하였다.  

## Conclusion  
본 연구는 기하학적 제거와 외관 렌더링을 분리한 두 단계 방식으로 객체와 그 인과적 시각 효과를 동시에 제거하며, 다양한 벤치마크에서 시각 및 구조적 품질 향상을 입증하였다.
