---
layout: post
title: "Daily Papers — 2025-10-28"
date: 2025-10-28 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [FARMER: Flow AutoRegressive Transformer over Pixels](https://arxiv.org/abs/2510.23588)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23588)

## Introduction
- Goal: 본 논문은 Normalizing Flows와 Autoregressive 모델을 통합한 FARMER라는 새로운 생성 프레임워크를 통해 원시 픽셀로부터 직접 고품질 이미지 합성 및 추론 가능한 확률 밀도 추정을 목표로 한다.  
- Motivation: 기존 연속적 픽셀 기반 Autoregressive 모델은 고차원 공간 및 긴 시퀀스로 인해 학습과 샘플링이 비효율적이며, Normalizing Flows는 표준 가우시안 분포와 원본 데이터 분포 간 차이로 샘플 품질이 저하되는 한계를 가진다.  
- Contribution: FARMER는 가역적 Autoregressive Flow를 사용하여 이미지를 잠재 시퀀스로 변환하고, 해당 잠재 분포를 Autoregressive 모델로 암묵적으로 모델링하며, 자기지도 차원 축소와 원스텝 증류, 재샘플링 기반 무분류 지침법을 제안하여 효율성과 이미지 생성 품질을 향상시킨다.  

## Method  
FARMER는 Autoregressive Flow(AFlow)와 Autoregressive Transformer(AR) 모델을 결합하여 이미지 데이터를 잠재 토큰으로 변환하고, 자기지도 차원 축소를 통해 잠재 채널을 정보 채널과 중복 채널로 분리하여 효율적 모델링을 가능하게 한다. 또한, 원스텝 증류를 통해 느린 AFlow 역전파 과정을 가속화하며, 재샘플링 기반 무분류 지침법으로 생성 품질을 극대화한다. 최종적으로, FARMER는 end-to-end 방식으로 확률밀도함수를 최적화한다.

## Results  
ImageNet 256×256 클래스 조건부 생성 실험에서 FARMER는 기존 유사 기법 대비 FID 점수를 현저히 개선하였으며, 원-스텝 증류를 통해 역전파 속도를 22배 가속화하는 동시에 경쟁력 있는 이미지 품질을 달성하였다.  

## Limitations  
Autoregressive Flow의 순차적 역전파 과정은 본질적으로 느리며 이를 완벽히 해소하기 위해 추가 학습 및 증류 단계가 필요하다.  

## Conclusion  
FARMER는 Normalizing Flows와 Autoregressive 모델의 장점을 효과적으로 결합하여 고차원 연속 영상 데이터에 대해 정확한 우도 추정과 고품질 이미지를 생성하는 새로운 종단간 프레임워크임이 입증되었다.

# 2. [Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human   Animation](https://arxiv.org/abs/2510.23581)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23581)

## Introduction
- Goal: 본 연구는 오디오 기반 인간 애니메이션 생성에서 캐릭터 정체성의 점진적 변화를 방지하며 자연스러운 장시간 영상 생성을 목표로 한다.  
- Motivation: 기존 시간 순차적 생성법에서는 누적된 오류로 인해 캐릭터의 정체성이 서서히 손실되고, 키프레임 기반 강화법은 별도의 키프레임 생성 단계와 표현력 제한 문제를 가진다.  
- Contribution: 키프레임을 생성 구간 내가 아닌 미래 시점에 위치시키는 새로운 Lookahead Anchoring 기법을 제안하여, 별도의 키프레임 생성 없이 정체성을 유지하며 표현 가능한 장시간 애니메이션 생성을 가능하게 하였다.  

## Method  
Lookahead Anchoring은 현재 생성 구간보다 일정 거리 떨어진 미래 프레임의 키프레임을 조건으로 사용하여 캐릭터 정체성 유지와 자연스러운 표현의 균형을 조절하며, 미래 키프레임은 강제적 경계가 아닌 방향성 안내자로 작용한다. 이로 인해 사전 키프레임 생성 과정이 불필요해지며 참조 이미지 자체를 미래 키프레임으로 활용하는 self-keyframing 전략을 도입하였다. 또한, 이러한 방식을 기존 Diffusion Transformer 기반 오디오 구동 애니메이션 모델에 효과적으로 통합하기 위한 미세조정 기법을 개발하였다.  

## Results  
본 기법은 세 가지 최신 음성 기반 인간 애니메이션 모델에 적용 시, 장시간 생성에서도 입 모양 동기화, 캐릭터 정체성 유지, 시각적 품질 모두에서 기존 방법들보다 우수한 성능을 보였다.  

## Limitations  
키프레임과 생성 구간 간의 시간 거리에 따른 표현력과 정체성 유지 간의 트레이드오프가 존재하며, 지나치게 먼 거리에서는 동기화 정확도가 감소하는 한계가 있다.  

## Conclusion  
Lookahead Anchoring은 키프레임의 역할을 강제적 제약에서 방향성 안내로 전환하여 장시간 오디오 기반 인간 애니메이션에서 일관된 캐릭터 정체성을 유지하면서 자연스러운 동작을 생성하는 실용적이고 효과적인 접근법임을 입증하였다.

# 3. [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23052)

## Introduction
- Goal: 본 논문은 다중 헤드 어텐션의 각 헤드 간 상호작용을 강화하여 표현력을 개선하는 새로운 어텐션 메커니즘인 Knocking-Heads Attention(KHA)를 제안하는 것을 목표로 한다.  
- Motivation: 기존의 다중 헤드 어텐션은 독립적으로 작동하는 헤드들의 출력을 단순 연결하는 방식으로, 헤드 간 긴밀한 상호작용이 부족하고, 헤드 수 증가 시 개별 헤드의 표현력이 저하되는 문제를 갖는다.  
- Contribution: 대각선 초기화를 적용한 공유 변환 행렬을 통해 헤드 간 피처 수준 상호작용을 가능케 하며, 최소한의 추가 비용으로 다양한 어텐션 변형에 적용 가능한 KHA를 제안하고, 1조 개 토큰 규모 대형 MoE 모델 실험에서 성능 및 학습 안정성이 우수함을 입증하였다.  

## Method  
KHA는 기존의 헤드별 쿼리, 키, 값 투영 이후에 모든 헤드에 공유하는 변환 행렬(또는 MLP)을 적용하여 헤드 간 정보 교환을 수행한다. 공유 변환 행렬은 대각선 초기화되어 초기에는 각 헤드 고유의 특성을 유지하며, 학습 과정에서 점진적으로 통합 표현을 학습하게 한다. 이 기법은 쿼리와 키에 적용하는 변환은 선택적이며, 값(value) 투영에 MLP 기반 변환을 적용할 때 가장 큰 성능 향상을 보인다.

## Results  
1조 개 토큰으로 6.1B 파라미터 MoE 모델을 미세조정한 결과, KHA는 언어 이해, 코드 생성, 수학 문제 해결 과제에서 각각 4.32, 3.90, 1.62점의 평균 성능 향상을 기록하고, 전체 과제 평균 점수는 1.26점 상승하며 학습 손실 스파이크를 효과적으로 감소시켰다.

## Limitations  
KHA는 값(value) 투영에서 가장 큰 이점을 보이나 쿼리와 키 투영에 대한 추가 변환은 성능 개선이 미미하여 하위 구성 요소별 효과 차이가 존재한다.

## Conclusion  
KHA는 최소한의 계산 비용으로 다중 헤드 어텐션 내 헤드 간 유의미한 상호작용과 학습 안정성을 확보하며, 트랜스포머 아키텍처에 실용적인 성능 향상을 제공하는 혁신적인 방법이다.

# 4. [LightBagel: A Light-weighted, Double Fusion Framework for Unified   Multimodal Understanding and Generation](https://arxiv.org/abs/2510.22946)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.22946)

## Introduction
- Goal: 본 논문은 효율적인 통합 멀티모달 이해 및 생성 모델인 LIGHTBAGEL을 제안하는 것을 목표로 한다.  
- Motivation: 기존 통합 멀티모달 모델들은 대규모 연산 자원과 긴 학습 시간이 요구되며, 공개된 모델들의 효율적 결합을 통한 자원 절감이 필요한 상황이다.  
- Contribution: LIGHTBAGEL은 사전 학습된 이해 및 생성 특화 모델을 유지하면서 이중 융합(Double Fusion) 구조로 깊고 연속적인 멀티모달 상호작용을 가능하게 하여 데이터 및 연산 효율성을 획기적으로 개선하였다.  

## Method  
LIGHTBAGEL은 QWen2.5-VL-7B 기반 이해 경로와 Wan2.2-TI2V-5B 기반 생성 경로를 각각 사용하며, 두 경로의 각 레이어 사이에 0으로 초기화된 멀티모달 셀프 어텐션 모듈을 삽입하여 이중 융합을 구현한다. 이 구조는 고수준 의미 정보(ViT 토큰)와 저수준 공간 정보(VAE 토큰)의 효과적 통합을 통해 이미지 이해 및 생성 모두에서 우수한 성능을 도출한다. 학습 데이터는 공개된 텍스트-이미지 및 이미지 편집 데이터와 자체 합성 데이터를 포함해 약 4500만 샘플로 구성된다.  

## Results  
LIGHTBAGEL은 약 350억 토큰 학습만으로 GenEval 0.91, DPG-Bench 82.16, GEditBench 6.06, ImgEdit-Bench 3.77 점수를 획득하며, UniPiC, OmniGen2 등 대규모 모델 대비 우수하거나 동등한 성능과 함께 토큰 효율성을 크게 향상시켰다.  

## Limitations  
LIGHTBAGEL은 이해 경로를 학습 중 동결하여 이해 성능은 유지하였으나, 전체 모델 통합 최적화에 따른 추가 개선 가능성은 남아있다.  

## Conclusion  
LIGHTBAGEL의 이중 융합 설계는 강력한 멀티모달 상호작용과 효과적인 정보 통합을 가능하게 하여, 적은 학습 데이터와 자원으로 통합 멀티모달 모델의 최첨단 성능을 달성하는 새로운 아키텍처 방향을 제시한다.

# 5. [Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models   with Conditional Score Distillation](https://arxiv.org/abs/2510.21003)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.21003)

## Introduction
- Goal: 본 연구의 목표는 이미지 오토리그레시브(AR) 모델에서 단일 단계(one-step) 샘플링을 가능하게 하는 새로운 증류 디코딩 방식을 제안하는 것이다.  
- Motivation: 기존 Distilled Decoding 1(DD1)은 단일 단계 샘플링 시 성능 저하가 크고 사전 정의된 매핑에 의존해 유연성이 부족한 한계가 존재한다.  
- Contribution: 본 연구에서는 조건부 스코어 증류(Conditional Score Distillation, CSD) 손실을 도입해 사전 매핑에 의존하지 않고 AR 모델을 교사 모델로 활용하는 Distilled Decoding 2(DD2)를 제안하였다.  

## Method  
본 방법은 AR 모델을 잠재 임베딩 공간에서 각 토큰 위치별로 조건부 스코어를 제공하는 교사 모델로 재해석하며, 생성자와 조건부 가이드 네트워크를 함께 학습하여 조건부 스코어 정렬 손실을 최소화한다.  
이를 통해 생성자의 분포가 원본 AR 모델 분포와 일치하도록 하여 한 번의 연산으로 전체 토큰 시퀀스를 생성하도록 한다.  
또한 생성자와 가이드 네트워크의 안정적 학습을 위해 AR-디퓨전 모델 초기화 전략을 도입하였다.  

## Results  
ImageNet-256 데이터셋에서 VAR 및 LlamaGen 모델 기반으로 DD2는 1단계 샘플링 시 DD1보다 최대 67% 성능 격차를 줄이고, 최대 238배의 샘플링 속도 증가와 12.3배의 학습 효율 향상을 달성하였다.  

## Limitations  
DD2는 원본 AR 모델 대비 빠른 생성속도를 보이나 여전히 일정 수준의 성능 저하가 존재하며, 완전한 성능 일치는 향후 과제로 남아있다.  

## Conclusion  
DD2는 이미지 AR 모델의 단일 단계 생성 가능성을 크게 진전시켜 빠르면서도 고품질의 AR 이미지 생성에 새로운 가능성을 제시한다.

# 6. [PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error   Detection](https://arxiv.org/abs/2510.23594)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23594)

## Introduction
- Goal: 본 논문은 다단계 퍼즐 기반 시각 과제를 통해 다중모달 대형언어모델(MLLM)의 시각 추론 및 추론 오류 탐지 능력을 평가하는 벤치마크 PRISM-Bench를 제안한다.  
- Motivation: 기존 벤치마크가 최종 정답의 정확도만을 측정하여 모델의 추론 과정 신뢰성을 평가하는 데 한계가 있기 때문이다.  
- Contribution: PRISM-Bench는 총 1044개의 시각 퍼즐과 정답 추론 과정을 오류가 포함된 체인 오브 생각(CoT)과 함께 제공하여, 최종 정답 예측과 함께 첫 오류 발생 단계 탐지를 요구하는 이중 평가 프로토콜을 제시한다.  

## Method  
PRISM-Bench는 6개 유형의 퍼즐 문제로 구성되며, 각 문제는 이미지, 질문, 정답, 정답 추론 CoT 그리고 한 단계에서 인위적인 오류가 삽입된 변형된 CoT를 포함한다.  
오류 삽입은 24가지 유형 중 하나를 무작위로 선택해 특정 추론 단계를 변형하고 이후 단계를 재생성하되 오류는 하나만 포함되도록 한다.  
평가 방식은 (A) 최종 정답 예측 정확도와 (B) 오류가 포함된 CoT에서 첫 번째 오류 단계 위치 탐지 정확도로 나뉘어, 모델의 문제 해결 및 추론 검증 능력을 분리하여 측정한다.  

## Results  
최신 MLLM 평가 결과, 정답 예측과 오류 탐지는 상관관계가 약하며, 최고 성능 모델들도 오류 위치 탐지에서 약 50% 정확도를 기록하여 완전하지 않은 추론 검증 능력을 드러냈다.  

## Limitations  
PRISM-Bench는 인위적으로 삽입된 단일 오류만 탐지하는 방식을 채택하여, 실제 복합적 오류 환경에서의 모델 성능은 추가 연구가 필요하다.  

## Conclusion  
PRISM-Bench는 시각 퍼즐 기반의 체계적이고 진단적인 MLLM 추론 평가를 가능하게 하여, 기존 최종 정답 중심 평가의 한계를 극복하고 신뢰할 수 있는 다중모달 추론 모델 개발에 기여한다.

# 7. [EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion   Personalization](https://arxiv.org/abs/2510.20512)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.20512)

## Introduction
- Goal: 본 연구는 텍스트-이미지(diffusion) 모델의 개인화에서 빠른 생성이 가능하도록 1단계 확산(personalization) 문제인 1-SDP를 해결하는 것이다.  
- Motivation: 기존의 다단계 확산 모델 기반 개인화 방법들은 1단계 모델에 적용 시 학생 모델의 개념 적응력 부족, 효율성 저하, 교사 모델의 신뢰성 문제로 인해 성능 저하가 발생한다.  
- Contribution: 본 논문은 교사(다단계 확산)와 학생(1단계 확산) 모델을 공동으로 훈련시키고 상호 지식을 교환하는 양방향 증폭(distillation) 프레임워크 EchoDistill을 제안하였다.  

## Method  
EchoDistill은 교사와 학생 모델 간에 텍스트 인코더를 공유하여 의미론적 정합성을 확보하고, 정렬 및 적대적 손실을 활용한 종단 간 공동학습 방식을 도입한다. 학생 모델의 빠른 생성 능력을 활용해 교사 모델 학습을 보완하는 에코잉 단계를 포함한다. 이는 1단계 개인화에서 학생의 개념 적응력을 높이고 교사 성능도 함께 개선시킨다.  

## Results  
DreamBench 벤치마크 실험에서 EchoDistill은 기존 1단계 및 다단계 개인화 방법 대비 생성된 이미지의 개념 정합성과 품질 측면에서 우수한 성능을 나타냈다.  

## Limitations  
본 연구는 1단계 확산 모델 중 SDTurbo 기반에 주로 최적화되어, 다른 1단계 백본에 대해서는 성능 차이가 존재한다.  

## Conclusion  
EchoDistill은 1-SDP 문제를 최초로 정의하고 해결한 방법으로, 빠른 생성 속도를 유지하면서도 효과적인 개념 개인화를 가능하게 하는 새로운 패러다임을 제시하였다.

# 8. [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view   Human Reconstruction](https://arxiv.org/abs/2510.07723)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.07723)

## Introduction
- Goal: 본 연구는 단일 이미지로부터 고품질의 의복을 착용한 전신 3D 인간 메쉬를 재구성하는 방법을 제안하는 것이다.  
- Motivation: 기존의 SMPL 기반 방법들은 부정확한 3D 형상 정보와 복잡한 자세 및 미세한 디테일 재구성에 한계가 존재한다.  
- Contribution: 본 논문에서는 2D 다중 시점 생성 모델과 3D 네이티브 생성 모델을 동기화하는 새로운 프레임워크인 SyncHuman을 제시한다.  

## Method  
SyncHuman은 2D 다중 시점 생성 분포와 3D 희소 구조 생성 분포를 2D-3D 동기화 어텐션을 통해 정합시키며, 이를 통해 구조적 일관성과 고해상도 디테일을 동시에 달성한다. 또한, 생성된 다중 시점 이미지로부터 특징을 주입하는 다중 시점 안내 디코더를 도입하여 3D 메쉬의 질감과 기하학적 정확도를 향상시킨다. 모델은 flow matching 손실 함수로 공동 학습되어 단일 이미지에서 복잡한 인간 자세도 안정적으로 복원할 수 있다.  

## Results  
SyncHuman은 여러 공개 데이터셋에서 기존 최첨단 방법 대비 기하학적 정확도와 시각적 충실도 모두에서 우수한 성능을 보였다.  

## Limitations  
학습 데이터의 제한으로 균일 조명 환경에 최적화되어 있어 비균일 조명 조건에서는 텍스처 부정합 현상이 발생할 수 있다.  

## Conclusion  
본 연구는 2D 및 3D 생성 모델의 동기화 학습을 통해 단일 시점 이미지에서 사실적인 전신 3D 인간 재구성을 성공적으로 수행함을 입증하였다.

# 9. [Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with   Progressive Texture Infilling](https://arxiv.org/abs/2510.23605)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.23605)

## Introduction
- Goal: 본 논문은 특정 인물이나 객체의 정체성을 보존하며 개인 맞춤형 3D 및 4D 자산을 생성하는 방법을 제안한다.  
- Motivation: 기존 3D/4D 생성 방식은 주로 사실적이고 효율성에 초점을 맞추지만, 다양한 시점에서 대상의 의미론적 정체성 유지에 어려움이 있다.  
- Contribution: 본 연구에서는 Progressive Texture Infilling 개념을 적용한 TIRE(Track, Inpaint, REsplat) 방법을 개발하여 대상 중심의 3D/4D 생성에서 정체성 보존을 크게 향상시켰다.  

## Method  
대상 식별 및 수정이 필요한 영역을 추적하는 Track 단계와, 맞춤형 2D 인페인팅 모델을 이용해 점진적으로 영역을 채우는 Inpaint 단계를 거친다.  
마지막 Resplat 단계에서는 다중 시점의 2D 보기를 3D 공간에 일관성 있게 재투영한다.  
이 세 단계의 협력으로 시점별로 일관되며 정체성을 유지하는 3D/4D 자산을 생성한다.  

## Results  
DreamBooth-Dynamic 벤치마크 및 다양한 실제 데이터에서 TIRE는 기존 최첨단 방법 대비 주관적·객관적 평가에서 정체성 보존과 지오메트리 품질 면에서 우수함을 입증했다.  

## Limitations  
정량 평가 지표 중 일부는 정체성 보존의 미묘한 부분을 완벽히 반영하지 못하는 한계가 존재한다.  

## Conclusion  
TIRE는 기존 피드포워드 3D/4D 생성 기법과 보완적이며, 대상 맞춤형 3D/4D 생성에서 정체성 보존과 텍스처 완성도를 효과적으로 개선하는 유용한 도구임을 확인했다.

# 10. [Mitigating Attention Sinks and Massive Activations in Audio-Visual   Speech Recognition with LLMS](https://arxiv.org/abs/2510.22603)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.22603)

## Introduction
- Goal: 본 연구는 멀티모달 음성 인식에서 대형 언어 모델(LLM)의 내부 동작을 분석하고, 주목(attention) 싱크와 대규모 활성화 현상을 완화하는 방법을 제안하는 것이다.  
- Motivation: LLM이 음성, 시각 및 음성-시각 인식에서 좋은 성능을 보이나, 내부 메커니즘과 특히 주목 싱크 및 대규모 활성화의 영향에 대한 이해가 부족하기 때문이다.  
- Contribution: 본 연구는 음성-시각 LLM에서 처음으로 중간 토큰과 시작 토큰(BOS)에서 발생하는 주목 싱크와 대규모 활성화의 원인을 규명하고, 이를 완화하는 새로운 상관관계 분리 손실을 제안한다.  

## Method  
멀티레이어 퍼셉트론(MLP) 층에서 발생하는 대규모 활성화가 BOS 토큰과 중간 토큰 간의 고유 방향성 정렬에서 유래한다는 점을 발견하였다. 이에 BOS와 다른 토큰 간 코사인 유사도를 줄이는 상관관계 분리 손실(decorrelation loss)을 도입하여 중간 주목 싱크와 대규모 활성화를 효과적으로 억제한다. 제안 기법은 아키텍처 변경 없이 LoRA 기반 미세조정에 통합되고 추론 비용을 증가시키지 않는다.  

## Results  
제안한 상관관계 분리 손실은 LRS2와 LRS3 데이터셋의 ASR, VSR, AVSR 작업에서 높은 압축률 하에서도 WER(단어 오류율)를 개선하며 기존 방법 대비 우수한 성능을 보였다.  

## Limitations  
본 연구에서는 제안 기법이 낮은 압축률에서는 성능 향상이 미미하여 중간 주목 싱크가 압축 환경에서 주로 문제를 야기함을 시사한다.  

## Conclusion  
LLM 기반 음성-시각 인식에서 BOS와 중간 토큰 간 방향 정렬로 발생하는 주목 싱크와 대규모 활성화를 상관관계 분리 손실로 완화하여 모델의 단어 인식 성능을 향상시킬 수 있음을 입증하였다.

# 11. [Memory-based Language Models: An Efficient, Explainable, and   Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.22317)

## Introduction
- Goal: 본 논문의 목표는 메모리 기반 언어 모델링이 딥 뉴럴 네트워크 기반 모델에 대안이 될 수 있음을 탐구하고 문서화하는 것이다.  
- Motivation: 기존 Transformer 기반 언어 모델 학습은 고비용이고 환경에 미치는 부정적 영향이 크므로 효율적이고 친환경적인 대체 방법이 필요하다.  
- Contribution: 본 연구에서는 k-최근접 이웃 분류기를 활용한 메모리 기반 언어 모델 OLIFANT를 제안하고, GPT-2 및 GPT-Neo와의 성능, 속도, 탄소 배출량을 비교 분석하였다.  

## Method  
OLIFANT는 토큰 예측에 k-NN 분류기와 접두어 트라이 자료구조를 활용하며, IB1-IG, TRIBL2, IGTree의 세 가지 분류 방법으로 구현되었다.  
k-NN 기반의 IB1-IG는 정확하지만 느린 반면, TRIBL2는 트리 기반 분류와 k-NN을 혼합해 속도와 정확도 균형을 맞추었다.  
IGTree는 결정 트리 분류를 사용해 데이터 압축과 빠른 예측을 가능케 하며, 맥락 길이는 4토큰으로 제한되었다.  

## Results  
OLIFANT의 TRIBL2와 IGTree는 GPT-2 및 GPT-Neo 대비 예측 정확도는 다소 낮으나, 다음 토큰 예측 속도는 빠르면서 추론 시 탄소 배출량은 최대 수십 배 적게 발생하였다.  

## Limitations  
맥락 길이가 4토큰으로 제한되며, 대량 데이터 학습 시 메모리 요구량이 매우 커져 최대 256GB RAM이 필요했다.  

## Conclusion  
메모리 기반 언어 모델링은 학습 및 추론 비용이 낮고 뛰어난 설명 가능성을 지니며, 친환경적 대안으로서 Transformer 모델과 경쟁 가능한 효율적 접근법임이 확인되었다.

# 12. [FlowOpt: Fast Optimization Through Whole Flow Processes for   Training-Free Editing](https://arxiv.org/abs/2510.22010)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.22010)

## Introduction
- 본 연구의 목표는 흐름(flow) 기반 생성 모델에서 전체 플로우 과정을 이용하여 학습 없이 이미지를 제어하는 최적화 방법을 제안하는 것이다.  
- 기존 흐름 및 확산 모델은 반복적인 샘플링 과정 때문에 최종 결과물을 직접적으로 제어하는 경사 기반 최적화가 계산적으로 비현실적이었다.  
- 이에 본 연구는 역전파 없이 전체 플로우 과정을 블랙박스로 취급하는 제로차(gradient-free) 최적화 프레임워크인 FlowOpt을 도입하였다.  

## Method  
FlowOpt은 플로우 모델의 샘플링 과정을 연쇄된 신경망인 “디노이저”들의 블랙박스로 보고, 목표 이미지와의 손실을 최적화하기 위해 야코비안 계산 없이 반복적으로 초기 노이즈 벡터를 갱신한다.  
적절한 스텝 사이즈 조건 하에서 전역 최적화 수렴을 이론적으로 보장하며, 이를 경험적으로 산출하여 효율적이고 안정적인 최적화를 수행한다.  
이 방법은 이미지 역전과 직접 편집 모두에 적용 가능하며, 중간 결과를 모니터링하며 조기 종료도 가능하다.  

## Results  
FlowOpt은 FLUX와 Stable Diffusion 3 모델 기반의 이미지 역전과 텍스트 기반 편집 모두에서 기존 방법과 유사한 신경망 함수 평가 수(NFE)로 최고 수준의 성능을 입증하였다.  

## Limitations  
본 접근법은 여전히 이미지 내 대규모 영역 변경과 같은 특정 상황에서는 한계가 존재하였다.  

## Conclusion  
FlowOpt은 학습 없이 플로우 모델 전체 과정을 효율적으로 최적화할 수 있는 제로차 방법으로, 다양한 응용 분야에서 사전학습된 플로우 모델 활용 가능성을 확장한다.

# 13. [Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion   Transformers](https://arxiv.org/abs/2510.21986)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.21986)

## Introduction
- Goal: 본 논문은 Diffusion Transformer(이하 DiT)의 효율적인 학습을 위해 고비율 토큰 삭제를 지원하면서도 성능 저하를 최소화하는 Sparse-Dense Residual Fusion 기법인 SPRINT을 제안하는 데 목적이 있다.  
- Motivation: DiT는 시퀀스 길이에 따라 학습 비용이 제곱으로 증가하여 대규모 사전학습이 매우 비싼 문제점이 존재하는데, 기존의 토큰 삭제 방법은 표현력 저하 또는 많은 파라미터 추가 등의 한계가 있다.  
- Contribution: SPRINT은 얕은 층에서는 모든 토큰을, 깊은 층에서는 희소 토큰만 처리하고 이들의 출력을 잔차 연결로 융합하는 단순한 구조와 2단계 학습 일정으로 효율적이고 견고한 학습을 가능하게 하여, 기존보다 최대 9.8배 학습 비용을 절감하면서도 성능은 유지하거나 향상시켰다.  

## Method  
SPRINT은 DiT를 인코더, 중간 블록, 디코더로 분할하고, 인코더에서 모든 토큰을 처리해 밀집 표현을 생성하며, 중간 블록에서는 75%의 토큰을 제거한 희소 표현만으로 전산량을 줄였다.  
희소 경로의 출력은 마스킹 토큰으로 위치를 복원하여 인코더의 밀집 특성과 채널 차원에서 결합되며, 합성된 표현은 디코더로 전달되어 최종 예측을 수행한다.  
학습은 먼저 토큰을 대폭 제거한 상태로 장시간 사전학습을 수행한 뒤, 전체 토큰 처리로 짧게 미세조정하여 학습-추론 간 간극을 해소한다.  

## Results  
ImageNet-1K 256×256 조건부 생성 실험에서 SPRINT은 기존 SiT 대비 최대 9.8× 훈련 비용 절감과 동등하거나 우수한 세대 품질(FDD, FID)을 달성하였으며, Path-Drop Guidance 기법으로 추론 시 연산량을 약 절반으로 줄이면서 품질 향상까지 입증되었다.  

## Limitations  
정보 부족이다.  

## Conclusion  
SPRINT은 깊고 얕은 층의 상보적 기능을 활용해 구조 변경 최소화로 대규모 DiT 학습을 효율화하고, 폭넓은 아키텍처와 해상도에 적용 가능하며, 추론 비용까지 절감하는 실용적이고 강력한 접근법임이 입증되었다.

# 14. [MARS-M: When Variance Reduction Meets Matrices](https://arxiv.org/abs/2510.21800)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2510.21800)

## Introduction
- Goal: 본 논문은 행렬 기반 프리컨디셔닝 최적화 기법과 분산 감소 기술을 결합한 새로운 최적화 알고리즘 MARS-M을 제안하는 것을 목표로 한다.  
- Motivation: 기존 대규모 신경망 학습에서 행렬 기반 최적화기가 벡터 기반에 비해 효율적이며, 분산 감소 방법이 학습 속도를 크게 향상시키지만 이 두 기술의 효과적인 통합은 미해결 문제로 남아있었다.  
- Contribution: MARS-M을 통해 Muon 최적화기와 MARS 분산 감소 기법을 통합하고, 이론적 수렴률 개선 및 다양한 자연어 처리와 컴퓨터 비전 벤치마크에서 성능 향상을 입증하였다.  

## Method  
MARS-M은 Muon 최적화기의 무니라이트(Moonlight) 변형에 MARS 기반의 스케일 조절 분산 감소를 결합하여 행렬 형태의 파라미터에 직접 작용하는 업데이트 규칙을 개발하였다.  
이 알고리즘은 뉴턴-슐츠 반복법을 이용해 행렬 모멘텀의 특이값 분해를 근사하며, 가중치 감쇠와 크기 조절 인자를 도입해 최적화 성능을 향상시킨다.  
또한, 분산 감소 기법을 효율적으로 적용하기 위해 근사 버전을 제안하여 실제 학습에서 계산 비용을 줄였다.  

## Results  
MARS-M은 GPT-2 기반 대규모 언어 모델 및 CIFAR-10 기반 컴퓨터 비전 과제에서 Mooonlight 및 AdamW와 비교하여 일관되게 낮은 손실과 우수한 성능을 기록하였다.  

## Limitations  
제안 기법은 이론적 수렴 분석에서 가변 모멘텀 매개변수를 요구하나, 실제 적용에서는 상수형을 사용하며 이로 인한 성능 차이가 존재한다.  

## Conclusion  
MARS-M은 행렬 기반 최적화와 분산 감소 기술의 효과적인 결합을 통해 대규모 신경망 학습에서 최적화 속도와 성능을 동시에 개선하는 실용적이며 이론적으로 뒷받침된 신규 알고리즘임이 입증되었다.
