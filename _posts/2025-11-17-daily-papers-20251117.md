---
layout: post
title: "Daily Papers — 2025-11-17"
date: 2025-11-17 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [DoPE: Denoising Rotary Position Embedding](https://arxiv.org/abs/2511.09146)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.09146)

## Introduction
- Goal: 본 연구는 Transformer 모델에서 사용되는 Rotary Position Embedding(RoPE)의 길이 일반화 한계를 해결하기 위해 노이즈를 제거하는 위치 인코딩 기법인 DoPE를 제안하는 데 목적이 있다.  
- Motivation: 기존 RoPE 방식은 장문의 문맥 처리 시 모델 성능이 급격히 저하되는 문제와 attention sink 현상을 내포하고 있어 이를 개선할 필요가 있었다.  
- Contribution: 본 논문은 트렁케이티드 행렬 엔트로피를 활용하여 노이즈 주파수 대역을 검출하고 파라미터 없는 가우시안 분포로 재파라미터화하는 훈련 불필요한 DoPE 방식을 개발하였다.  

## Method  
위치 인코딩을 노이즈가 포함된 특징 맵으로 재해석하고, 행렬 엔트로피 기반으로 low-rank 구조를 띠는 이상 주파수 대역을 검출한다. 특정 attention head의 위치 인코딩 변화를 선택적으로 제거하거나 가우시안 노이즈로 대체하는 방식으로 노이즈를 제거하여 장기 문맥에 대한 견고한 길이 외삽을 달성한다. 또한 DoPE는 attention sink 현상의 원인을 수학적으로 규명하고, 균형 잡힌 attention 분포 회복을 가능하게 한다.  

## Results  
각종 needle-in-a-haystack 및 대규모 in-context learning 실험에서 DoPE는 최대 64K 토큰 길이의 문맥까지 성능을 현저히 개선하면서 retrieval 정확도와 추론 안정성을 향상시켰다.  

## Limitations  
본 연구에서 제안한 방법은 특정 frequency 밴드에서 low-rank 특성이 강한 attention head의 선정과 알맞은 임계값 설정에 민감하며, 자동 선택 기준에 대한 최적화가 필요하다.  

## Conclusion  
DoPE는 RoPE의 고질적 문제인 attention sink를 효과적으로 완화하며, 단순하지만 강력한 노이즈 제거 기반 위치 인코딩 개선 기법으로서 길이 일반화 문제에 실질적 해결책을 제공한다.

# 2. [Virtual Width Networks](https://arxiv.org/abs/2511.11238)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.11238)

## Introduction
- Goal: 본 논문은 Transformer 모델의 임베딩 차원은 확장하되, 중간 은닉층의 크기는 유지하여 계산 비용 증가를 최소화하는 Virtual Width Networks (VWN) 프레임워크를 제안하는 것이다.  
- Motivation: 기존의 넓은 모델 폭 확장은 계산 비용이 제곱적으로 증가하는 문제가 있으며, 이를 해결하기 위한 효율적인 확장 방법이 필요했다.  
- Contribution: VWN은 Generalized Hyper-Connections (GHC)를 활용하여 임베딩 차원과 백본(hidden) 차원을 분리하고, 확장된 임베딩 공간의 효율적 사용과 다중 토큰 예측(MTP)과의 시너지를 통해 성능과 학습 효율을 향상시켰다.  

## Method  
VWN은 넓어진 토큰 임베딩(Over-Width Embedding)을 입력으로 받아 계산량이 거의 일정한 GHC를 통해 임베딩과 백본 간 정보를 압축하고 확장하는 과정을 반복한다. 동적 GHC(DGHC)로 입력에 적응하는 변환 매트릭스를 학습하여 네트워크 유연성을 확보하며, MTP 손실을 추가하여 가중치학습을 강화한다. 이렇게 하여 임베딩 확장과 은닉층 유지의 균형 속에서 모델 표현력을 증대시킨다.  

## Results  
VWN은 백본 폭을 고정한 채 임베딩 폭을 8배 확장할 때, 같은 손실 수준에 도달하기 위해 다음 토큰 예측에서 2.5배, 2개 토큰 예측에서 3.5배 적은 학습 토큰을 요구하며, downstream 벤치마크에서 최대 4점 이상의 정확도 향상을 보였다.  

## Limitations  
VWN은 추가적인 활성화 메모리 오버헤드와 약간의 계산 비용 증가가 발생하며, 초기화 및 최적화 안정성에 관해 추가 연구가 필요하다.  

## Conclusion  
Virtual Width Networks는 임베딩 폭을 확장하면서도 백본의 계산 비용을 거의 유지하여, 대규모 언어 모델에서 효율적인 학습 가속과 성능 향상을 동시에 달성하는 새로운 확장 차원을 제시한다.

# 3. [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.11062)

## Introduction
- Goal: 본 연구는 동영상 생성에 사용되는 Diffusion Transformers의 높은 연산 복잡도를 줄이기 위해 시간적으로 일관된 희소 패턴을 활용하는 LiteAttention 기법을 제안하는 데 목적이 있다.  
- Motivation: 기존 동적 희소성 방법은 매 단계마다 높은 계산 비용과 추정 오류를 초래하며, 정적 희소성 방법은 전체 과정에서 최적이 아니기 때문에 이들 간의 근본적인 절충이 존재한다.  
- Contribution: 시간적 일관성을 활용하여 한 번 결정된 계산 건너뛰기 패턴을 전체 소음 제거 과정에 걸쳐 전파함으로써 동적 적응성과 정적 효율성을 결합한 LiteAttention 방법을 개발하였다.  

## Method  
LiteAttention은 각 타임스텝에서 불필요한 타일을 초기 단계에서 식별하고 이후 단계에서도 동일한 타일을 계산하지 않도록 스킵 마스크를 유지한다. FlashAttention3 기반의 CUDA 커널에 통합되어 최소한의 메모리 오버헤드로 실행되며 누적 오차 보정을 통해 정확도를 유지한다. 이러한 진화적 스킵 메커니즘을 통해 주의 계산량을 대폭 줄이고 전체 처리 속도를 향상시킨다.  

## Results  
Wan2.1-14B 및 Wan2.2-14B 동영상 확산 모델에서 LiteAttention은 최대 42% 계산량 희소성을 달성하며, 기존 SparseVideoGen 및 RadialAttention 대비 실행 시간을 최소 10% 이상 단축하면서 시각 품질 저하 없이 유사한 혹은 더 나은 성능을 보였다.  

## Limitations  
높은 희소성(약 70% 이상)에서는 가시적인 영상 왜곡과 품질 저하가 나타나며, 이를 방지하기 위한 세밀한 보정이 필요하다.  

## Conclusion  
LiteAttention은 시간적 희소성의 일관성을 이용하여 동영상 확산 변환기의 주의 연산을 효과적으로 가속화하면서 뛰어난 영상 품질을 유지하는 실용적이고 확장 가능한 가속화 방법임이 입증되었다.

# 4. [HI-TransPA: Hearing Impairments Translation Personal Assistant](https://arxiv.org/abs/2511.09915)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.09915)

## Introduction
- Goal: 본 연구는 난청인의 불명확한 음성과 입술 움직임을 융합하여 정확한 번역과 대화를 지원하는 HI-TransPA라는 통합 멀티모달 개인비서 모델을 제안하는 데 목적이 있다.  
- Motivation: 난청인은 음성 생성의 어려움으로 인해 일상 의사소통에 큰 장벽을 경험하며 기존 음성 인식 시스템은 이들의 표현을 제대로 지원하지 못한다.  
- Contribution: 본 연구는 고속 입술 동작 인코딩을 특징으로 하는 3D-Resampler와 품질 인지 커리큘럼 학습 전략을 도입하여 난청인 음성의 독특한 특성을 효과적으로 학습하는 통합 멀티모달 프레임워크를 구축하였다.  

## Method  
HI-TransPA는 (1) 얼굴 랜드마크 기반 입술 영역 추출과 고품질 샘플 선택을 위한 음악-영상 데이터 전처리 및 큐레이션, (2) 입술 동작에 특화된 Vision Transformer 및 3D-Resampler를 포함하는 시각 인코더 아키텍처, (3) 품질 순서 기반 커리큘럼 학습을 통한 쉬운 샘플에서 어려운 샘플 순의 다단계 면밀 학습 과정으로 구성된다.  

## Results  
HI-TransPA는 난청인 대화 데이터셋(HI-Dialogue)에서 최첨단 자동음성인식(ASR), 대규모 음성언어 모델(LALMs) 및 범용 옴니모델 대비 문자 오류율과 의미적 정합성 지표 모두에서 우수한 성능(CS=0.79, CER=27%, EmbSim=0.84)을 기록하였다.  

## Limitations  
본 연구는 잡음과 발화 불명확성 문제를 완화하였으나, 입술 움직임 기반 시각 정보의 품질 저하나 일부 극단적 난청 환경에 대한 적응성은 아직 제한적이다.  

## Conclusion  
HI-TransPA는 난청인을 위한 정확한 음성 번역과 자연스러운 대화를 지원하는 고속 멀티모달 Omni-Model로서, 접근성 향상을 위한 통합 지능형 보조기술 발전에 중요한 기초를 제공한다.

# 5. [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.11519)

## Introduction
- Goal: 본 연구는 추론 시점에서 누적된 경험을 바탕으로 문제별 맞춤형 추론 전략을 동적으로 생성하는 시스템을 제안하는 것을 목표로 한다.  
- Motivation: 기존의 AI 시스템은 추론 시 전략 구성이 정적이며 경험에 따른 전략적 적응이 불가능해 효율성과 정확성 향상에 한계가 존재한다.  
- Contribution: 경험에 기반하여 프롬프트, 샘플링 파라미터, 도구 구성과 제어 로직 등 모든 전략 구성요소를 동적 생성 가능한 메타전략 시스템 EGUR을 개발하였다.  

## Method  
EGUR은 (1) 문제 및 경험 메모리를 조건으로 다양한 후보 전략을 생성하는 Guide와 (2) 실행 결과를 통합해 기억을 갱신하는 Consolidator로 구성된다.  
전략은 상태를 가지는 컴포넌트들의 조합으로 형식화되어, 전략 간 비교 기반 지속적 개선과 효과적 캐싱 및 재사용이 가능하다.  
Guide는 LLM을 이용한 전략 코드 생성 방식을 사용하며, Consolidator는 실행 경험을 요약·관리하여 향후 전략 생성에 활용한다.  

## Results  
다섯 가지 다양한 벤치마크에서 EGUR은 최고 성능 대비 최대 14% 정확도 향상과 최대 111배 계산 비용 절감을 달성하였으며, 경험 축적에 따라 두 성능 지표가 모두 개선되었다.  

## Limitations  
EGUR은 현행에서는 정답 기반 평가자 피드백에 의존하며, 미지의 문제 유형에 대해 Guide의 전략 생성 능력이 미흡할 경우 별도 학습이나 최적화가 요구된다.  

## Conclusion  
EGUR은 경험 기반으로 완전한 전략을 추론 시점에 동적으로 생성하여 정확도와 계산 효율성을 크게 향상시키는 새로운 적응형 AI 추론 프레임워크임을 입증하였다.

# 6. [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.11002)

## Introduction
- Goal: 본 논문은 감정 중심의 영상 이해 및 생성 연구를 위해 예술적 스타일의 영상에 특화된 다중모달 감정 주석 비디오 데이터셋 EmoVid를 제안하는 데 목적이 있다.  
- Motivation: 기존 비디오 생성 시스템은 주로 시각적 품질에 집중하여 감정적 표현을 충분히 반영하지 못하며, 특히 창의적이고 비현실적 영상 분야에서 감정 이해와 생성 과제를 잇는 자원이 부족하였다.  
- Contribution: EmoVid는 만화, 영화, 애니메이션 스티커 등 세 가지 영역에서 8가지 감정 범주로 라벨링된 22,758개 클립을 포함하며, 시각 속성 및 텍스트 캡션과 함께 감정 패턴을 분석하고 이를 통해 감정 조건부 영상 생성 모델을 개발하였다.  

## Method  
- EmoVid 데이터셋은 인간-기계 협업 방식으로 Mikels의 8감정 분류체계를 활용해 라벨링되고, 밝기·채도·색조 등의 시각적 속성과 텍스트 캡션을 포함한다.  
- Wan2.1 모델을 EmoVid로 미세조정하여 감정 정보를 명시적으로 반영하는 영상 생성 프레임워크를 구축하였다.  
- 다양한 최첨단 텍스트-투-비디오 및 이미지-투-비디오 모델과 비교 평가하여 감정 정확도 및 영상 품질을 향상시켰다.  

## Results  
- EmoVid로 미세조정한 Wan2.1 모델은 감정 일치도 지표(EA-2cls, EA-8cls)에서 기존 모델 대비 각각 최대 88.33%와 48.33%의 향상을 보이며, 텍스트 및 이미지 기반 영상 생성에서 유의미한 감정 표현 품질 개선을 입증하였다.  

## Limitations  
- 본 연구는 클립 단위에서 단일 감정 표현을 가정하였으나, 실제 인간 감정의 복합성과 세밀한 표현은 반영하지 못하며 오디오 신호의 통합 활용 여지는 남아 있다.  

## Conclusion  
- EmoVid는 창의적 미디어 영상에서 감정 이해와 생성 간의 간극을 메우는 최초의 대규모 다중모달 감정 비디오 데이터셋으로, 감정 인지 및 조절 영상 생성 연구의 새로운 벤치마크 및 실용적 기반을 제공한다.
