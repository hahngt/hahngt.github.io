---
layout: post
title: "Daily Papers — 2025-11-07"
date: 2025-11-07 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm](https://arxiv.org/abs/2511.04570)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.04570)

## Introduction
- Goal: 본 논문은 비디오 생성 모델을 활용하여 시각 및 텍스트 추론을 시간적 통합 틀 내에서 수행하는 ‘Thinking with Video’라는 새로운 다중모달 추론 패러다임을 제안하는 데 목적이 있다.  
- Motivation: 기존의 ‘Thinking with Text’ 및 ‘Thinking with Images’ 패러다임은 정적인 이미지에 한정되고, 텍스트와 시각 정보가 분리되어 있어 통합적 다중모달 이해와 생성에 한계가 존재하였다.  
- Contribution: 제안한 ‘Thinking with Video’ 패러다임을 평가하기 위해 시각 중심 및 텍스트 중심 과제를 포함하는 Video Thinking Benchmark (VideoThinkBench)를 구축하고, 비디오 생성 모델 Sora-2의 우수한 추론능력을 입증하였다.  

## Method  
‘Thinking with Video’는 시각적 동적 과정과 텍스트 정보가 통합된 연속적 비디오 생성 방식을 통해 복잡한 추론을 수행한다.  
VideoThinkBench는 시각 중심(예: Eyeballing Puzzles, 미로 탐색)과 텍스트 중심(예: GSM8K, MMMU) 두 가지 과제군으로 구성되어 Sora-2의 성능 평가에 활용된다.  
최신 VLM과 비교 실험을 통해 Sora-2가 다중모달 통합 및 연속 추론에서 경쟁력 있는 성능을 보임을 보였다.  

## Results  
Sora-2는 시각 중심 과제에서 SOTA VLM과 비슷하거나 일부 과제에서 우수한 성능을 보였고, 텍스트 중심 과제에서 GSM8K 92%, MMMU 75.53%의 정확도를 달성하여 비디오 기반 다중모달 추론의 잠재력을 확인하였다.  

## Limitations  
Sora-2는 텍스트 기반 추론 과정에서 일관된 논리적 과정 출력을 어려워하며 일부 추론 정확성 향상과 텍스트 생성 품질 개선이 필요하다.  

## Conclusion  
비디오 생성 모델은 시각적 및 텍스트 정보를 통합하는 범용 다중모달 추론 및 생성 모델로서 유망하며, ‘Thinking with Video’는 통합 다중모달 추론의 새로운 패러다임임을 입증하였다.

# 2. [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.03929)

## Introduction
- Goal: Nemotron Nano V2 VL은 실세계 문서 이해, 긴 영상 해석, 추론 과제에서 탁월한 성능을 목표로 하는 12B 비전-언어 모델이다.  
- Motivation: 전작 Llama-3.1-Nemotron-Nano-VL-8B 대비 모델 구조, 데이터셋, 학습 방법론에서 대폭 개선하여 성능 향상을 이루기 위함이다.  
- Contribution: 멀티모달 및 텍스트 도메인에서 우수한 정확도를 달성하며, 긴 문서와 영상 처리에 특화된 토큰 감소 기법과 128K까지 확장된 컨텍스트 길이를 지원하는 모델과 학습 코드, 데이터셋을 공개하였다.  

## Method  
Nemotron Nano V2 VL은 RADIOv2.5 비전 인코더와 Nemotron Nano V2 LLM을 결합한 멀티모달 융합 아키텍처를 채택하였다. 입력 이미지들은 동적 타일링 방식을 통해 512×512 크기의 시퀀스로 변환되며, 영상은 프레임 단위로 샘플링되어 처리된다. 다단계 Supervised Finetuning(SFT)를 통해 단계별로 컨텍스트 길이를 확장하고, 비전과 텍스트 이해 능력을 균형 있게 강화하였다.  

## Results  
Nemotron Nano V2 VL은 OCRBench v2를 포함한 45개 이상의 벤치마크에서 전작 대비 일관된 성능 향상을 보였으며, 특히 영상 처리에서는 Efficient Video Sampling 적용으로 처리 속도는 2배 이상 증가시키면서 정확도는 거의 유지되었다.  

## Limitations  
텍스트 추론 능력과 비전 성능 간의 균형 조절 과정에서 일부 초기 학습 단계에서 성능 저하가 관찰되어 추가 복구 학습 단계를 필요로 하였다.  

## Conclusion  
본 연구는 Nemotron Nano V2 VL을 통해 향상된 멀티모달 이해 및 추론 능력을 제시하였으며, 다양한 태스크 평가 및 효율적 영상 처리 기법을 포함한 완성도 높은 비전-언어 모델을 공개함으로써 관련 연구 발전에 기여하였다.

# 3. [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.04217)

## Introduction
- Goal: 본 연구는 멀티-헤드 어텐션 기구 내에 강력한 로또 티켓(Strong Lottery Tickets, SLTs)이 존재함을 이론적으로 입증하는 것을 목표로 한다.  
- Motivation: 기존의 강력한 로또 티켓 가설(Strong Lottery Ticket Hypothesis, SLTH)은 다양한 신경망 아키텍처에 대해 증명되었으나, 트랜스포머의 핵심 구조인 멀티-헤드 어텐션에 대해서는 이론적 이해가 부족하였다.  
- Contribution: 본 연구는 멀티-헤드 어텐션 구조의 쿼리와 키의 내적을 선형 신경망으로 재해석하여, 이를 기반으로 SLTH를 멀티-헤드 어텐션 및 정규화 없는 트랜스포머에 확장한 최초의 이론적 증거를 제시하였다.  

## Method  
멀티-헤드 어텐션 내 쿼리와 키의 결합된 프로젝션을 단일 행렬로 병합하고, 이를 두 개의 무작위 행렬을 가지는 두 층 구조로 근사하는 새로운 접근법을 도입하였다.  
이러한 배열을 기반으로 이차원 합을 통한 근사 보조 결과와 함께, 기존의 two-layers-for-one approximation 기법을 변형하여 적용하였다.  
소프트맥스 이후 단계에 대해서는 입력 길이 T에 무관한 오차 상한을 보장하는 정밀한 오차 분석을 수행하였다.  

## Results  
은닉 차원을 증가시킴에 따라 소스 모델의 근사 오차가 지수적으로 감소하며, 이 근사 오차는 입력 길이 증가에도 발산하지 않음을 합성 데이터 실험과 트랜스포머 블록 수 변화 실험을 통해 검증하였다.  

## Limitations  
본 연구의 이론적 증명은 정규화 층이 없는 트랜스포머 구조에 한정되며, 실제 트랜스포머 모델의 완전한 구조를 포괄하지는 않는다.  

## Conclusion  
본 연구는 멀티-헤드 어텐션 및 트랜스포머 내부에 강력한 로또 티켓이 존재함을 이론적으로 규명함으로써 SLTH를 트랜스포머 아키텍처로 확장하고, 이를 통해 과도한 파라미터 모델에 대한 근본적인 이해를 심화하였다.

# 4. [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.03295)

## Introduction
- Goal: 본 연구는 음성-텍스트 번역(ST) 평가에 있어 소스 음성을 반영하는 신경 기계 번역(MT) 지표의 적용 가능성을 체계적으로 분석하는 데 목적이 있다.  
- Motivation: 기존의 ST 평가는 참조 번역과의 비교에 의존해 소스 음성의 유용한 정보를 활용하지 못하는 한계가 존재한다.  
- Contribution: 음성의 합성 텍스트 대체물을 자동 음성 인식(ASR) 및 역번역(BT)으로 생성하고, 소스-참조 정렬 문제 해결을 위한 새로운 교차언어 재분할 알고리즘(XLR-Segmenter)을 제안하며, 이를 79개 언어 쌍과 6개 ST 시스템에서 평가하였다.  

## Method  
음성 소스가 텍스트가 아닌 상황에서, ASR 및 BT를 활용해 합성 소스 텍스트를 생성하였다.  
합성 소스와 참조 번역의 구간 불일치를 해소하기 위해 교차언어 재분할 알고리즘(XLR-Segmenter)을 개발하였다.  
COMET와 MetricX 두 가지 소스 인지 신경 MT 평가 지표를 사용해 다양한 설정에서 평가를 수행하였다.  

## Results  
ASR 기반 합성 소스는 단어 오류율(WER)이 20% 이하일 경우 BT보다 신뢰성이 높으며, XLR-Segmenter는 소스-참조 불일치 상황에서도 안정적인 평가를 가능케 하였다.  

## Limitations  
ASR 모델의 언어 커버리지와 정확성이 제한적이고, 일부 ASR 세팅에서 성능 저하가 관찰되어 그 원인 규명과 개선은 후속 연구 과제이다.  

## Conclusion  
합성 소스 기반 소스 인지 MT 지표와 교차언어 재분할 기법은 실제 조건에서 ST 평가의 정밀도와 원칙성을 향상시키는 실용적 방안임이 확인되었다.

# 5. [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.00956)

## Introduction
- 본 연구의 목표는 추가적인 비연결(reference) 이미지 정보를 활용하여 타깃 인물 사진에 의류를 직접 입히는 엔드투엔드 가상 피팅 모델 EVTAR을 제안하는 것이다.  
- 기존 가상 피팅 기법이 인물의 자세, 마스크 등 복잡한 전처리 데이터를 요구하여 실제 적용에 한계가 있어 이를 간소화하고 리얼리티를 높이고자 하였다.  
- 본 연구는 EVTAR 모델과 보조 참조 이미지 활용, 이에 대응하기 위한 가상 피팅 데이터셋 구축, 그리고 마스크 없이도 고품질 결과물을 생성하는 방식을 포함하여 실용성과 성능을 동시에 개선하였다.  

## Method  
EVTAR은 두 단계 학습 전략을 통해 먼저 마스크 기반의 의류-인물 합성 모델로 비연결 인물 이미지를 생성한 뒤, 이를 원본 인물 이미지와 목표 의류, 참조 이미지와 함께 입력받아 직접 의류를 입히는 확장 모델을 학습한다.  
본 모델은 기존 조건 정보(자세, 마스크 등)를 제거하고 대신 동일 의류를 착용한 다른 인물의 참조 이미지로 세부 의류 질감과 디테일을 보존한다.  
또한 다중 조건 입력을 처리하기 위해 Transformer 기반의 diffusion 모델 아키텍처를 LoRA 기법으로 효율적 미세조정하여 실용적인 가상 피팅 시스템을 구현한다.  

## Results  
VITON-HD와 DressCode 두 공개 벤치마크에서 참조 이미지 입력 시 EVTAR이 기존 최첨단 기법 대비 SSIM, FID, LPIPS 등 지표에서 우수한 성능을 보이며 마스크 없이도 높은 품질과 의류-인물 일관성을 달성하였다.  

## Limitations  
정확한 참조 이미지 생성과 데이터셋 구축을 위하여 별도의 인공합성 파이프라인에 의존해야 하는 점은 실제 적용 시 추가 비용으로 작용할 수 있다.  

## Conclusion  
본 연구는 복잡한 전처리 없이 단순 인물 및 의류 이미지 입력으로 사실적이고 세밀한 가상 피팅을 구현하며, 사용자 행동과 유사한 참조 이미지 활용을 통해 도메인 적응성과 재현성을 대폭 향상시키는 차세대 가상 피팅 모델을 제안하였다.
