---
layout: post
title: "Daily Papers — 2025-12-19"
date: 2025-12-19 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16915)

## Introduction
- Goal: Mono(단안) 입력 영상으로부터 고품질의 입체(스테레오) 영상을 효율적으로 자동 변환하는 모델을 개발하는 것이다.  
- Motivation: 기존의 깊이 추정 기반 다단계 “Depth-Warp-Inpaint” 방식은 깊이 불확실성과 포맷 불일치 문제 등으로 인해 변환 품질과 계산 효율성에 한계가 있다.  
- Contribution: 병렬형과 수렴형 스테레오 포맷 모두를 아우르는 대규모 통합 데이터셋 UniStereo를 구축하고, 선행 확산 모델의 생성 사전지식을 활용하는 단일 단계 피드포워드 아키텍처 StereoPilot을 제안하였다.  

## Method  
StereoPilot은 명시적 깊이 지도 없이도 목표 시점을 직접 합성하는 단일 단계 모델로, 학습 가능 도메인 스위처와 순환 일관성 손실로 다양한 스테레오 포맷 간 일관성과 정밀도를 보장한다. 본 모델은 깊이 모호성 및 포맷 특화 가정을 극복하며, 확산 모델의 생성적 사전 지식을 효율적으로 활용한다.  

## Results  
StereoPilot은 UniStereo 벤치마크에서 기존 최첨단 방법 대비 영상 품질 및 변환 속도에서 우수한 성능을 입증하였다.  

## Limitations  
실시간 라이브 스트리밍과 같은 실시간 변환 적용을 위해서는 현재 11초의 5초 영상 처리 속도가 여전히 아쉽다.  

## Conclusion  
본 연구는 다양한 스테레오 포맷을 통합 처리하는 효율적이고 고품질의 자동 입체 영상 변환을 가능하게 하는 StereoPilot과 이를 평가할 UniStereo 데이터셋을 처음으로 제시하였다.

# 2. [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16923)

## Introduction
- Goal: 본 연구의 목표는 단일 이미지로부터 유연하게 초점면과 조리개 형태를 제어하는 생성적 리포커싱 기법을 개발하는 것이다.  
- Motivation: 깊이 초점 조절은 사진 촬영에서 필수적이나 완벽한 초점을 맞추기 위해 여러 차례 촬영이나 특수 장비가 필요하며, 단일 이미지로 리포커싱하는 기술은 여전히 어려움이 존재한다.  
- Contribution: 본 논문은 DeblurNet과 BokehNet의 두 단계 모델과 합성-비합성 반지도학습 기법을 도입하여 실제 광학 특성을 반영한 고품질 디포커스 제거 및 컨트롤 가능한 보케 합성을 동시에 수행한다.  

## Method  
제안된 방법은 우선 DeblurNet을 통해 다양한 초점 상태의 입력 이미지에서 올-인-포커스(all-in-focus) 이미지를 복원하며, 다음으로 BokehNet이 반지도학습으로 학습되어 깊이맵, 초점면, 보케 강도, 조리개 형태를 조절하며 보케 합성을 수행한다. 실험을 통해 합성 데이터와 실제 비짝 데이터 및 EXIF 메타데이터를 활용하는 반지도학습 프레임워크를 사용함으로써 기존 방법들의 한계를 극복하였다. 또한 사용자가 텍스트 지침이나 임의 조리개 모양을 지정하여 보케 효과를 다양하게 조절할 수 있도록 하였다.  

## Results  
본 연구의 방법은 DPDD, REALDOF, LF-BOKEH, LF-REFOCUS 벤치마크에서 기존 최첨단 기법들을 능가하는 디포커스 디블러링, 보케 합성 및 리포커싱 성능을 일관되게 입증하였다.  

## Limitations  
본 방법은 단안 심도 추정에 의존하며 심도 추정 실패 시 초점면 위치가 잘못 지정되는 한계가 존재한다.  

## Conclusion  
Generative Refocusing은 단일 이미지로부터 초점면, 보케 세기 및 조리개 형태를 고도로 제어할 수 있는 가상 카메라를 구현하는 두 단계 확산모델 기반 혁신적 프레임워크로서 실제 광학 효과 재현 및 다양한 편집 가능성을 제공한다.

# 3. [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16625)

## Introduction
- Goal: 본 연구의 목표는 Diffusion Transformer(DiT) 기반의 인컨텍스트 이미지 편집에서 사용자 이미지의 무단 변형을 방지하는 방어 기법을 제안하는 것이다.  
- Motivation: 인컨텍스트 확산 모델이 현실적인 이미지 수정을 가능하게 하는 반면, 개인 이미지가 무단으로 조작되어 사생활 침해 및 허위정보 생성에 악용될 수 있는 심각한 위험이 존재한다.  
- Contribution: 본 논문은 DiT 모델 내 멀티모달 어텐션 메커니즘에 주목하여, 해당 어텐션 흐름을 교란하는 정교한 입력 이미지 교란 기법인 DeContext를 개발하여 효과적인 보호를 달성하였다.  

## Method  
DeContext는 입력 이미지에 미세한 교란을 주입하여 DiT 모델의 타겟 쿼리와 컨텍스트 키 간의 어텐션 연결을 약화시키고, 중요 단계의 디노이징 시점과 조기-중간 트랜스포머 블록에 집중적으로 개입한다. 이러한 교란은 멀티모달 어텐션을 차단함으로써 입력과 출력 간의 연관성을 단절하고, 시각적 품질을 유지하면서 무단 편집을 방지한다. 모델 가중치는 고정되며, 입력 이미지 픽셀만 업데이트하여 효율성과 범용성을 확보하였다.  

## Results  
FLUX.1-Kontext와 Step1X-Edit 실험 결과, DeContext는 얼굴 인식 정확도를 70% 이상 감소시키며 무단 편집을 효과적으로 차단하는 동시에 기존 UNet 기반 보호 기법 대비 시각 품질 면에서 우수한 성능을 보였다.  

## Limitations  
본 연구는 대형 모델에서의 방어 효율성 향상과 블랙박스 환경에서의 견고성 및 전이성 강화가 향후 과제로 남아있다.  

## Conclusion  
DeContext는 DiT 기반 인컨텍스트 이미지 편집에 대한 최초의 효율적 방어법으로, 멀티모달 어텐션 흐름을 교란하여 사생활 보호와 고품질 이미지 생성을 동시에 실현하였다.

# 4. [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16636)

## Introduction
- Goal: 본 연구는 VAE 잠재공간과 비전 파운데이션 모델(VFM)의 전역 및 지역 의미론적 표현을 통합하여 잠재 확산 모델의 성능을 향상시키는 통합 잠재 확산 프레임워크 REGLUE를 제안하는 것이다.  
- Motivation: 기존 잠재 확산 모델들은 간접적인 의미 정보만을 활용하여 학습 수렴이 느리고 생성 이미지 품질이 제한되며, 최근 방법들은 VFM 기반 의미를 충분히 활용하지 못하는 한계를 가지고 있었다.  
- Contribution: REGLUE는 VAE 잠재벡터, 지역(패치 수준) 및 전역(이미지 수준) VFM 의미론적 토큰을 단일 Transformer 백본에서 비선형 압축 방식으로 결합하여 의미 정보의 완전한 활용과 모델의 빠른 수렴 및 고품질 이미지 생성을 달성하였다.  

## Method  
REGLUE는 다층 VFM 패치 특징을 비선형 경량 컨볼루셔널 압축기를 통해 저차원 공간에 압축하고, 이를 VAE 잠재 벡터 및 전역 [CLS] 토큰과 함께 SiT 기반 확산 모델에 통합한다. 학습 과정에서 외부 정렬 손실을 추가하여 내부 특징을 고정된 VFM 표상과 정렬시킴으로써 의미적 일관성을 강화한다. 입력 토큰 구성은 전역 토큰과 결합된 채널 통합 방식의 지역 토큰으로 구성하며, 확산 과정은 이 세 모달리티를 동시에 모델링하는 멀티모달 속도 손실로 최적화된다.  

## Results  
ImageNet 256×256 벤치마크에서 REGLUE는 기존 SiT-B/2 및 SiT-XL/2 기반 확산 모델과 REPA, ReDi, REG 대비 FID 수치를 현저히 개선하며 수렴 속도를 크게 가속화하여 1M 단계에서 REG의 80%, ReDi의 30% 이하의 반복 횟수로 동등 또는 우수한 성능을 달성하였다.  

## Limitations  
본 연구는 VFM 특징 압축 및 통합에 초점을 맞추었으나, 더욱 복잡한 의미 표현이나 다양한 VFM 아키텍처에 대한 일반화 가능성에 대해서는 추가 연구가 필요하다.  

## Conclusion  
REGLUE는 VAE 잠재공간과 전역 및 지역 VFM 의미 표현을 비선형적으로 결합하여 잠재 확산 모델의 생성 품질과 학습 효율을 크게 향상시키는 효과적인 통합 잠재 확산 프레임워크임을 입증하였다.

# 5. [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16920)

## Introduction
- Goal: 본 연구는 지침 기반 고품질 영상 편집을 위한 간단하고 효과적인 프레임워크 EasyV2V를 제안하는 데 목적이 있다.  
- Motivation: 이미지 편집은 빠르게 발전한 반면, 영상 편집은 일관성, 제어력 및 일반화 문제로 인해 상대적으로 미진하였다.  
- Contribution: 데이터 구성, 모델 구조, 제어 방식을 통합하여 유연한 입력과 최첨단 성능을 달성하는 가벼운 영상 편집 모델을 제안하였다.  

## Method  
EasyV2V는 기존 전문가 모델들의 조합과 영상 지속성을 활용해 다양한 영상-편집 쌍을 구축하며, 단일 프레임을 통한 영상 지도학습과 강력한 비디오 생성기 백본을 기반으로 한다.  
입력 영상과 마스크를 순차열 형태로 통합하여 편집을 유연하게 제어하며, LoRA 미세조정을 통해 효율적이고 안정적으로 학습된다.  
추가로 참조 영상 조건부를 지원하여 편집 정확도와 스타일 일치도를 향상시키며, 공간 및 시간적 제어를 마스크 영상 하나로 통합 관리한다.  

## Results  
EasyV2V는 EditVerseBench 벤치마크에서 동시출간 및 상업 시스템을 능가하는 VLM 점수 7.73/9를 기록하며 지침 준수와 영상 품질 모두에서 우수한 성능을 입증하였다.  

## Limitations  
본 모델은 동시대의 확산 기반 영상 생성 모델들과 마찬가지로 1분 내외의 추론 시간이 필요하여 실시간 적용에는 한계가 존재한다.  

## Conclusion  
본 연구는 혁신적 데이터 전략과 최소한의 미세조정 아키텍처를 결합하여 유연한 제어가 가능한 고품질 지침 기반 영상 편집의 새로운 표준을 제시하였다.

# 6. [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16378)

## Introduction
- Goal: 음성 모달리티를 대형 언어 모델(LLM)에 통합하는 것이 음성-텍스트 번역(ST) 품질 향상에 실질적으로 기여하는지 평가하는 것이다.  
- Motivation: 전통적인 전사 기반 파이프라인을 우회하여 음성 언어를 직접 번역할 수 있는 SpeechLLM의 효용성이 아직 명확하지 않기 때문이다.  
- Contribution: 13개 언어쌍과 9가지 실제 음성 현상을 포함한 16개 벤치마크를 대상으로 5종 SpeechLLM과 16종 강력한 직접 및 파이프라인 시스템을 종합 비교한 최초의 체계적 평가 결과를 제시하였다.  

## Method  
음성-언어 이해를 다루는 SpeechLLM과 음성 인식 기반 파이프라인 및 직접 모델들을 21종 선정하여 다양한 음향 및 언어 현상에 대응하는 성능을 품질 추정 기반 자동평가 지표로 비교하였다.  
성별, 악센트, 코드전환, 잡음, 비유창성 등 실제 음성 현상을 반영하는 9가지 조건을 포함한 Hearing to Translate 테스트 스위트를 구축하였다.  
자동평가와 더불어 다국어 네이티브 화자에 의한 소규모 인간 평가를 수행하여 자동 지표의 신뢰성을 검증하였다.  

## Results  
전 영역에서 파이프라인 시스템이 가장 안정적이고 우수한 번역 품질을 보여주었으며, SpeechLLM은 일부 시나리오(잡음, 코드전환, 비유창성)에서만 경쟁력을 갖추었고 독립적인 음성 기초 모델은 상대적으로 뒤처졌다.  

## Limitations  
다양한 모델들이 특정 언어 변종과 실제 음성 조건에 민감하게 반응하며, 성별 바이어스와 악센트 편향 문제를 완전히 해결하지 못하였다.  

## Conclusion  
음성 모달리티를 LLM에 직접 통합하는 방법은 일부 영역에서 가능성을 보이나, 전반적인 ST 품질 확보에는 여전히 강력한 음성 인식 및 번역 모듈을 결합한 파이프라인 방식이 우세하다.

# 7. [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16106)

## Introduction
- Goal: 본 논문은 AI 모델과 관련된 성능 및 구성 표를 포괄적으로 수집하고 관련성을 평가할 수 있는 대규모 테이블 코퍼스 및 벤치마크인 ModelTables를 제안하였다.  
- Motivation: 기존의 표 코퍼스들이 광범위한 주제를 포함하는 반면, 모델에 특화된 체계적이고 관련성 기반의 평가 데이터셋 부재 문제를 해결하고자 한다.  
- Contribution: ModelTables는 6만 개 이상의 모델과 9만 개 이상의 테이블을 포함하며, 논문 인용, 모델 카드 연결, 공통 학습 데이터셋을 기반으로 한 관련성 기준을 제공한다.  

## Method  
Hugging Face 모델 카드, GitHub README, 참고 논문에서 테이블을 자동으로 추출하고 품질 검증을 거쳤다.  
모델 관련성은 논문 인용관계, 명시적 모델 링크 및 상속, 공통 학습 데이터셋 세 가지 신호를 사용하여 다중 수준 그래프로 구축하였다.  
추출한 테이블의 표현 다양성을 해소하기 위해 전치 및 헤더-셀 병합 증강 기법을 적용하였다.  

## Results  
테이블 검색 실험에서 기존의 데이터 레이크 기반 검색 연산자 및 정보 검색 기법 대비 밀집 벡터 검색이 최고 정확도(P@1 66.5%)를 달성했으나 전반적으로 높은 개선 여지를 보였다.  

## Limitations  
표본 추출 및 품질 보정 과정에서 arXiv PDF 기반 테이블 추출의 낮은 품질로 일부 과학논문 테이블은 제한적으로 활용되었다.  

## Conclusion  
ModelTables는 모델 이해와 테이블 검색을 위한 최초의 대규모 AI 모델 표 벤치마크로서, 다양한 관련성 신호를 기반으로 한 테이블 검색 및 구조적 비교 연구를 촉진하는 중요한 자산이다.

# 8. [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16767)

## Introduction
- 본 논문은 3D 휴머노이드 캐릭터의 포징을 잠재 공간에서의 변환 문제로 재정의하여, 단일 feed-forward 패스로 고품질 포즈 변형을 구현하는 것을 목표로 한다.  
- 기존의 자동 리깅이나 포즈 조건 생성 방식이 스킨닝 아티팩트, 위상 변화 처리 불가, 낮은 포즈 일치도 등의 한계로 인해 강건성과 일반화 능력이 부족한 문제점을 해결하고자 하였다.  
- 제안하는 Make-It-Poseable 프레임워크는 스킨닝 없이 스켈레톤 모션에 기반한 잠재 토큰 조작을 통해 효율적이고 정밀한 3D 캐릭터 포징을 가능하게 하였다.  

## Method  
본 방법은 소스 및 타겟 스켈레톤과 3D 메쉬를 각각 잠재 벡터 세트로 인코딩하고, 이들 간의 밀도 높은 스켈레톤 정보를 활용하여 잠재 공간에서 포즈 변형을 진행하는 latent posing transformer를 중심으로 한다.  
잠재 공간에서의 분류 가능한 대응 관계를 구축하고, 새로 노출되는 기하학적 구조를 처리하기 위한 적응형 완성 모듈과 잠재 공간 감독 전략을 도입하여 위상 변화와 세밀한 구조 보존을 지원한다.  
이는 빠른 추론 속도와 높은 기하학적 정확도 및 포즈 일치도를 실현하며, 파츠 교체 및 디테일 개선 등 3D 편집 응용에도 확장 가능하다.  

## Results  
본 모델은 기존 자동 리깅 및 포즈 조건 생성 모델 대비 메쉬 정확도, 상세도 보존, 속도 면에서 현저히 우수한 성능을 보였으며, 특히 비정상 포즈나 위상 오류가 포함된 입력에서 높은 강건성을 입증하였다.  

## Limitations  
현재 구현은 양족 보행 휴머노이드에 최적화되어 있으며, 비휴머노이드와 기타 관절 객체로의 확장은 대규모 애니메이션 데이터 수집 및 추가 연구가 필요하다.  

## Conclusion  
본 연구는 3D 휴머노이드 캐릭터 포징 문제를 잠재 공간 변환 문제로 새롭게 정의하여 고품질, 고속, 강건한 포즈 변형을 가능케 하는 최초의 feed-forward 3D 잠재 공간 포징 모델을 제시하였다.

# 9. [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16670)

## Introduction
- Goal: 본 연구는 G-buffer 데이터를 조건으로 하여 신경망 기반의 자기회귀적 신경 렌더링 프레임 생성을 구현하는 FrameDiffuser를 제안하는 것이다.  
- Motivation: 기존의 단일 이미지 확산 모델은 시간적 일관성을 확보하지 못하며, 전체 시퀀스를 미리 요구하는 비디오 모델은 인터랙티브 응용에 부적합한 한계가 있기 때문이다.  
- Contribution: FrameDiffuser는 ControlNet과 ControlLoRA를 결합한 이중 조건 부여 아키텍처와 3단계 자기조건 훈련 전략을 도입하여 환경 특화 모델 학습을 통해 시공간적 일관성과 고품질 광선 효과를 달성한다.  

## Method  
FrameDiffuser는 Stable Diffusion 1.5 기반으로 G-buffer 구조 정보는 ControlNet을, 이전 프레임 정보는 ControlLoRA를 통해 조건화하여 자기회귀적으로 프레임을 생성한다. 3단계 훈련법은 흑백 조도 조건에서 구조 학습을 시작해 점진적으로 시간적 조건과 자기조건을 도입하여 출동 에러 누적을 극복한다. 조도 지도는 이전 출력과 베이스컬러로부터 근사 조도를 계산하여 시간적 조명 안내를 제공한다.  

## Results  
FrameDiffuser는 Unreal Engine 5 환경별 특화 모델들을 대상으로 한 평가에서 기존 RGB↔X 대비 PSNR, SSIM이 대폭 개선되고 LPIPS는 감소하여 시간적 일관성과 지각 품질에서 우수함을 입증하였다.  

## Limitations  
환경 특화 모델 학습 전략으로 인해 하나의 모델이 여러 시각적 스타일을 포괄하지 못하며, 실시간 처리 속도는 1초당 1프레임 수준에 머문다.  

## Conclusion  
본 연구는 인터랙티브 영상 응용에 적합한 G-buffer 조건의 자기회귀 신경 렌더링 기법을 제안하여 기존 모델의 시간적 일관성 한계를 극복하고 환경 특화 학습을 통해 우수한 사진실사 품질을 달성하였다.

# 10. [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.16615)

## Introduction  
- Goal: 본 연구는 Diffusion Transformers에서 긴 토큰 시퀀스에 대해 계산 복잡도를 로그-선형으로 줄이는 학습 가능한 희소 어텐션 메커니즘을 제안하는 데 목적이 있다.  
- Motivation: 기존 Top-K 희소 어텐션 방법은 단일 레벨 설계로 인해 압축 토큰에 대해 여전히 제곱 복잡도 선택 비용이 발생하고, 시퀀스가 길어질수록 K 값을 키워야 하는 한계를 가진다.  
- Contribution: 본 논문에서는 계층적 구조를 도입한 Log-linear Sparse Attention(LLSA)를 통해 선택 및 어텐션 비용을 O(N²)에서 O(N log N)으로 낮추고, 전역 문맥을 유지하는 Hierarchical KV Enrichment를 도입하여 효율성과 성능을 동시에 달성하였다.  

## Method  
LLSA는 여러 계층의 토큰 압축과 계층적 Top-K 선택을 통해 희소 어텐션을 수행하며, 각 단계에서 이전 수준의 선택 인덱스를 활용한다. 전역 문맥 손실을 방지하기 위해 서로 다른 해상도의 키-값 토큰을 병합하는 Hierarchical KV Enrichment 기법을 도입하였다. 또한, 고성능 GPU 구현을 통해 희소 인덱스만을 사용하여 순전파와 역전파를 처리하여 계산 및 메모리 오버헤드를 줄였다.  

## Results  
LLSA는 256×256 픽셀 토큰 시퀀스에서 기존 Top-K 희소 어텐션 대비 최대 28.27배 빠른 추론 속도와 6.09배 빠른 학습 속도를 달성하며, FID 점수에서도 더 우수한 품질을 유지하였다.  

## Limitations  
본 연구는 LLSA의 설계 및 구현에 집중하였으나, 극도로 긴 시퀀스나 복잡한 영상 데이터에 대한 추가적인 확장성 검증은 필요하다.  

## Conclusion  
LLSA는 계층적 탐색과 KV 강화 메커니즘을 결합하여 Diffusion Transformers의 효율적인 장기 시퀀스 처리와 고품질 생성이라는 두 가지 목표를 동시에 달성하는 혁신적인 희소 어텐션 기법임을 입증하였다.

# 11. [Vibe Spaces for Creatively Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2512.14884)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.14884)

## Introduction
- 목표: 시각적 개념 간 가장 관련성 높은 속성인 ‘vibe’를 발견하고 이를 융합하여 창의적인 시각 이미지 혼합을 생성하는 방법을 제시하는 것이다.  
- 동기: 기존 기법은 개념 간 비선형적 경로를 탐색하거나 ‘vibe’를 포착하는 데 어려움을 겪어 의미 있고 일관된 융합 이미지 생성이 힘들었다.  
- 기여: Vibe Blending이라는 새로운 혼합 과제를 도입하고, CLIP 등의 특징 공간에서 비선형 경로를 학습하는 계층적 그래프 다양체인 Vibe Space를 제안하였다.  

## Method  
Vibe Space는 DINO와 CLIP 특징을 기반으로 다중 해상도의 그래프 라플라시안 고유벡터를 이용해 비선형 지오데식 경로를 학습한다. 학습된 저차원 잠재공간에서 의미 있는 ‘vibe’ 속성들을 인코딩하고, 속성별 대응 관계를 수립하여 혼합 경로를 보간 후 다시 CLIP 공간으로 디코딩한다. 이러한 프로세스는 추가적인 경량 MLP 인코더-디코더 훈련으로 가속화되며, IP-Adapter 확산 모델로 최종 이미지를 생성한다.  

## Results  
인간 평가 및 LLM 추론 결과, Vibe Space는 Gemini, GPT 등 최첨단 기법 대비 개념 간 경계가 뚜렷하고 난이도가 높은 이미지 쌍에서 창의성 및 일관성 면에서 우수한 혼합 이미지를 지속적으로 생성하였다.  

## Limitations  
복잡한 ‘vibe’를 포착하는 데는 긍정적인 성과가 있으나, 부적절한 ‘vibe’ 식별이나 과도한 관련성 해석 등 LLM 평가기의 한계와 혼합할 이미지 쌍 선정의 어려움은 남아있다.  

## Conclusion  
Vibe Space는 시각 개념 간 창의적 연결과 표현을 가능하게 하는 계층적 그래프 다양체 기반 기법으로, 인간이 선호하는 고차원적 의미 융합을 자동으로 구현하며 향후 평가 지표 및 데이터셋 확장 가능성을 제시한다.

# 12. [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.10953)

## Introduction
- Goal: 본 연구는 Normalizing Flows(NFs)에서 전통적으로 요구되던 완전 해석적 역함수의 필요성을 제거하고, 학습 가능한 역과정을 도입한 BiFlow 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 기존 NFs는 명시적 가역성 제약으로 인해 건축적 한계와 느린 샘플링 속도를 가지며, 최근 TARFlow는 트랜스포머와 자기회귀 흐름을 결합했으나 인과적 디코딩 병목 현상을 내포한다.  
- Contribution: BiFlow는 역과정을 별도의 모델로 학습하여 자유로운 구조와 손실함수를 가능하게 하고, ImageNet 실험에서 최고 수준의 생성 품질과 최대 2자릿수 속도 향상을 입증하였다.  

## Method  
BiFlow는 정방향 모델과 역방향 모델을 분리하여 비가역적 구조의 역 모델을 학습함으로써 완전 해석적 역 함수의 필요성을 없앴다. 역 모델 학습에는 단순한 재구성 손실과는 달리, 숨겨진 상태 정렬(hidden alignment) 전략을 적용하여 전체 변환 경로에서 풍부한 지도 신호를 사용한다. 또한 TARFlow의 점수 기반 노이즈 제거 단계를 학습된 노이즈 제거기로 통합하여 추론 효율성을 획기적으로 개선하였다.  

## Results  
ImageNet 256×256 클래스 조건부 생성에서 BiFlow-B/2는 improved TARFlow-XL/2 대비 FID 점수 2.39를 기록하며 최대 224배 이상의 샘플링 속도 향상을 달성하여 NF 기반 모델 중 최고 및 단일 함수 평가(1-NFE) 모델과 경쟁력 있는 성능을 보였다.  

## Limitations  
BiFlow는 VAE 디코더 단계에서 여전히 상당한 연산 비용이 소요되어 전체 생성 파이프라인의 병목으로 작용하였다.  

## Conclusion  
본 연구는 전통적 NF의 완전 해석적 역함수 요구를 비판하며, 학습 가능한 역 모델을 도입해 NFs의 생성 속도와 품질을 동시에 향상시킨 새로운 양방향 흐름 생성 모델을 제시하였다.

# 13. [Sharing State Between Prompts and Programs](https://arxiv.org/abs/2512.14805)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.14805)

## Introduction
- Goal: 자연어 코드와 정형 프로그래밍 언어 간의 상태 공유를 가능하게 하는 새로운 프로그래밍 추상화인 공유 프로그램 상태(shared program state)를 제안하는 것이다.  
- Motivation: 기존에는 자연어 코드와 정형 코드 간 상호 운용을 위해 프로그래머가 프로그램 상태의 수동적 전송을 직접 구현해야 했으며, 이는 개발 효율성을 저해하였다.  
- Contribution: 공유 프로그램 상태 추상화를 정의하고 이를 자연어 함수 인터페이스로 형식화하며, 이를 구현한 NIGHTJAR 시스템을 통해 자연어 코드가 파이썬 프로그램 상태를 직접 읽고 쓰며 제어 흐름까지 조작할 수 있음을 보였다.  

## Method  
공유 프로그램 상태에서는 자연어 코드가 변수, 힙 객체, 제어 상태를 공유하여 프로그래밍 언어의 상태를 직접 조작하도록 하였다. 이를 위해 효과와 핸들러 기반의 자연어 함수 인터페이스를 설계하여 자연어 코드가 프로그램 상태에 접근하는 메커니즘을 형식화하였다. NIGHTJAR는 파이썬을 호스트 언어로 사용하여 공유 프로그램 상태 추상화를 구현하고, 자연어 코드를 LLM 에이전트가 해석하여 변수 조작 및 제어 흐름 변경 등의 효과를 발생시키면 핸들러가 이를 처리하는 구조이다.  

## Results  
NIGHTJAR 프로그램은 수동 구현 대비 평균 39.6% 코드 감소와 +4~19%의 정확도 향상을 달성하였으나, 실행 시간은 0.4~4.3배 증가하는 성능 트레이드오프가 나타났다.  

## Limitations  
공유 프로그램 상태 접근으로 인해 자연어 코드가 메모리에 직접 읽기·쓰기가 가능하여 기존 분리된 시스템 대비 안전성과 보안 위험이 증가하는 한계가 존재한다.  

## Conclusion  
본 연구는 자연어 코드를 정형 프로그램 내에서 실행하며 프로그램 상태를 공유할 수 있는 새로운 프로그래밍 모델을 제안 및 구현하여 자연어와 형식 코드 간 상호운용성의 효율성과 정확성을 향상시켰음을 입증하였다.
