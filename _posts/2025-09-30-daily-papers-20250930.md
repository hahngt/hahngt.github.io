---
layout: post
title: "Daily Papers — 2025-09-30"
date: 2025-09-30 08:15:00
tags: [papers, hugginface]
categories: []
---

# 1. [SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention](https://arxiv.org/abs/2509.24006)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24006)

## Introduction

- Goal: 본 연구의 목표는 확산 변환기(Diffusion Transformer, DiT) 모델에서 주의(attention) 연산의 계산 복잡도를 크게 줄이면서 품질 저하 없이 가속화하는 방법을 제안하는 것이다.
- Motivation: 기존의 희소(sparse) 주의와 선형(linear) 주의 방법들은 DiT, 특히 비디오 생성에서 각각 고유의 한계로 인해 효율성과 품질을 동시에 만족시키지 못하는 문제점이 존재한다.
- Contribution: 본 논문은 주의 가중치를 중요도를 기준으로 세 가지 범주(중요, 경계, 무시)로 분류하고, 중요 가중치에는 희소 주의를, 경계 가중치에는 선형 주의를 적용하는 SLA(Sparse-Linear Attention)라는 통합 가중치 기반 학습 가능한 하이브리드 주의 메커니즘을 제안하였다.

## Method

SLA는 attention 가중치를 블록 단위로 분류하여 상위 일부 중요 가중치를 블록 희소 FlashAttention을 통해 정확하게 계산하고, 경계 가중치는 선형 주의로 근사하며 무시 가중치는 계산을 생략한다. 두 연산은 단일 GPU 커널에 융합되어 전·후방 연산을 지원하며, 모델을 소수의 파인튜닝 단계로 미세 조정하여 효율성과 품질 간의 균형을 달성한다. 이러한 분류 기준과 계산 방식의 결합은 희소성과 저차원 근사 방식의 장점을 동시에 활용하여 기존 방법 대비 주의 연산 비용을 획기적으로 줄인다.

## Results

Wan2.1-1.3B 비디오 생성 모델에서 SLA는 95%의 주의 연산 비용 절감과 13.7배 GPU 커널 속도 향상, 그리고 2.2배의 전체 영상 생성 속도 향상을 이루면서도 생성 품질 저하가 없음을 실험적으로 검증하였다.

## Limitations

선형 주의 부분의 근사 효과가 모델에 최적화되기 위해서는 최소한의 파인튜닝 과정이 필수적이며, 완전한 무학습 적용은 어려웠다.

## Conclusion

SLA는 희소 주의와 선형 주의를 적절히 융합하고 중요도 기반 가중치 계산 배분으로 DiT 모델의 주의 연산을 효율적으로 가속화하여, 품질 유지와 계산 비용 절감이라는 두 마리 토끼를 동시에 달성하는 효과적인 접근법임을 입증하였다.

# 2. [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22220)

## Introduction

- Goal: 본 논문은 잡음 환경에서도 의미적 안정성을 보장하는 견고한 음성 토크나이저인 StableToken을 제안하는 것을 목표로 한다.
- Motivation: 기존 의미 기반 음성 토크나이저는 높은 신호대잡음비(SNR)에서도 음향적 변동에 매우 민감하여 출력 토큰이 크게 변하며, 이는 하위 LLM 학습에 큰 부담으로 작용한다.
- Contribution: StableToken은 다중 분기 구조와 비트 단위 투표 기반 합의 메커니즘을 도입하여 토큰 안정성을 획기적으로 향상시키고, 이를 통해 다양한 잡음 조건에서 유닛 편집 거리(UED)를 대폭 감소시켰다.

## Method

StableToken은 단일 경로 기반의 기존 양자화 구조 취약점과 원거리 감독 신호의 한계를 극복하기 위해, Voting-LFQ라는 다중 분기 병렬 양자화 모듈과 노이즈 인지 합의 학습 전략을 결합하였다. 각 분기는 독립적으로 이진 벡터를 생성하며, 비트 단위 다수결 투표를 통해 견고하고 오류 보정이 가능한 최종 토큰을 산출한다. 학습 시에는 전체 분기 중 일부에 잡음이 섞인 입력을 주고 합의 손실로 표현 일관성을 유지하여 안정적인 표현 학습을 유도한다.

## Results

StableToken은 FLEURS 벤치마크 내 다양한 합성 및 실제 잡음 환경에서 평균 UED를 10.17%로 줄임으로써 최고 수준의 잡음 견고성을 달성하였고, 이로 인해 ASR, 감정 인식, 음성 합성 등 하위 SpeechLLM의 성능을 다방면에서 크게 향상시켰다.

## Limitations

현재 StableToken은 다중 분기 수 증가에 따른 연산 복잡도 상승 문제와 일부 극단적 잡음 조건에서의 한계가 존재한다.

## Conclusion

StableToken은 잡음에 강인한 다중 분기 및 합의 기반 토크나이저 구조와 대응하는 학습 전략으로 의미적 안정성을 크게 개선하여 내구성 높은 SpeechLLM 구축에 핵심적인 기여를 하였다.

# 3. [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24014)

## Introduction

- 본 논문의 목표는 확산 언어 모델(Diffusion Language Models, DLMs)의 높은 추론 지연 문제를 해결하기 위한 희소(attention) 어텐션 방식을 제안하는 것이다.
- 기존 DLM들은 문맥 길이에 따른 쿼리-키 쌍의 2차 복잡도 때문에 추론 속도가 느리며, 이런 복잡도를 줄이기 위해서는 관련성 높은 연결만 남기는 희소 어텐션이 필요하다.
- 본 연구는 DLM에 특화된 희소 어텐션 방법인 SparseD를 제안하며, 이는 헤드별 주의 패턴을 한번 계산 후 재사용하고 초기 단계에서는 전체 어텐션을 적용해 생성 품질 저하를 방지한다는 점에서 기여한다.

## Method

SparseD는 DLM의 고유한 특성을 반영해 헤드별로 중요 쿼리-키 쌍을 블록 단위로 선별하고, 초기 확산 단계에는 전체 어텐션을 수행한 후 선택된 희소 패턴을 이후 단계에서 재사용한다. 이를 통해 모든 단계에서 희소 패턴을 재계산하는 비용을 줄이고 생성 품질의 저하를 방지한다. 또한 프리필과 생성 토큰을 분리하여 개별적으로 선정함으로써 주요 토큰에 대한 관심도를 균형 있게 유지한다.

## Results

SparseD는 64k 문맥 길이, 1,024 확산 단계 조건에서 FlashAttention 대비 최대 1.50배 가속을 달성하면서도 원본 모델과 거의 동일한 정확도를 유지하였다.

## Limitations

초기 확산 단계에 희소 어텐션을 적용할 경우 생성 품질이 크게 저하되므로 이를 완전히 해결하지 못했다.

## Conclusion

SparseD는 DLM의 헤드별 희소성, 단계 간 패턴 일관성, 초기 단계 생성 중요성 등의 특징을 반영하여 효율적이고 손실 없는 가속화를 실현하는 실용적인 희소 어텐션 기법임이 입증되었다.

# 4. [VGGT-X: When VGGT Meets Dense Novel View Synthesis](https://arxiv.org/abs/2509.25191)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.25191)

## Introduction

- Goal: 본 연구는 3D Foundation Models(3DFMs)를 밀집된 Novel View Synthesis(NVS)에 적용하는 문제를 다룬다.
- Motivation: 기존 NVS 기법들은 정확한 3D 속성 및 카메라 자세에 의존하며, 이의 획득이 느리고 불안정하여 대규모 이미지에 적용이 어려웠다.
- Contribution: VGGT-X라는 메모리 효율적 구현, 적응적 전역 정렬, 그리고 견고한 3D Gaussian Splatting(3DGS) 학습 방식을 도입하여 COLMAP 초기화 없이도 밀집 NVS와 자세 추정에서 최첨단 성능을 달성하였다.

## Method

VGGT의 중복 피처 제거와 저정밀 연산 적용으로 1,000장 이상의 이미지에 대해 효율적 추론이 가능하도록 하였다.  
에피폴라 제약 기반 적응적 전역 정렬 기법으로 VGGT 출력의 자세 정밀도를 향상시켰으며, MCMC-3DGS 및 잔여 자세 공동 최적화로 초기 자세 노이즈에 강한 3DGS 학습을 수행하였다.  
또한 고신뢰도 대응점을 통한 점군 초기화 전략으로 복원 품질을 개선하였다.

## Results

MipNeRF360, Tanks and Temple, CO3Dv2 등의 벤치마크에서 VGGT-X는 COLMAP 초기화 기반 기존 기법과 유사한 수준의 신뢰도와 렌더링 품질을 보이며, 자세 추정과 COLMAP 자유 환경에서 최첨단 성능을 달성하였다.

## Limitations

COLMAP 초기화 기반 모델 대비 테스트 세트에서 다소 과적합 현상과 함께 자세 정확도가 부족하여 일반화 능력 개선이 필요한 한계가 존재한다.

## Conclusion

본 연구는 3DFMs의 밀집 NVS 적용을 가로막는 계산 자원과 예측 정확도 문제를 VGGT-X로 해결하고, COLMAP 없이도 대용량 밀집 장면에서 고품질 NVS를 가능하게 하는 방향성을 제시하였다.

# 5. [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24663)

## Introduction

- Goal: 본 논문은 단일 모델이 짧은 시퀀스부터 긴 시퀀스까지 효율적으로 적응할 수 있는 dense-sparse 전환 가능한 어텐션 메커니즘인 InfLLM-V2를 제안하는 것을 목표로 한다.
- Motivation: 기존의 Transformer 기반 자기어텐션은 긴 시퀀스 처리 시 심각한 계산 및 메모리 병목을 겪으며, 기존 학습 가능한 희소 어텐션은 추가 파라미터와 학습 불안정 문제로 인해 짧은 시퀀스에서 긴 시퀀스로의 전이 적응에 어려움이 존재한다.
- Contribution: InfLLM-V2는 기존의 dense 어텐션 파라미터를 재사용하고, 짧은 시퀀스에는 dense 어텐션을, 긴 시퀀스에는 sparse 어텐션을 효율적으로 전환하며, 실제 가속을 위한 고성능 구현을 함께 제시한다.

## Method

InfLLM-V2는 단일 키-값(KV) 투영 파라미터를 공유하여 dense와 sparse 어텐션 간 아키텍처 불일치를 제거한다. 또한 두 가지 sparse 패턴(Selected Attention과 Sliding Attention)을 통합해 단일 sparse 어텐션 모듈을 구성하며, 불필요한 압축 어텐션 출력은 제거한다. 효율적 블록 선택을 위해 FlashAttention 기반 최적화 및 로그-합-지수(LSE) 근사 기법을 도입하여 메모리 입출력 병목을 완화하였다.

## Results

InfLLM-V2는 긴 컨텍스트 이해 및 긴 사슬추론(chain-of-thought) 과제에서 dense 어텐션 대비 최대 4배 빠른 속도를 보이면서 성능은 각각 98.1%와 99.7%에 달하는 우수한 결과를 기록하였다.

## Limitations

InfLLM-V2는 피드포워드 네트워크(FFN) 레이어 가속을 포함하지 않아 전체 모델 가속 한계가 존재한다.

## Conclusion

InfLLM-V2는 기존 학습 가능한 sparse 어텐션의 한계점을 극복하고 짧은 시퀀스에서 긴 시퀀스까지 효율적이고 안정적으로 적응 가능한 실용적이고 강력한 dense-sparse 전환 어텐션 프레임워크를 제안하였다.

# 6. [Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding](https://arxiv.org/abs/2509.23050)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.23050)

## Introduction

- Goal: 본 연구는 대형 시각-언어 모델(LVLMs)의 언어 우선성(language prior)을 내부 표현 변화를 통해 체계적으로 이해하고 정량화하는 것을 목표로 한다.
- Motivation: 기존 연구들은 입력-출력 관계 분석에 의존하여 LVLM이 시각 정보를 언제 어떻게 활용하는지를 내재적으로 해석하는 데 한계가 존재하였다.
- Contribution: 본 논문은 LVLM 레이어별 내재 표현 연쇄(chain-of-embedding)를 대비하는 기법을 통해 시각 통합 지점(VIP)을 발견하고, 총 시각 통합량(TVI) 지표를 제안하여 언어 우선성의 강도를 정량적으로 평가하였다.

## Method

LVLM 내부의 각 레이어에서 시각 입력을 포함한 임베딩과 시각 정보가 제거된 임베딩의 차이를 계산하여 시각 통합 지점(VIP)을 탐지하였다.  
VIP 이후 레이어들에서 임베딩 간 거리 평균을 총 시각 통합량(TVI)으로 정의하여, 모델이 시각 정보를 얼마나 효과적으로 통합하는지 평가하였다.  
이를 기반으로 LVLM별, 샘플 단위로 언어 우선성의 강도를 구체적이고 정밀하게 진단할 수 있는 프레임워크를 구축하였다.

## Results

54개의 실험 설정(9개 LVLMs × 6개 데이터셋)에서 VIP의 보편적 존재가 확인되었고, TVI 지표가 기존 시각 어텐션 또는 출력 차이 기반 지표보다 언어 우선성 예측 및 시각적 추론 성능과 높은 상관관계를 보였다.

## Limitations

본 연구는 내부 표현 기반 평가에 집중하였으나, 실제 활용 시 LVLM의 다변량 행동과 복합적 데이터 분포에서의 한계 및 확장성 검증이 추가 검토되어야 한다.

## Conclusion

본 연구는 LVLM 내부 표현 역학을 통해 언어 우선성의 실체를 규명하고 정량화하는 새로운 이론적·실험적 도구를 제시하여 향후 신뢰성 높은 다중모달 모델 개발에 기여한다.

# 7. [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.21875)

## Introduction

- Goal: 본 연구는 Retrieval-Augmented Generation(RAG) 시스템 내 대형 언어 모델(LLM)의 환각(hallucination)을 외부 문맥과 내부 지식의 활용 신호를 통해 검출하는 새로운 프레임워크 LUMINA를 제안하는 것이다.
- Motivation: 기존의 RAG 기반 LLM에서도 충분하고 올바른 문맥이 제공되었음에도 환각이 발생하는 문제는 내부 지식과 외부 문맥 활용의 불균형에서 기인하며, 기존 검출 방법은 과도한 하이퍼파라미터 조정으로 일반화에 한계가 있었다.
- Contribution: LUMINA는 외부 문맥 활용을 분포 간 거리로, 내부 지식 활용을 트랜스포머 층의 토큰 예측 변화로 정량화하며, 통계적 검증 프레임워크를 도입하여 높은 성능과 강건성을 보여 기존 방법 대비 최대 13% 향상된 AUROC를 기록하였다.

## Method

LUMINA는 외부 문맥 활용도를 랜덤 문서와의 토큰 확률 분포 차이를 통해 최대 평균 불일치(Maximum Mean Discrepancy)로 측정한다. 내부 지식 활용도는 각 층의 출력 토큰 예측 확률 변화율인 정보처리율을 기반으로 산출한다. 이 두 신호를 결합한 환각 점수를 통계적 가설 검정을 통해 검증하였다.

## Results

다양한 RAG 환각 벤치마크 및 4종 오픈소스 LLM 실험에서 LUMINA는 일관되게 높은 AUROC 및 AUPRC 성능을 보였으며, 특히 HalluRAG 데이터셋에서 기존 활용 기반 방법 대비 최대 13% 향상된 AUROC를 나타냈다.

## Limitations

LUMINA는 일부 모델과 노이즈가 높은 문서 환경에서 성능 저하가 관찰되어 완전한 무조정 환경에서의 보편적 적용에는 추가 연구가 필요하다.

## Conclusion

본 연구는 외부 문맥과 내부 지식 활용 신호의 정량화를 통해 RAG 시스템 내 LLM 환각을 효과적이고 실용적으로 검출하는 LUMINA를 제안하였으며, 이는 환각 감소와 신뢰성 향상에 기여할 것으로 기대된다.

# 8. [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.25149)

## Introduction

- Goal: 본 논문은 NVFP4 4비트 플로팅포인트 형식을 활용하여 대규모 언어 모델(LLM)의 사전학습을 안정적이고 정확하게 수행하는 방법을 제안하는 데 목적이 있다.
- Motivation: 대규모 언어 모델의 사전학습에는 막대한 연산량과 에너지가 요구되어 연산 효율성과 메모리 활용을 개선할 필요가 있다.
- Contribution: NVFP4 형식과 이에 최적화된 훈련 기법을 통합하여 120억 매개변수 모델을 10조 토큰으로 학습하는 장기 대규모 4비트 정밀도 학습의 성공 사례를 제시하였다.

## Method

NVFP4는 16개 요소 단위의 블록 규모 축소와 더 정밀한 FP8 및 FP32 두 단계 스케일링을 통해 데이터 표현 정확도를 높인다. 제안된 훈련법은 수치 민감층 혼합정밀 유지, 랜덤 하다마드 변환을 통한 이상치 분산, 2D 블록 스케일링으로 전후향 일관성 확보, 확률적 라운딩으로 그라디언트 편향 제거를 포함한다.

## Results

NVFP4 기반 4비트 사전학습법은 FP8 대비 손실과 다운스트림 작업 정확도에서 근접한 성능을 나타냈으며, MMLU 프로 5-샷 정확도가 각각 62.58%와 62.62%로 거의 동일하였다.

## Limitations

현재 훈련법은 일부 민감층에서 여전히 높은 정밀도를 요구하여 완전한 4비트 양자화 적용에 한계가 존재한다.

## Conclusion

NVFP4와 본 논문의 훈련 전략은 다조 토큰 스케일 대규모 모델의 안정적이고 효율적인 4비트 사전학습을 가능하게 하는 중요한 진전이다.

# 9. [Hyperspherical Latents Improve Continuous-Token Autoregressive Generation](https://arxiv.org/abs/2509.24335)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24335)

## Introduction

- Goal: 본 연구는 continuous-token autoregressive (AR) 이미지 생성 모델의 핵심 문제인 분산 붕괴를 해결하기 위해 scale-invariant한 입력과 출력을 갖는 모델을 제안하는 것이다.
- Motivation: 기존 continuous-token AR 모델은 VAE 잠재공간 내 이질적인 분산 문제가 AR 디코딩 과정에서 증폭되어, 특히 classifier-free guidance(CFG) 사용 시 분산 붕괴 현상이 발생하는 한계를 지녔다.
- Contribution: 본 연구는 hyperspherical VAE를 활용하여 모든 AR 입력과 출력이 고정 반지름의 hypersphere 상에 존재하도록 제약함으로써 분산 붕괴를 방지하는 SphereAR 모델을 제안하였다.

## Method

SphereAR는 hyperspherical VAE(S-VAE)로 이미지 데이터를 고정된 ℓ2 노름을 갖는 방향성(latent direction) 잠재토큰으로 인코딩하고, 이를 causal Transformer와 토큰 단위 확산 헤드로 다음 토큰 분포를 autoregressive하게 예측한다. AR 모델의 출력과 CFG 적용 후의 예측값 모두 다시 hypersphere로 정규화하여 scale 변동이 누적되는 것을 차단한다. 또한, S-VAE에서 vMF 또는 Power Spherical 분포를 사용해 효율적이고 안정적인 latent 샘플링과 학습이 가능함을 이론적으로 및 실험적으로 입증하였다.

## Results

SphereAR는 ImageNet 256×256 클래스 조건부 생성에서 943M 파라미터로 FID 1.34를 기록하며 기존 AR 모델과 동급 이상 크기의 확산 및 마스크 생성 모델을 능가하는 우수한 성능을 달성하였다.

## Limitations

Gaussian posterior에 사후 정규화를 적용하는 방식은 이론적으로 S-VAE의 hyperspherical posterior에 비해 완전하지 않으며, 본 연구에서 제안한 hypersphere 제약과 일치하는 학습 목표를 완전히 구현하지 못한다는 한계가 있다.

## Conclusion

본 연구는 continuous-token AR 이미지 생성 분야에서 hyperspherical latent 토큰을 통한 scale-invariant 설계가 분산 붕괴 문제를 해결하며, 파라미터 효율성과 성능 측면에서 확산 및 마스크 기반 최첨단 모델을 능가하는 새롭고 강력한 접근법임을 보였다.

# 10. [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22570)

## Introduction

- Goal: 본 논문은 인간과 인공지능 간의 상호작용을 위한 통합 토큰 기반 다중모달 인터랙티브 코딩 프레임워크인 UniMIC을 제안하는 데 목적이 있다.
- Motivation: 기존 압축기법들은 단일 모달, 일방향 통신에 최적화되어 반복적 압축–전송–복원 과정에서 품질 저하가 발생하는 문제점이 존재한다.
- Contribution: UniMIC은 토큰 단위의 압축 전송 방식을 채택하여 초저비트율에서도 반복 압축 손실 없이 멀티모달 상호작용을 효율적으로 지원하는 새로운 통신 패러다임과 경량 트랜스포머 엔트로피 모델을 개발하였다.

## Method

UniMIC은 텍스트와 이미지 등 다양한 모달리티 입력을 토큰화하여 경량 트랜스포머 기반 엔트로피 모델을 활용해 토큰 시퀀스의 중복성을 최소화하고 효율적 압축을 수행한다. 에지 디바이스는 필요한 토큰 부분 집합만을 전송하며, 클라우드 AI는 해당 토큰을 기반으로 생성 또는 이해 작업을 수행하여 토큰화된 결과만을 다시 전송한다. 이러한 반복적 토큰 교환을 통해 누적 압축 손실을 제거하고 의미론적 완전성을 유지하는 비트스트림 전송 방식을 구현하였다.

## Results

UniMIC은 텍스트-이미지 생성, 텍스트 기반 이미지 보간 및 확장, 시각 질문응답 등 다양한 작업에서 전통적인 픽셀 기반 압축 기법 대비 실질적인 비트율 절감과 높은 복원 품질, 그리고 의미적 일관성을 유지함을 실험적으로 입증하였다.

## Limitations

정보 부족.

## Conclusion

UniMIC은 인간-인공지능 협업을 위한 차세대 멀티모달 인터랙티브 통신의 실용적이고 혁신적인 프레임워크로서, 초저비트율에서도 고품질 상호작용을 가능하게 하는 새로운 토큰 기반 코딩 패러다임이다.

# 11. [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22518)

## Introduction

- Goal: 본 연구는 대형 언어 모델(LLM)의 복잡한 추론 과정을 이해하고 추론 실패 원인을 해석하는 통합적인 기하학적 분석 프레임워크인 REMA를 제안하는 데 목적이 있다.
- Motivation: 기존 연구들은 추론 실패 분석에서 특정 오류 유형이나 통제된 입력 대비에 의존하는 한계를 가지며, 다양한 자연 발생 오류를 포괄하는 일반적이고 정량적인 실패 원인 분석 도구가 부재하였다.
- Contribution: Reasoning Manifold 개념을 도입하여 LLM 내부 표현의 기하학적 구조를 규명하고, REMA를 통해 오류 표현과 정상 표현 간의 기하학적 편차를 정량화 및 국소화하여 추론 실패를 체계적으로 분석하는 방법론을 개발하였다.

## Method

REMA는 추론 데이터셋의 정오답 샘플에서 추출한 레이어별 내부 표현을 평균 집약하여 표현 공간 내 'Reasoning Manifold'를 근사한다. 각 오류 표현과 정상 표현 간 k-최근접 이웃 거리 기반 편차를 계산하여 오류의 심각도를 정량화하고, 층별 편차 변화를 통해 추론 오류가 발생하는 층을 국소화한다. 또한, 분류 모델을 활용해 오류와 정상 표현 간의 구분 가능성을 평가한다.

## Results

다양한 LLM 및 멀티모달 LLM과 여러 추론 과제에서 연구된 Reasoning Manifold는 저차원 구조를 형성하며, 오류 표현이 명확히 정상 표현과 기하학적으로 구분되어 REMA가 실패 원인 분석에 효과적임을 입증하였다.

## Limitations

본 연구에서는 제안된 REMA 프레임워크의 적용과 분석에 집중하였으나, 오류 수정 또는 모델 개선을 위한 직접적 개입 방법에 대해서는 다루지 않았다.

## Conclusion

REMA 프레임워크는 LLM 내 추론 실패를 기하학적 편차 관점에서 해석하고 국소화함으로써 블랙박스 모델 내부 계산 과정을 심층적으로 이해하고 진단하는 새로운 방향을 제시한다.

# 12. [BPMN Assistant: An LLM-Based Approach to Business Process Modeling](https://arxiv.org/abs/2509.24592)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24592)

## Introduction

- Goal: 본 논문은 자연어를 활용하여 BPMN 다이어그램을 생성하고 수정할 수 있는 LLM 기반 도구인 BPMN Assistant를 제안하는 것을 목표로 한다.
- Motivation: BPMN 모델링의 복잡성과 비전문가의 접근성 부족, 그리고 IT와 비즈니스 사용자 간의 의사소통 격차로 인한 비효율 문제를 해결하기 위함이다.
- Contribution: 본 연구는 JSON 기반의 구조화된 BPMN 표현 방식을 도입하여 모델 수정 정확도를 향상시키고, 자연어 명령에 의한 BPMN 다이어그램 편집 기능을 효과적으로 구현하였음을 보고한다.

## Method

BPMN Assistant는 Python 백엔드, BPMN 레이아웃 서버, Vue.js 프론트엔드로 구성되며, 사용자 입력처리와 LLM과의 연동을 통해 JSON 형식의 BPMN 다이어그램을 생성 및 수정한다.  
LLM의 출력인 JSON은 BPMN XML로 변환되어 시각화되며, 고유한 편집 함수 집합을 통해 자연어 기반의 모델 수정 작업을 수행한다.  
시스템은 다양한 AI 모델을 통합하고, BPMN의 주요 요소(업무, 게이트웨이, 이벤트)를 지원하며, 그래프 편집 거리(GED, RGED)를 활용해 구조적 정확도를 평가한다.

## Results

JSON 표현 기반의 BPMN Assistant는 XML 처리 방식에 비해 유사도는 비슷하지만, 편집 성공률과 처리 속도에서 현저히 우수한 성능을 보였다.

## Limitations

지원되는 BPMN 요소가 제한적이며, LLM 품질 의존도 및 높은 입력 토큰 소모는 여전히 한계로 남아있다.

## Conclusion

BPMN Assistant는 LLM을 활용해 자연어로 BPMN 생성 및 수정의 진입 장벽을 낮춤으로써 비전문가도 효과적인 프로세스 모델링에 참여할 수 있는 가능성을 제시한다.

# 13. [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.18582)

## Introduction

- 본 연구의 목표는 전문 사진작가와 유사한 수준으로 이미지의 미적 요소를 인지하고 평가할 수 있는 멀티모달 대형언어모델(MLLM)을 개발하는 것이다.
- 기존 MLLM은 일반적인 시각 인식에 비해 미적 이해에서는 데이터와 모델 구조 측면에서 한계가 있어 실제 사진 전문가 수준의 분석 및 평가가 어렵다는 문제를 가지고 있었다.
- 본 논문은 대규모의 전문 사진가 및 애호가 토론에서 추출한 PhotoCritique 데이터셋과 언어 안내형 다중 시각 융합 메커니즘을 갖춘 PhotoEye 모델, 그리고 전문성 높은 PhotoBench 평가 벤치마크를 제안한다.

## Method

- PhotoCritique 데이터셋은 45만 개 이상의 이미지와 260만 쌍 이상의 미적 지시문 쌍으로 구성되며, 다양한 사진 장르와 전문적인 미적 토론을 포함한다.
- PhotoEye 모델은 다양한 비전 인코더에서 추출한 시각 특징을 언어 지시에 맞추어 다중 시각 융합하는 구조로, 미적 이해를 위한 세밀한 시각 요소 학습이 가능하다.
- PhotoBench는 전문가 커뮤니티에서 도출한 284개의 세부 사진 촬영 주제를 포함하는 전문적 다지선다형 질문들로 MLLM의 미적 시각 이해 능력을 평가한다.

## Results

- PhotoEye는 기존 오픈소스 MLLM 및 GPT-4o 대비 실제 사진 평가 상황에서 미적 요소 인지와 판단 정확성에서 현저한 성능 우위를 보였다.

## Limitations

- 정보 부족

## Conclusion

- 본 연구는 방대한 전문가 데이터와 다중 시각 융합 모델을 통한 미적 시각 이해 강화를 통해 MLLM의 사진 미적 평가 능력을 크게 향상시켰음을 입증하였다.

# 14. [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.24988)

## Introduction

- Goal: 본 연구는 대형 언어 모델(LLM)의 출력 정답 여부를 정확히 예측하고 보정된 신뢰도를 제공하는 모델-불가지론적 정합성 예측자(Correctness Model)를 학습하는 방법을 제안하는 데 있다.
- Motivation: 기존 방법들은 LLM이 자신의 출력 정합성에 관한 독점적 정보를 갖는다고 가정하였으나, 본 연구는 이러한 자기 인식(self-knowledge)이 부족함을 실험적으로 증명하였다.
- Contribution: 다수 LLM의 역사적 정답 예측 패턴을 반영하여 모델 간 일반화가 가능한 범용 정합성 모델(Generalized Correctness Model, GCM)을 개발하고, 이를 통해 다양한 모델과 데이터셋에서 우수한 성능과 보정 정확도를 달성하였다.

## Method

다양한 LLM에서 생성된 질의-응답-정합성 데이터셋을 통합하여 Qwen3-8B 모델을 GCM으로 학습하였다.  
입력 변수로 질의, 답변 문장, 예측된 답변, 타겟 모델의 이름 여부 등 조건을 조작하며 정합성 예측에 미치는 영향을 분석하였다.  
추가로, 학습 없이 역사 정보를 포함시키는 문맥내 학습(in-context learning)과 사후 보정(post-hoc calibration) 방법을 탐구하였다.

## Results

GCM은 특정 모델만을 대상으로 학습된 정합성 모델(Specific Correctness Model, SCM)보다 평균 정확도를 2% 이상 높였고, 강력한 대규모 LLM의 자체 신뢰도보다 더 우수한 보정 신뢰도와 예측력을 보였다.

## Limitations

데이터셋 간 전이 성능은 모델 간 전이에 비해 제한적이며, 문맥내 학습 방식은 추론 비용이 크고 보정 오류가 상대적으로 높았다.

## Conclusion

LLM의 자기 인식 능력 부재에도 불구하고, 다양한 모델의 정답 예측 이력을 체계적으로 학습하는 GCM이 보다 일반화 가능하고 정확도 높은 정합성 예측 수단임을 입증하였다.

# 15. [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.22991)

## Introduction

- Goal: 생애 기록에 관한 다국어·다중모달 대규모 언어모델의 추론 능력을 평가하고 향상시키기 위한 ADAM 프레임워크를 제안하는 것이다.
- Motivation: 기존의 전통적 데이터와 모델이 전 세계 언어·문화적 다양성과 인물의 인지도 차이를 제대로 반영하지 못해, 전기적 사실 검증과 고차원적 추론에 한계가 존재하기 때문이다.
- Contribution: 400만 명 이상의 인물 정보를 포함하는 다국어·다중모달 AdamDB, Bloom 분류기반 인지적 평가도를 제공하는 AdamBench, 그리고 전기 분야 맞춤형 검색증강생성(AdamRAG) 시스템을 통합한 최초의 프레임워크를 개발하였다.

## Method

ADAM은 자동화된 파이프라인으로 인물 중심의 위키DB를 다국어·다중모달로 구축한 AdamDB를 중심 데이터베이스로 활용한다. AdamBench는 Bloom의 인지수준 분류에 기반하여 영어 및 원어로 구성된 다중선택 문제를 생성해 평가 기준을 확립한다. AdamRAG는 AdamDB에서 관련 정보를 검색해 LLM에 증거 기반 문맥을 제공함으로써 허위정보 생성과 다의성 문제를 감소시킨다.

## Results

검색증강생성 시스템인 AdamRAG는 모든 모델에서 사실성 정확도를 크게 향상시키며, 특히 인지 하위 수준과 비인기 인물에 대해 개방형 모델의 성능 개선 효과가 두드러졌다.

## Limitations

얼굴 이미지 등 다중모달 입력의 성능 향상 효과는 제한적이고 일관되지 않아, 고차원적 인지 추론에서는 여전히 해결해야 할 난제가 존재한다.

## Conclusion

ADAM은 다국어·문화적 다양성과 인지도 가중치를 포함한 통합적 전기 추론 평가 및 개선 프레임워크로, 향후 다중모달 융합과 형평성 기반 평가 프로토콜 개발에 기초를 제공한다.

# 16. [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.16538)

## Introduction

- 본 연구의 목표는 참조 캡션 없이도 영상 자막의 사실적 정확성을 평가하는 참조-프리 평가 프레임워크인 VC-Inspector를 개발하는 것이다.
- 기존 참조 기반 평가 방법이 다양한 영상 도메인에서의 평가에 비현실적이며, 사실적 오류를 제대로 검출하지 못하는 한계가 존재한다.
- 본 논문은 사실적 근거에 기반한 참조-프리 평가 모델과 이를 위한 데이터 생성 파이프라인, 그리고 영상 자막 평가에서 인간 평가와의 높은 상관관계를 입증한 새로운 평가자를 제안한다.

## Method

VC-Inspector는 대형 멀티모달 모델(Qwen2.5-VL)을 활용해 영상과 자막을 입력받아 1~5점의 품질 점수와 평가 근거 설명을 동시에 생성하도록 지침 튜닝되었다.  
학습을 위해 대형 언어 모델을 이용해 사실적 요소(객체, 동작)를 변형한 다양한 품질의 의사 캡션 데이터셋(ActivityNet-FG-It)을 합성하여 점수 및 설명을 할당하였다.  
이러한 지도 학습 과정에서 영상 자막의 사실적 오류를 민감하게 탐지하고 설명하는 능력을 습득하도록 하였다.

## Results

VC-Inspector는 VATEX-Eval, ActivityNet-FG-Eval, YouCook2-FG-Eval 등 영상 자막 데이터셋에서 기존 참조-프리 및 참조 기반 평가 지표들을 능가하는 인간 평가 상관성을 보였으며, 단일 프레임 영상으로 처리한 이미지 자막 데이터셋(Flickr8K-Expert, Flickr8K-CF)에서도 우수한 일반화 성능을 입증하였다.

## Limitations

본 연구에서는 객체 및 동작 관련 사실적 오류에 중점을 두었으나, 속성 대체나 시간적 일관성 등 보다 광범위한 오류 유형 평가는 아직 미흡하다.

## Conclusion

VC-Inspector는 참조 캡션 없이도 사실적 근거에 기반해 영상 자막의 품질을 정확하고 해석 가능하게 평가할 수 있는 실용적이고 확장 가능한 평가 도구임이 입증되었다.
