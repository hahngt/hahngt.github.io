---
layout: post
title: "Daily Papers — 2025-09-22"
date: 2025-09-22 08:15:00
tags: [papers, hugginface]
categories: []
---

# 1. [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)

## Introduction

- Goal: 본 논문은 시각적 이해와 생성 능력을 통합한 단순하고 확장 가능한 멀티모달 모델인 Manzano를 제안하는 데 목적이 있다.
- Motivation: 기존의 통합 모델들은 이해 및 생성 성능 간에 상충 현상이 존재하여, 두 능력을 모두 효과적으로 달성하는 데 어려움이 있었다.
- Contribution: Manzano는 혼합형 비전 토크나이저와 통합된 학습법을 통해 이러한 상충을 대폭 완화하고, 이해와 생성 작업 모두에서 최첨단 성능을 달성하였다.

## Method

Manzano는 하나의 공유 비전 인코더와 두 개의 경량 어댑터(연속 임베딩 생성용, 이산 토큰 생성용)를 활용하는 혼합형 비전 토크나이저를 사용한다.  
통합된 자기회귀형 언어 모델이 텍스트와 이미지 토큰의 고수준 의미를 예측하며, 보조 확산 디코더가 이미지 토큰을 픽셀로 변환한다.  
훈련은 텍스트 전용, 이미지 이해, 텍스트-이미지 생성 데이터를 혼합하여 세 단계(사전학습, 계속학습, 지도미세조정)로 수행된다.

## Results

Manzano는 3B 및 30B 모델 규모에서 텍스트 정보가 풍부한 벤치마크를 포함한 여러 이해 및 생성 평가에서 통합 모델 중 최고 또는 경쟁력 있는 성능을 기록하였다.

## Limitations

생성 이미지의 미학적 품질은 이미지 디코더 크기 확장 시 일부 감소하는 현상이 관찰되어 추가 연구가 요구된다.

## Conclusion

Manzano는 혼합형 토크나이저 설계와 통합 학습 전략을 통해 이해와 생성 간의 상충 문제를 극복하며, 확장 가능하고 우수한 멀티모달 성능을 입증하였다.

# 2. [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)

## Introduction

- Goal: 본 연구는 생성 모델링, 표현 학습, 분류 세 가지 핵심 머신러닝 문제를 하나의 통합 원리로 해결하는 Latent Zoning Network (LZN)을 제안하는 데 목적이 있다.
- Motivation: 기존의 최신 기법들은 각 문제에 대해 별도의 독립적인 접근법을 사용하며, 이로 인해 모델 간 협력이 어려운 한계가 존재한다.
- Contribution: LZN은 공유 가우시안 잠재 공간을 통해 여러 데이터 유형을 인코딩 및 디코딩하며, 다양한 머신러닝 작업을 하나의 프레임워크 내에서 효과적으로 수행함을 보였다.

## Method

LZN은 이미지, 텍스트, 레이블 등 각각의 데이터 타입에 대해 인코더와 디코더를 정의하고, 이들을 공유 잠재 공간의 서로 분리된 영역으로 매핑한다. 잠재 공간은 단순 가우시안 분포를 따르며, 플로우 매칭(flow matching) 기법을 활용해 각 데이터를 고유한 잠재 영역으로 변환하고, 서로 다른 데이터 타입 간 정렬을 위해 소프트 할당 방식을 도입한다. 이를 통해 LZN은 생성, 표현 학습, 분류 작업을 인코더-디코더 조합이나 단독 모듈로 처리할 수 있다.

## Results

LZN은 (1) Rectified Flow 기반 무조건적 이미지 생성에서 CIFAR10 FID를 2.76에서 2.59로 개선했고, (2) ImageNet에서 무감독 표현 학습 후 선형 분류를 통해 MoCo 대비 9.3%, SimCLR 대비 0.2% 높은 정확도를 내었으며, (3) CIFAR10에서 조건부 생성과 분류를 동시에 수행하며 최고 수준의 성능을 달성했다.

## Limitations

현재 LZN의 성능은 일부 최첨단 방법에 비해 격차가 존재하며, 확장성 및 다양한 데이터 타입 추가에 대한 추가 연구가 필요하다.

## Conclusion

LZN은 하나의 단일 잠재 공간을 기반으로 생성, 표현 학습, 분류 세 작업을 통합하는 새로운 원리를 제시하며, 다양한 머신러닝 문제 간 시너지 효과를 창출할 수 있는 유망한 프레임워크임이 입증되었다.

# 3. [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)

## Introduction

- Goal: 본 연구의 목표는 단일 입력 이미지로부터 고해상도 개인화 영상 생성을 수행하는 Lynx 모델을 제안하는 것이다.
- Motivation: 기존 개인화 영상 생성 기법들은 신원 유지와 시각적 자연스러움 사이의 균형을 맞추기 어렵고, 대규모 재학습에 제약이 많았기 때문이다.
- Contribution: Lynx는 ID-어댑터와 Ref-어댑터라는 두 개의 경량화 모듈을 도입하여 신원 정보의 충실한 보존과 시간적 일관성 및 시각적 사실성을 동시에 달성하였다.

## Method

Lynx는 공개된 Diffusion Transformer (DiT) 기반의 비디오 기초 모델을 활용하며, 신원 임베딩을 효과적으로 압축하는 Perceiver Resampler를 포함한 ID-어댑터와, 정지된 참조 경로에서 추출한 VAE 특징을 다층 교차 주의 메커니즘을 통해 통합하는 Ref-어댑터를 결합하였다. 두 어댑터는 신원 유지 성능을 크게 향상시키면서 기존 모델 전체를 재구성하거나 미세 조정하는 부담을 줄였다. 또한, 공간-시간적 프레임 패킹 및 점진적 이미지-비디오 훈련 전략을 적용하여 다양한 해상도와 길이의 데이터에 대한 학습 효율성을 확보하였다.

## Results

40명의 다양한 인물과 20개의 공정한 프롬프트로 구성된 800개 테스트 케이스 평가에서 Lynx는 세 가지 얼굴 인식 평가 지표 모두에서 최상위 신원 유사도를 달성하고, Gemini-2.5-Pro 기반 자동화 평가에서 프롬프트 일치도, 영상 미학, 전반적 영상 품질에서 뛰어난 성과를 보였다.

## Limitations

정보 부족.

## Conclusion

Lynx는 단일 이미지 기반 개인화 영상 생성 분야에서 고도의 신원 유지와 영상 품질을 동시에 실현하는 확장 가능하고 효율적인 어댑터 기반 프레임워크를 제시하여, 향후 다중 모달 및 다중 인물 개인화 연구의 기초를 마련하였다.

# 4. [Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems](https://arxiv.org/abs/2509.13989)

## Introduction

- Goal: 본 연구의 목표는 자연어 지시어에 따라 생성된 음성의 스타일과 청취자의 지각 간 차이, 즉 Instruction-Perception Gap을 정량적으로 분석하는 것이다.
- Motivation: 기존 텍스트-음성 변환(TTS) 시스템은 저수준 음향 제어나 전문 레이블이 필요하나, ITTS는 자연어를 통해 직관적이고 유연한 음성 생성 제어를 가능하게 하므로, 이에 대한 신뢰성 평가가 필요하다.
- Contribution: 본 연구는 정도 부사와 감정 강도 척도 등 세밀한 표현 차원을 포함한 새로운 평가 프레임워크와 대규모 인간 평가 데이터셋(E-VOC 코퍼스)을 제안하였다.

## Method

다섯 개 대표 ITTS 모델(Parler-TTS, PromptTTS++, GPT-4o-mini-TTS, UniAudio)을 대상으로 아동부터 노인까지 연령대, 단어 강조, 감정 강도(부사 및 감정형용사)를 포함한 네 가지 제어 차원에서 인간 평가와 객관적 음향 특성 분석을 병행하였다.  
인간 평가자는 미국 원어민으로 엄격한 품질관리 하에 주관적 지각 척도를 제공하였고, 각 음성 샘플에 대해 다중 평가를 수집하였다.  
이를 통해 ITTS 모델의 명령 이행 능력과 청취자 지각의 일치도를 심층적으로 평가하였다.

## Results

모델 간 비교에서 GPT-4o-mini-TTS가 정도 부사 및 감정 강도 명령을 음향적으로 가장 신뢰성 있게 반영하고, 청취자 지각과도 일관된 정렬을 보였으나, 모든 모델에서 아동 및 노인 음성 생성은 어려웠으며 단어 강조 제어 역시 불안정하였다.

## Limitations

대부분 모델이 미세한 감정 및 강조 조절에 한계를 보였고, 특히 아동과 노인 음성 표현에서 지각 일치도가 현저히 낮아 세밀한 제어 구현에 여전히 큰 개선이 필요하다.

## Conclusion

본 연구는 ITTS 시스템이 일부 고수준 스타일 명령을 거칠게 수행할 수 있으나, 인간 지각과 일관되는 세밀하고 안정적인 음성 제어 능력 확보는 앞으로도 중요한 연구 과제로 남아 있음을 밝혀냈다.

# 5. [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)

## Introduction

- Goal: 본 연구는 사전 학습된 인코더-디코더 음성 인식 변환기 모델에 대해 텍스트 전용 도메인 적응 방법인 WhisTLE을 제안하는 데 목적이 있다.
- Motivation: 음성 데이터 수집이 어려운 실제 환경에서 미지의 어휘와 발화를 인식하기 위해 텍스트만을 사용한 적응 필요성이 존재한다.
- Contribution: WhisTLE은 사전 학습된 ASR 모델의 인코더 출력을 텍스트로부터 모델링하는 변분 오토인코더를 학습하고, 디코더를 미세 조정하여 텍스트 전용 적응에서도 입력-출력 및 잠재 상태에 대해 깊이 있는 감독을 수행하는 최초의 방법이다.

## Method

WhisTLE은 Whisper와 같은 사전 학습된 음성 인식 변환기의 인코더 출력을 텍스트만으로 근사하는 변분 오토인코더를 학습한다. 이후 학습된 텍스트-투-잠재 인코더를 고정시키고 Whisper 디코더를 미세 조정하며, 추론 시에는 원래의 인코더를 복원하여 추가 실행 비용 없이 사용한다. 이 과정은 텍스트 전용 학습으로 도메인 적응을 가능하게 하며, 선택적으로 텍스트-투-스피치(TTS) 기반 적응과 병합할 수 있다.

## Results

WhisTLE은 네 가지 사전 학습된 ASR 모델과 네 개의 도메인 외 데이터셋에서 TTS와 결합 시 TTS 단독 대비 평균 단어 오류율(WER)를 12.3% 상대적으로 감소시키고, 총 32개 실험 시나리오 중 27개에서 모든 비-WhisTLE 기준 방법을 능가하였다.

## Limitations

본 연구는 텍스트 전용 적응 시 인코더 출력을 근사하는 VAE 학습에 자원이 소요되며, 반드시 TTS 적응과 결합할 때 가장 큰 성능 향상을 보였다.

## Conclusion

WhisTLE은 사전 학습된 음성 인식 변환기의 텍스트 전용 도메인 적응에 있어 깊은 잠재 상태 감독을 통해 기존 방식 대비 현저한 성능 향상을 달성하였으며, 향후 ASR을 넘어 다양한 분야로의 확장이 기대된다.
