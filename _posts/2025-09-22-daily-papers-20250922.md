---
layout: post
title: "Daily Papers — 2025-09-22"
date: 2025-09-22 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid   Vision Tokenizer](https://arxiv.org/abs/2509.16197)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.16197)

## Introduction
- 본 논문의 목표는 시각 이해와 생성 기능을 통합하여 성능 저하를 최소화하는 단순하고 확장 가능한 통합 다중모달 대형언어모델을 제안하는 것이다.  
- 기존 공개 오픈소스 모델들은 이해와 생성 기능 간의 성능 상충 문제를 겪고 있으며, 특히 텍스트가 풍부한 벤치마크에서 이해 성능이 떨어지는 한계가 존재한다.  
- 본 연구는 하이브리드 비전 토크나이저와 정제된 학습 레시피를 결합하여 두 기능 사이의 갈등을 크게 완화한 Manzano 모델을 개발한 점에 기여한다.  

## Method  
Manzano는 하나의 공유된 비전 인코더에서 두 개의 경량 어댑터를 통해 이해용 연속 임베딩과 생성용 이산 토큰을 동시에 생산하는 하이브리드 이미지 토크나이저를 사용한다.  
공통 의미 공간에서 고수준 의미를 생성하는 자회귀 LLM이 텍스트와 이미지 토큰을 예측하며, 이후 확산 디코더가 이미지 토큰을 픽셀로 변환한다.  
학습은 텍스트 전용, 이미지 이해, 텍스트-이미지 생성 데이터를 혼합하여 일괄적으로 수행되어 두 영역의 확장 가능한 공동 학습을 가능케 한다.  

## Results  
Manzano는 통합 모델 중 최첨단 결과를 달성하였으며, 특히 텍스트가 풍부한 평가에서 전문 모델들과 경쟁력 있는 성능을 보여준다.  

## Limitations  
대규모 확장 시 미세한 미적 품질 하락 현상이 관찰되어 향후 심층 연구가 필요하다.  

## Conclusion  
Manzano의 하이브리드 토크나이저와 통합 학습 설계는 이해와 생성 기능 간의 충돌을 효과적으로 완화하며, 확장성 높은 통합 다중모달 언어모델 구축에 기여한다.

# 2. [Latent Zoning Network: A Unified Principle for Generative Modeling,   Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.15591)

## Introduction
- Goal: 본 논문은 생성 모델링, 표현 학습, 분류라는 세 가지 핵심 머신러닝 문제를 하나의 통합 원리로 해결하는 방법을 제안하는 데 목적이 있다.  
- Motivation: 이들의 최첨단 솔루션이 서로 분리되어 있는 현황에서 단일 원리가 세 과제를 모두 처리할 수 있다면 머신러닝 파이프라인 간소화와 작업 간 상호 시너지 촉진이 가능하다는 점에 착안하였다.  
- Contribution: Latent Zoning Network(LZN)라는 통합 프레임워크를 제안하여, 공통된 가우시안 잠재 공간 내에서 서로 다른 데이터 유형에 대응하는 인코더와 디코더를 연결하여 다양한 머신러닝 작업을 하나의 공간에서 처리함을 보여주었다.  

## Method  
LZN은 모든 데이터 유형(예: 이미지, 텍스트, 레이블)이 별도의 인코더를 통해 불교차 잠재 영역을 형성하는 공용 가우시안 잠재 공간으로 매핑되도록 설계된다.  
잠재 영역 계산과 잠재 정렬이라는 두 가지 기본 연산으로, 인코더가 Anchor points를 생성하고 flow matching 기법으로 영역을 분할 및 정렬하여 다양한 태스크 간 일관성과 생성 가능성을 확보한다.  
이러한 잠재 공간 내에서 인코더-디코더 조합 또는 단독 모듈을 활용해 조건부 생성, 표현학습, 분류 등 여러 작업을 단일 프레임워크 내에서 수행할 수 있다.  

## Results  
LZN은 (1) 무조건적 이미지 생성에서 기존 최첨단 Rectified Flow 모델의 FID를 CIFAR10에서 2.76에서 2.59로 개선하고, (2) 지도 없이 표현 학습을 수행해 MoCo 대비 9.3%, SimCLR 대비 0.2% 높은 ImageNet 선형 분류 정확도를 달성하며, (3) 분류와 생성 작업을 동시에 해결해 CIFAR10에서 두 작업의 최첨단 성능을 기록하는 등 다양한 시나리오에서 유의미한 성능 향상을 입증하였다.  

## Limitations  
추가적인 인코더-디코더의 확장성과 대규모 적용 시 효율적인 학습 및 잠재 공간의 규모 확장 문제 등 여전히 미해결 과제와 도전이 존재한다.  

## Conclusion  
LZN은 다양한 머신러닝 과제를 단일 잠재 공간에서 통합적으로 처리하여 상호 보완적 학습과 활용이 가능한 새로운 통일 원리를 제시하며, 향후 연구 방향에 새로운 가능성을 열었다.

# 3. [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.15496)

## Introduction
- Goal: 본 논문은 단일 입력 이미지로부터 고해상도의 개인화된 비디오를 생성하는 Lynx 모델을 제안하는 데 목적이 있다.  
- Motivation: 기존의 텍스트-기반 비디오 생성 모델들은 인물의 일관된 정체성 보존에 한계가 있었으며, 이를 극복하고자 하는 요구가 증대되고 있다.  
- Contribution: Lynx는 ID-adapter와 Ref-adapter라는 두 개의 경량 어댑터를 도입하여 정체성 일관성과 세밀한 디테일을 유지하는 고품질 개인화 비디오 생성 체계를 구축하였다.  

## Method  
Lynx는 DiT(Diffusion Transformer) 기반의 오픈소스 비디오 기초 모델 위에 두 가지 어댑터 모듈을 통합하여 작동한다. ID-adapter는 ArcFace로부터 추출한 얼굴 임베딩을 Perceiver Resampler를 통해 압축된 토큰으로 변환하여 정체성 조건을 부여하며, Ref-adapter는 고정된 참조 경로에서 VAE 특징을 획득하여 모든 변환기 층에 교차 주의를 통해 세밀한 정보를 주입한다. 다단계 점진적 학습 전략과 다양한 비디오 및 이미지 입력 크기를 효과적으로 처리하는 프레임 패킹 방식을 통해 학습이 진행된다.  

## Results  
Lynx는 40명의 인물과 20개의 공정한 텍스트 프롬프트로 구성된 800개의 테스트 케이스 평가에서 기존 최첨단 방법 대비 얼굴 유사성, 프롬프트 일치도, 비디오 품질 모든 면에서 우수한 성능을 입증하였다.  

## Limitations  
정보 부족.  

## Conclusion  
Lynx는 단일 참조 이미지로부터 강인한 정체성 보존과 자연스러운 동작, 높은 시각적 일관성을 유지하는 개인화 비디오 생성의 새로운 표준을 제시하였다.

# 4. [Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems](https://arxiv.org/abs/2509.13989)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.13989)

## Introduction
- Goal: 본 논문은 자연어 명령어 기반 표현형 텍스트-투-스피치(ITTS) 시스템에서 사용자 명령과 청취자 인지 간 격차를 정량적으로 분석하는 것을 목표로 한다.  
- Motivation: 기존 ITTS 연구는 명령어와 실제 음성 출력 간 정밀한 정렬 문제를 충분히 탐구하지 못하였으며, 미세한 감정 강도나 말씨 조절에 대한 청취자 인지가 불명확하였다.  
- Contribution: 본 연구는 미세 표현 제어의 두 핵심 차원(부사 정도 조절 및 감정 강도 형태소)을 포함해 5개 ITTS 모델을 대상으로 165명 이상의 대규모 인간 평가를 진행한 E-VOC 코퍼스를 구축하였다.

## Method  
- 평가 프레임워크는 부사 정도, 감정 강도, 화자 연령, 단어 강조 네 가지 제어 차원을 설정하여, 객관적 음향 지표와 주관적 인간 인식 평가를 병행하였다.  
- 음향적 측정에는 음압(LUFS), 기본주파수(F0), 발화 속도를 사용하였고, 인간 청취자는 감정 강도, 연령 인지, 단어 강조 여부를 평가하였다.  
- 5종 ITTS 시스템에 동일한 스크립트와 명령어를 적용하고 고품질의 검증 절차를 거쳐 대규모 평가 데이터를 수집하였다.

## Results  
- gpt-4o-mini-tts 모델이 부사 및 감정 강도에 따른 음향 변화와 청취자 인지 모두에서 가장 일관되고 신뢰할 만한 결과를 보였으며, 나머지 모델들은 미세한 속성 조절에서 성능이 떨어졌다.

## Limitations  
- 모든 시스템은 아동 및 노인 음성을 정확하게 생성하지 못하며, 단어 수준 강조 또한 구현이 미흡하여 세밀한 명령어 해석 능력이 부족했다.

## Conclusion  
- 현재 ITTS는 고수준 스타일을 대략적으로 구현할 수 있으나, 인간 인지와 일치하는 정밀하고 세밀한 명령어 제어는 여전히 해결해야 할 주요 과제이다.

# 5. [Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](https://arxiv.org/abs/2509.16622)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.16622)

## Introduction
- Goal: 본 연구는 음성 인식(ASR)에서 확산 기반 대형 언어 모델인 LLaDA의 활용 가능성을 경험적으로 평가하는 데 목적이 있다.  
- Motivation: 기존 자기회귀(AR) 디코딩은 높은 계산 비용과 효율성 저하 문제를 가지므로, 이를 보완할 수 있는 비자기회귀(NAR) 및 확산 모델 활용 방안이 요구된다.  
- Contribution: 본 연구는 (1) 외부 숙고(deliberation) 처리 모듈로서 LLaDA 적용, (2) 내부 디코더로서 LLaDA 활용, (3) LibriSpeech 데이터셋을 활용한 다양한 디코딩 전략 분석을 체계적으로 수행하였다.  

## Method  
Whisper-LLaDA는 Whisper-Large-v3 음성 인코더와 확산 기반 LLaDA-8B-Instruct 디코더를 결합한 모델이며, Q-Former와 LoRA를 통해 음성 및 텍스트 임베딩을 융합시킨다.  
디코딩에는 확산 기반 전역 병렬 생성과 준자기회귀(semi-autoregressive) 전략을 적용하였다.  
또한 Whisper-LLaMA 산출물을 개선하기 위한 무작위 마스킹, 낮은 확신 마스킹 및 준자기회귀 숙고 방법을 탐구하였다.  

## Results  
LibriSpeech 기준 Whisper-LLaMA 대비 Whisper-LLaDA를 이용한 숙고 처리 시 WER(test-other)에서 최대 12.3% 상대 성능 향상을 달성하였으며, 확산 기반 준자기회귀 디코딩은 속도 및 정확도 간 균형에서 경쟁력 있는 결과를 보였다.  

## Limitations  
텍스트만을 입력으로 하는 LLaDA는 음성 임베딩 없는 상태에서는 정확도 향상에 실패하여, 효과적인 확산 기반 ASR을 위해 음성 조건화 임베딩의 중요성이 강조된다.  

## Conclusion  
확산 기반 LLaDA는 음성 조건화 시 ASR에서 효율적이고 효과적인 숙고 처리 및 디코딩 수단을 제공하나, 사전학습된 자기회귀 시스템 대비 정확도 격차 해소를 위한 추가 연구가 필요하다.

# 6. [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained   Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2509.10452)

## Introduction
- Goal: 본 연구는 사전학습된 인코더-디코더 자동 음성 인식(ASR) 트랜스포머 모델을 위한 텍스트만 활용하는 도메인 적응 방법을 제안하는 것이다.  
- Motivation: 실제 환경에서는 도메인에 따른 음성 데이터를 수집하는 것이 어려워 텍스트 데이터만으로 도메인 적응이 필요하다.  
- Contribution: WhisTLE이라 명명된, 변분 오토인코더(VAE)를 활용해 ASR 인코더 출력을 텍스트로 모델링하고 디코더를 미세조정하는 깊은 감독 기반 텍스트 전용 도메인 적응 방식을 최초로 제안하였다.  

## Method  
WhisTLE는 Whisper와 같은 사전학습된 인코더-디코더 ASR 모델에서 텍스트 입력으로 인코더 출력의 잠재 표현을 생성하는 VAE를 학습한다. 학습된 텍스트-투-잠재 인코더(TLE)를 이용해 디코더를 미세조정하며, 추론 시에는 원래 인코더를 사용하여 추가 비용이 발생하지 않는다. 이 방법은 텍스트만으로 내부 잠재 상태를 깊게 감독하고, 선택적으로 텍스트-투-스피치(TTS) 적응과 결합할 수 있다.  

## Results  
WhisTLE와 TTS 적응을 결합할 경우 네 가지 도메인 외 데이터셋과 네 가지 ASR 모델에서 TTS 단독 대비 평균 12.3% 상대 단어 오류율(WER)을 감소시키고, 32개 실험 중 27개에서 모든 비WhisTLE 방법을 능가하였다.  

## Limitations  
WhisTLE는 기존 사전학습된 모델을 활용하는 한계로, 완전히 새로운 아키텍처를 사용하는 텍스트만 학습 방식과는 다른 범주에 속한다.  

## Conclusion  
본 연구는 사전학습 ASR 모델의 텍스트 전용 도메인 적응에서 심층 잠재 상태 감독의 효능을 증명하였으며, 향후 ASR 외 분야로의 확장 가능성을 계획하고 있다.
