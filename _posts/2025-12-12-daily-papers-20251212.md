---
layout: post
title: "Daily Papers — 2025-12-12"
date: 2025-12-12 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction](https://arxiv.org/abs/2511.23386)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.23386)

## Introduction
- Goal: 본 연구는 시각적 이해, 생성 및 재구성을 단일 토크나이저에서 통합하는 VQRAE 모델을 제안하는 데 목적이 있다.  
- Motivation: 기존 연구들은 시각적 이해와 생성을 위해 이중 인코더 구조를 주로 사용하여 모델 복잡성을 증가시키고 효과적인 표현 통합에 어려움이 존재하였다.  
- Contribution: VQRAE는 사전학습된 비전 파운데이션 모델을 기반으로 고차원 의미 벡터 양자화 코드북과 대칭적 ViT 디코더를 활용하여 연속적 의미 특징과 이산적 토큰을 동시에 생성하는 통합 토크나이저를 최초로 구현하였다.  

## Method  
VQRAE는 사전학습된 비전 파운데이션 모델을 단일 인코더로 사용하며, 두 단계 학습 전략에서 첫 번째 단계는 인코더를 고정하고 의미적 벡터 코드북과 픽셀 재구성 디코더를 훈련한다. 두 번째 단계에서는 인코더를 포함한 전체 모델을 자기증류 손실과 재구성 손실로 함께 최적화하여 의미 정보 유지와 세밀한 재구성을 달성한다. 또한, 본 연구는 고차원(예: 1536차원) 코드북이 100% 활용률을 달성하는 점을 세계 최초로 검증하였다.  

## Results  
제안된 VQRAE는 이미지넷 기반 재구성, MME-Perception 등 시각 이해 및 생성 벤치마크에서 이중 인코더 방식 대비 경쟁력 있는 성능과 확장 가능성을 보였다.  

## Limitations  
본 연구에서 VQRAE는 일부 실험에서 고차원 코드북 훈련에 시간 소요가 크고 크기 확장 시 수렴 속도가 늦어지는 한계가 존재한다.  

## Conclusion  
VQRAE는 의미적 연속 특징과 이산적 세밀 토큰을 단일 토크나이저 내에서 통합 구현하여 멀티모달 이해, 생성 및 재구성의 균형을 효과적으로 달성하는 혁신적 방법임을 확인하였다.

# 2. [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.10938)

## Introduction
- Goal: 본 연구는 점별 함수(point-wise function)를 이용하여 전통적 정규화층을 대체하고 이를 능가하는 함수 설계를 찾아 강력한 정규화-프리 Transformer 아키텍처를 제안하는 것이다.  
- Motivation: 기존의 정규화층은 학습 시 통계량 의존성과 배치 크기 민감성 등으로 인한 메모리 및 동기화 비용과 불안정성 문제를 내포하고 있어 이를 극복할 대안이 필요하다.  
- Contribution: 본 연구는 정규화층 대체에 적합한 함수 특성을 정립하고, 대규모 탐색을 통해 동적 오차 함수(Dynamic erf, Derf)를 도출하여 다양한 분야의 Transformer에서 정규화층 및 기존 점별 함수 대비 우수한 성능을 입증하였다.  

## Method  
본 연구는 점별 함수가 갖추어야 할 핵심 속성인 제로 중심성(zero-centeredness), 유계성(boundedness), 중심 민감도(center sensitivity), 단조성(monotonicity)을 체계적으로 분석하였다.  
이를 기반으로 다수의 후보 함수를 생성 및 평가하여 Derf 함수(erf(αx + s))를 최적 함수로 선정하고, Transformer 내 기존 정규화층을 Derf로 일대일 대체하였다.  
또한, 학습 안정성 및 성능 향상을 위해 Derf의 파라미터 α와 시프트 s를 학습 가능하게 설계하고 초기화하였다.  

## Results  
Derf는 ViT, Diffusion Transformer, wav2vec 2.0, DNA 시퀀스 모델, GPT-2 등 다양한 모델과 데이터셋에서 LayerNorm, RMSNorm, DyT 대비 우월한 정확도, 낮은 FID 및 손실 값을 일관되게 기록하였다.  

## Limitations  
본 연구는 Derf 함수가 기존 정규화층에 비해 우월한 일반화 성능을 보이나 훈련 손실 상에서는 정규화층보다 다소 낮은 적합성을 보이는 한계가 있다.  

## Conclusion  
Derf는 간결함과 뛰어난 성능을 겸비한 실용적인 점별 함수로, 정규화층을 대체할 차세대 강력한 정규화-프리 Transformer 구현에 적합함을 입증하였다.

# 3. [MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification](https://arxiv.org/abs/2512.09270)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.09270)

## Introduction
- 본 연구의 목표는 장기 동영상 내 4D 동작을 메모리 사용을 제한하면서 시간적 일관성과 고품질 시각화를 유지하며 모델링하는 것이다.  
- 기존 4D Gaussian Splatting 기법은 장기 동작 영역에서 메모리 폭발과 시간적 깜빡임 현상, 그리고 새롭게 나타나거나 사라지는 가림현상을 처리하지 못하는 한계가 존재한다.  
- 이에 저자들은 Anchor Relay 기반 양방향 블렌딩과 특징 분산 기반 계층적 고밀도화 기법을 도입한 MoRel 프레임워크를 제안하였다.  

## Method  
MoRel은 전역 및 키프레임 앵커 공간을 단계적으로 학습하며, 키프레임 앵커 간 양방향 변형을 학습하고 학습 가능한 불투명도 제어를 통해 부드러운 시간적 전이를 실현한다.  
특징 분산에 따른 계층적 고밀도화 기법으로 불필요한 앵커 증식을 억제하면서 고주파 세부 정보를 보존한다.  
이와 함께 필요 시 앵커와 변형 정보를 동적으로 로드하여 GPU 메모리 사용을 제한하고 임의 시점 접근성을 지원한다.  

## Results  
제안된 MoRel은 SelfCapLR 신규 데이터셋에서 기존 4DGS 기법 대비 더 낮은 temporal optical flow (tOF) 값과 우수한 PSNR, SSIM, LPIPS 지표 성능을 보이며 장기 4D 동작 재구성에서 시간적 일관성과 시각적 품질, 메모리 효율성을 동시에 확보하였다.  

## Limitations  
제안 기법의 학습 및 렌더링은 복잡한 단계와 연산을 포함하여 시스템 구현 복잡도가 존재한다.  

## Conclusion  
MoRel은 Anchor Relay 기반 양방향 블렌딩과 계층적 고밀도화를 통해 메모리 사용을 제한하면서 장기 4D 동작의 시간적 일관성 및 고품질 재구성을 효과적으로 달성하였다.

# 4. [Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization](https://arxiv.org/abs/2512.10955)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.10955)

## Introduction
- Goal: 본 논문은 이미지의 특정 속성(attribute)만을 고충실도로 분리하여 표현하는 오픈-어휘(open-vocabulary) 이미지 속성 인코더인 Omni-Attribute를 제안하는 데 목적이 있다.  
- Motivation: 기존 범용 이미지 인코더들은 다양한 시각 요소들이 뒤섞인 전반적 임베딩을 생성하여 단일 속성 격리가 어렵고, 정보 누수로 인해 비일관적 합성이 발생하는 문제를 보인다.  
- Contribution: Omni-Attribute는 이미지 쌍에 대한 긍정 및 부정 속성 주석과, 생성 충실도와 대조 분리도를 동시에 최적화하는 이중 목적 학습 방식을 도입하여 고품질의 속성별 표현을 추출한다.  

## Method  
Omni-Attribute는 이미지와 텍스트 속성 설명을 동시에 입력받아 속성별 정보만을 선택적으로 인코딩하며, 긍정 속성과 부정 속성으로 주석된 이미지 쌍을 활용해 학습한다. 학습은 생성 손실로 속성 정보를 복원하고 대조 손실로 부적합 정보를 억제하는 방식으로 진행된다. 모델은 다중 모달 대형 언어 모델을 기반으로 LoRA 튜닝 및 경량 커넥터를 적용하여 속성 분리를 효과적으로 수행한다.  

## Results  
Omni-Attribute는 속성 지향 이미지 검색, 개인화, 다중 속성 조합 이미지 합성 작업에서 기존 방법 대비 높은 충실도와 자연스러움을 갖는 성능을 보여주었으며, 정성적·정량적 평가에서 최첨단 결과를 기록하였다.  

## Limitations  
속성 임베딩이 단일 또는 소수 속성 중심으로 설계되어 대부분 시각 정보를 그대로 유지하며 일부 속성만 변경하는 이미지 편집 작업에는 제한적이며, 상호 연관된 속성 간 완전한 분리는 아직 해결되지 않은 과제로 남아 있다.  

## Conclusion  
Omni-Attribute는 생성 충실도와 속성 분리를 동시에 달성하는 오픈-어휘 속성 인코더로서, 시각 개념의 개인화 및 조합 생성에 효과적이며 멀티모달 이해와 제너레이티브 조작의 연결 고리를 마련하였다.
