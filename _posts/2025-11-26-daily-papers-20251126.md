---
layout: post
title: "Daily Papers — 2025-11-26"
date: 2025-11-26 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20635)

## Introduction
- Goal: 본 연구는 다양한 입력 이미지로부터 다수의 고일관성 출력 이미지를 생성하는 통합적이고 다방면에 활용 가능한 다대다 이미지 생성 모델 iMontage를 제안하는 것이다.  
- Motivation: 기존의 대규모 비디오 모델은 시공간 일관성에서는 우수하지만 연속적 영상 클립 기반으로 동적 범위가 제한되어 다이나믹한 다대다 이미지 생성에는 한계가 있었다.  
- Contribution: 본 연구는 비디오 모델의 운동 사전지식을 보존하면서 이미지 데이터의 풍부한 다양성을 주입하는 최소 침습적 적응 전략과 고품질 데이터 셋 구성, 다중 작업 통합 학습 체계를 통해 시공간 및 내용 일관성을 유지하는 다대다 이미지 생성 방법을 제안하였다.  

## Method  
iMontage는 3D VAE와 언어 모델을 결합한 MMDiT 아키텍처를 활용하며 입력 및 출력을 가변 길이의 pseudo-프레임으로 처리한다. 위치 임베딩에서는 입출력을 구분하는 Marginal RoPE를 도입하여 시간적 순서정보를 부여하되 공간적 포지셔널 인코딩은 유지한다. 학습은 대용량 고동적 프레임 페어와 이미지 편집 데이터로 구성된 사전학습 데이터와 다중 과제를 아우르는 정교한 프롬프트 기반 지도학습으로 진행된다.  

## Results  
iMontage는 이미지 편집, 다중 입력-출력 조건화 생성, 스토리보드 생성 등 다양한 다대다 시나리오에서 기존 최첨단 모델을 능가하는 질적 및 양적 성과를 나타내었다.  

## Limitations  
현재 연산 및 데이터 제약으로 최대 4개 입력과 출력에 대해 우수한 품질을 발휘하며, 더 긴 컨텍스트 설정과 일부 세부 기능에서는 한계가 존재한다.  

## Conclusion  
iMontage는 비디오 기반 사전학습과 이미지 데이터 다양성 융합을 통해 매우 동적이면서도 일관성 높은 다대다 이미지 생성에 성공한 통합 모델로, 향후 장기 문맥 처리 및 데이터 확장을 통해 성능 개선 가능성이 있다.

# 2. [Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20561)

## Introduction
- Goal: 본 연구는 통합 멀티모달 모델에서 이해 능력이 실제 생성 과정에 실질적인 영향을 미치는지 체계적으로 분석하는 데 목적이 있다.  
- Motivation: 기존 모델들은 내적 지식과 추론 능력을 활용하는 데 한계가 있으며, 평가 시 데이터 누수와 실패 원인 불명확성 문제가 존재한다.  
- Contribution: 이를 해결하기 위해 데이터 누수를 방지하는 합성 데이터와 분리된 평가 체계인 UniSandbox를 제안하고, 이해-생성 간의 격차를 정밀 분석하였다.  

## Method  
본 연구는 이해 능력을 지식과 추론 두 차원으로 분해하여 평가를 수행하였으며, 수학적 계산과 기호 매핑 기반의 합성 태스크를 설계하였다.  
추론 능력을 강화하기 위해 Chain-of-Thought(CoT)를 도입하고, 자기교사학습(STARS)을 통해 CoT의 명시적 추론을 암묵적으로 내재화하는 방법을 제안하였다.  
또한, 새로운 지식을 이해 모듈에 주입 후 이를 생성 모듈로 효과적으로 전달하는 지식 전이 능력을 쿼리 기반 구조의 CoT 유사 메커니즘과 함께 분석하였다.  

## Results  
CoT 적용 시 추론 기반 생성 정확도가 대폭 향상되었고, 자기교사학습은 수학 연산 태스크에서 추론 능력의 내재화를 성공시켰으며, 쿼리 기반 아키텍처는 암묵적인 CoT 능력을 지녀 지식 전이 효율을 일부 개선하였다.  

## Limitations  
고난도 기호 매핑 태스크에 대한 CoT 내재화는 단일 난이도 학습 시 실패하며, 이를 극복하기 위해 커리큘럼 학습이 필요하였다.  

## Conclusion  
UniSandbox를 통해 통합 멀티모달 모델에서 이해와 생성을 효과적으로 연계하는 메커니즘의 한계와 개선 방안을 밝히며 향후 통합 아키텍처 설계 방향성을 제시하였다.

# 3. [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20102)

## Introduction
- Goal: 본 논문은 큰 문맥을 효율적으로 처리하기 위한 희소-희소 주의 메커니즘인 SSA (Sparse Sparse Attention)를 제안하는 데 목적이 있다.  
- Motivation: 기존의 희소 주의 학습 방식은 선택되지 않은 키-값 쌍에 대해 그래디언트 업데이트가 이루어지지 않아 충분한 희소성을 확보하지 못하는 한계가 존재하였다.  
- Contribution: SSA는 희소 주의와 완전 주의를 공동 학습하고 양방향 정렬 손실을 도입하여 모든 토큰에 대한 그래디언트 흐름을 유지하며, 주의 분포의 희소성을 극대화하는 통합 훈련 프레임워크를 제안하였다.  

## Method  
SSA는 학습 시 절반의 확률로 완전 주의(full attention)와 희소 주의(sparse attention)를 번갈아 사용하며, 각 계층에서 두 주의 출력 간의 양방향 정렬 손실을 적용한다. 이 정렬 손실은 완전 주의가 희소 주의 출력을 모방하도록 하는 희소성 손실과 희소 주의가 완전 주의 출력을 유지하도록 하는 커밋먼트 손실로 구성되어 서로의 표현 공간을 일치시킨다. 이를 통해 모델은 내재적인 희소성을 높이고 희소 및 완전 주의 환경 모두에서 성능을 향상시킨다.  

## Results  
SSA는 여러 벤치마크에서 최첨단 성능을 달성하였으며, 희소 및 완전 주의 추론 모두에서 높은 효율성 및 정확도를 보였고, 다양한 희소성 예산에 따라 성능이 일관되게 개선되며 긴 문맥에 대한 일반화 능력도 뛰어났다.  

## Limitations  
제한점으로는 SSA의 훈련 과정에서 완전 주의 출력을 추가로 계산해야 하므로 계산 비용이 일부 증가하는 점이 있다.  

## Conclusion  
본 연구는 희소 주의 학습 시 발생하는 그래디언트 업데이트 부족 문제를 해결하기 위해 희소 및 완전 주의 공동 학습과 양방향 정렬을 통한 희소성 증진을 제안하여, 보다 효율적이고 정확한 장문맥 처리 모델을 실현하였다.

# 4. [UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers](https://arxiv.org/abs/2511.20123)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20123)

## Introduction
- Goal: 본 논문은 비디오 확산 트랜스포머(Video Diffusion Transformers)의 영상 길이 외삽 문제를 해결하고자 한다.  
- Motivation: 기존 모델들은 학습 시퀀스 길이를 초과하는 긴 영상 생성 시 반복 콘텐츠 생성과 품질 저하라는 두 가지 주요 실패 현상을 보였다.  
- Contribution: 저자들은 이러한 실패 현상의 근본 원인을 주의(attention) 분산으로 규명하고, 이를 완화하는 학습이 필요 없는 UltraViCo 기법을 제안하였다.  

## Method  
UltraViCo는 학습 윈도우를 벗어난 토큰에 대한 주의 점수를 일정 비율로 감쇠시켜 주의 집중력을 회복하며, 주의 지도에서 주기적 패턴을 제거하고 품질 저하를 동시에 완화한다. 주기적 반복 문제는 위치 인코딩 주파수의 고조파 현상에서 비롯된 주의 점수의 구조적 분산으로 설명되고, 이를 해결하기 위해 고조파 위치에 더 강한 감쇠를 적용한다. 또한, 메모리 효율적인 CUDA 커널 구현을 통해 대규모 비디오 모델에도 효율적으로 적용 가능하다.  

## Results  
제안 방법은 다양한 최신 모델과 확장 비율에서 기존 기법들을 능가하며, 특히 4배 확장 시 동적도와 화질이 각각 233%와 40.5% 향상되어 2배였던 실용적 확장 한계를 4배로 대폭 연장하였다.  

## Limitations  
본 연구는 주로 토큰 간 주의 분산 문제에 집중하였으며, 이에 수반되지 않는 다른 확장 관련 문제는 다루지 않았다.  

## Conclusion  
UltraViCo는 비디오 길이 외삽 실패의 근본 원인인 주의 분산 현상을 해소하여 비디오 확산 트랜스포머의 실용적 길이 확장 한계를 크게 향상시킨다.

# 5. [OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation](https://arxiv.org/abs/2511.20211)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20211)

## Introduction
- 본 연구의 목표는 단일 시퀀스-투-시퀀스(sequence-to-sequence) 프레임워크를 통해 RGBA 이미지 생성 및 편집에 대한 통합 다중 작업 모델인 OMNIALPHA를 제안하는 것이다.  
- 기존의 RGBA 작업은 알파 채널 처리를 위한 전문 단일 작업 모델과 RGB 영역에 한정된 통합 다중 작업 모델로 분산되어 있어, 범용적이고 효율적인 RGBA 처리 모델의 부재가 문제로 제기되었다.  
- 이에 본 연구는 알파 인식 변분 오토인코더(alpha-aware VAE)와 다방향 확장 가능한 레이어 축을 도입한 MSRoPE-BiL 위치 임베딩을 활용해, 21개 다양한 RGBA 작업을 통합 학습하는 최초의 범용 다중 작업 RGBA 생성 프레임워크를 개발하였다.  

## Method  
OMNIALPHA는 불투명 초기화 전략으로 기존 RGB VAE를 확장하여 알파 채널을 포함하는 4채널 VAE를 구축하였으며, Diffusion Transformer 백본에 MSRoPE-BiL 위치 인코딩을 적용해 다중 RGBA 입력 및 출력을 동시에 처리한다.  
21개의 작업으로 구성된 통합 학습을 통해 AlphaLayers라는 1,000개의 고품질 RGBA 레이어 트리플릿 데이터셋을 활용하며, 이는 자동 합성 및 필터링 파이프라인을 통해 생성되었다.  
모델은 텍스트 프롬프트와 입력 RGBA 시퀀스에 조건화하여 타깃 RGBA 이미지 시퀀스를 생성하는 조건부 순차 생성 문제로 공식화하였다.  

## Results  
OMNIALPHA는 다양한 공개 및 자체 제작된 벤치마크에서 전문 단일 작업 모델 대비 최대 84.8%의 SAD 감소와 90% 이상의 인간 선호도를 달성하는 등 RGBA 다중 작업에서 최첨단 성능을 입증하였다.  

## Limitations  
본 연구는 아직 일부 복잡한 시나리오에서의 성능 한계 및 대규모 실시간 적용을 위한 최적화가 필요하다.  

## Conclusion  
통합 다중 작업 RGBA 생성 모델인 OMNIALPHA는 전문 모델과 RGB 전용 통합 모델 간의 갭을 극복하며 높은 범용성과 우수한 성능을 달성하여 알파 채널 인식 생성 시스템 발전에 중요한 전환점을 제공한다.

# 6. [VQ-VA World: Towards High-Quality Visual Question-Visual Answering](https://arxiv.org/abs/2511.20573)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20573)

## Introduction
- Goal: 본 논문은 시각적 질문에 대해 이미지로 답변하는 Visual Question-Visual Answering(VQ-VA) 능력을 개방형 모델에 적용하는 것을 목표로 한다.  
- Motivation: 기존 개방형 이미지 편집 모델들은 VQ-VA 작업에서 지식과 다단계 추론이 필요한 자유 형식의 시각적 생성 능력 부족으로 성능이 낮았다.  
- Contribution: 본 연구에서는 대규모 지식 및 추론 기반 데이터셋인 VQ-VA WORLD와 인간 전문가가 고안한 평가 벤치마크 IntelligentBench를 제안하였다.  

## Method  
VQ-VA WORLD는 웹 문서에서 시맨틱 연관된 이미지 쌍을 추출하고, 질문 생성, 필터링, 질문 재작성, 추론 경로 생성의 다섯 에이전트 모듈로 구성된 파이프라인으로 180만 개 고품질 학습 데이터를 구축하였다.  
이 데이터셋은 세계 지식, 설계 지식, 추론 영역을 포함하며, 이를 통해 지식과 추론 능력이 요구되는 VQ-VA 태스크 수행 능력 향상을 도모한다.  
평가용 IntelligentBench는 실세계 웹자료 기반 360개 사례로 구성되어 VQ-VA 성능을 엄격히 측정한다.

## Results  
LightFusion 모델을 본 데이터로 파인튜닝한 결과, IntelligentBench에서 기존 오픈소스 최고 성능(7.78) 대비 대폭 향상된 53.06을 기록하며, 일부 비공개 선도 모델 성능과의 격차를 크게 좁혔다.  

## Limitations  
현재 제안된 데이터셋과 모델은 선도 비공개 모델 대비 성능 격차가 존재하며 여전히 완전한 추론과 세계 지식 통합에서 한계가 있다.  

## Conclusion  
본 연구는 에이전트 기반 대규모 데이터 구축을 통해 개방형 모델의 VQ-VA 역량 강화를 달성하여, 향후 멀티모달 이미지 생성 연구를 촉진하는 기반을 마련하였다.

# 7. [Concept-Aware Batch Sampling Improves Language-Image Pretraining](https://arxiv.org/abs/2511.20643)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20643)

## Introduction
- Goal: 본 연구의 목표는 개념 단위 정보를 활용하여 시각-언어 모델의 사전학습 데이터 배치 샘플링을 개선하는 방법을 제안하는 것이다.  
- Motivation: 기존 데이터 큐레이션 방법들이 오프라인 및 개념 비고려 방식을 주로 사용하여 데이터 편향 문제 및 재활용의 어려움을 야기하기 때문이다.  
- Contribution: 1억 2800만 쌍의 이미지-텍스트 데이터를 개념별 주석과 함께 제공하는 DataConcept을 구축하고, 이를 활용하는 개념 인지형 배치 샘플링(CABS) 프레임워크를 개발하였다.  

## Method
DataConcept은 확장된 개념 사전 구축과 태깅, 위치 기반 개념 접지, 그리고 개념 인지 합성 캡션 생성 단계를 거친 대규모 개념 주석 데이터셋이다.  
CABS는 온라인으로 배치 구성 시 개념 다양성 최대화(CABS-DM)와 개념 빈도 최대화(CABS-FM)를 통해 각각 분류와 검색 태스크에 적합한 개념 분포를 유도한다.  
이 방식을 통해 사전학습 중 데이터 샘플의 배치 구성 방식을 유연하게 조절하며, 작업 특성에 맞는 개념 중심 데이터 큐레이션이 가능하다.  

## Results  
CABS-DM과 CABS-FM 모두 28개 벤치마크에서 CLIP 및 SigLIP 모델 기반 사전학습 성능을 향상시켰으며, ImageNet 제로샷 분류는 최대 +7%, 이미지-텍스트 검색에서는 최대 +9.1%의 성능 개선을 보였다.  

## Limitations  
정보 부족.  

## Conclusion  
본 연구는 개념 인지형 온라인 배치 샘플링이 기존 IID 샘플링 및 오프라인 큐레이션 대비 시각-언어 사전학습 효과를 크게 개선하며, 특정 작업에 맞춰 데이터 분포를 조절할 수 있는 강력한 공개 도구임을 입증하였다.

# 8. [Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation](https://arxiv.org/abs/2511.20250)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.20250)

## Introduction
- Goal: 본 논문은 단안 영상에서 탁구 공의 3차원 궤적 및 회전을 정확하게 추정하는 실용적이고 견고한 파이프라인을 제안하는 것을 목표로 한다.  
- Motivation: 기존 방법들은 합성 데이터에 기반해 훈련되어 실제 영상의 잡음과 불완전한 검출에 적응하지 못하는 한계가 존재한다.  
- Contribution: 본 연구는 전처리 2D 인식과 후처리 2D-3D 변환 단계를 분리하고, 새로운 고해상도 TTHQ 데이터셋을 도입하여 실제 환경에 견고한 3D 탁구 분석 시스템을 개발하였다.  

## Method  
본 파이프라인은 먼저 고해상도 영상에서 Segformer++ 기반의 볼과 테이블의 주요점 검출기를 활용하여 2D 위치를 추출하고, 이후 필터링을 거쳐 신뢰도 높은 2D 데이터 시퀀스를 생성한다. 후처리 단계에서는 이 2D 궤적과 테이블 주요점을 입력으로 하여 물리적으로 정확한 합성 데이터로만 학습된 2D-3D 업리프팅 네트워크를 사용해 3D 궤적과 초기 스핀벡터를 추정한다. 특히, 시간 정보의 정확한 인코딩과 결측 검출에 강한 모델 설계로 실환경의 변동성과 잡음에 대응 가능하다.  

## Results  
새로 개발한 Segformer++ 모델은 기존 최첨단 모델 대비 2픽셀 이내 정확도에서 우수한 성능을 보였으며, 전체 파이프라인은 실제 방송 영상 데이터인 TTHQ 및 TTST 데이터셋에서 높은 3D 재투영 정확도와 스핀 분류 정확도를 달성하였다.  

## Limitations  
실제 3D 정답 정보 부족으로 인해 백엔드 결과에 대한 직접적이고 완전한 정량 평가가 제한적이며, 일부 극한 상황에서 검출 누락 발생 가능성이 존재한다.  

## Conclusion  
본 연구는 실제 단안 방송 영상에서 탁구 공의 3차원 궤적 및 스핀을 견고하게 추론하는 최초의 학습 기반 완전 파이프라인으로서, 2D 전처리와 3D 업리프팅의 분리 전략과 고해상도 데이터셋 도입을 통해 실환경 분석 분야의 새로운 기준을 제시하였다.
