---
layout: post
title: Daily Papers — 2025-08-30"
date: 2025-08-30 08:15:00
tags: [papers, arxiv, ai]
categories: []
---


# 1. [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable   Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)

## Introduction
- Goal: 본 논문은 안정적인 텍스트-이미지 생성 강화학습을 위한 쌍대 선호 보상 기반 그룹 상대 정책 최적화법(PREF-GRPO)을 제안하는 것이다.  
- Motivation: 기존 포인트 단위 보상 모델은 이미지 간 보상 점수 차이가 미미하여 환상적 이점 문제를 일으키며 보상 해킹과 품질 저하를 초래하기 때문이다.  
- Contribution: 쌍대 선호 보상 모델을 활용하여 보상 점수 최대화에서 선호 순위 적합으로 최적화 목표를 전환하고, 정밀한 평가가 가능한 통합 벤치마크 UNIGENBENCH를 함께 제시한다.  

## Method  
PREF-GRPO는 각 학습 단계에서 생성된 이미지 집합 내 모든 쌍을 선호 보상 모델로 비교하여 승률을 계산하고, 이 승률을 정책 최적화의 보상 신호로 사용한다.  
이 방식은 보상 분산을 확대하고 미묘한 품질 차이를 효과적으로 구분하며, 과도한 최적화와 보상 해킹 문제를 완화시킨다.  
또한, 미리 정의된 다양한 세부 평가 차원과 주제별 프롬프트를 포함한 UNIGENBENCH를 통해 다면적이고 정밀한 텍스트-이미지 생성 모델 평가가 가능하다.  

## Results  
PREF-GRPO는 UNIGENBENCH에서 기존 보상 점수 최대화 기반 방법 대비 성능이 전반적으로 향상되었으며, 특히 논리 추론과 텍스트 일관성 부문에서 큰 개선을 보이고, 보상 해킹을 효과적으로 억제하였다.  

## Limitations  
세밀한 평가 차원에도 불구하고 일부 복잡한 논리와 텍스트 렌더링 분야에서 개방형 및 폐쇄형 모델 모두 아직 성능 한계가 존재한다.  

## Conclusion  
쌍대 선호 보상 기반 PREF-GRPO와 통합 벤치마크 UNIGENBENCH는 안정적이고 정밀한 텍스트-이미지 강화학습 및 평가를 위한 효과적인 해결책임이 실험을 통해 입증되었다.

# 2. [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)

## Introduction
- Goal: 본 연구는 agentic 강화학습을 활용하여 Python 코딩 도구를 능동적으로 활용하고 피드백을 반영하는 14억 매개변수 수학 추론 모델 rStar2-Agent를 개발하는 것이다.  
- Motivation: 기존의 장기 체인 오브 쓰론(Long Chain-of-Thought, CoT)이 복잡한 중간 단계 오류 탐지와 창의적 추론 전환에 한계가 있어 모델이 스스로 도구를 활용하며 더 똑똑하게 추론하도록 하는 필요성이 대두되었다.  
- Contribution: 효율적 강화학습 인프라 구축, 환경 잡음을 고려한 GRPO-RoC 알고리즘 제안, 그리고 단계적 RL 훈련법을 통해 7일 내 510 스텝 만에 최신 성능에 도달하는 rStar2-Agent를 개발하였다.  

## Method  
rStar2-Agent는 대규모, 격리된 고성능 코드 실행 환경과 동적 로드밸런싱 스케줄러를 마련하여 수만 건의 동시 도구 호출을 원활히 처리한다. GRPO-RoC라는 독창적 에이전트 강화학습 알고리즘을 통해 환경의 잡음과 도구 오류를 효과적으로 필터링하며, 결과만을 보상하는 outcome-only 보상체계하에서 고품질의 긍정 샘플을 우선 학습한다. 이러한 접근은 초기 비추론 SFT부터 단계별 강화학습으로 점진적 과제 난이도 및 길이 확장을 통해 컴퓨팅 비용을 최소화하면서도 고도화된 인지 능력을 획득하게 한다.  

## Results  
AIME24에서 80.6%, AIME25에서 69.8%의 평균 pass@1 점수를 기록하며 671B 파라미터의 DeepSeek-R1을 능가하고, 수학 외 과학 추론 및 에이전트 도구 사용 평가에서도 강력한 일반화 성능을 보였다.  

## Limitations  
agentic 강화학습 과정에서 코딩 도구 사용 중 발생하는 환경 잡음과 도구 호출 오류 문제는 여전히 완전한 해결이 어려워 일정 수준의 오류가 학습에 영향 미칠 수 있다.  

## Conclusion  
rStar2-Agent는 agentic 강화학습과 효율적 인프라 및 알고리즘 개선을 결합해 제한된 자원으로도 매우 효율적이고 뛰어난 수준의 수학 추론 성능과 도구 사용 능력을 실현하였다.

# 3. [USO: Unified Style and Subject-Driven Generation via Disentangled and   Reward Learning](https://arxiv.org/abs/2508.18966)

## Introduction
- Goal: 본 연구는 스타일 기반 및 주제 기반 이미지 생성 작업을 하나의 통합된 프레임워크로 통합하는 USO 모델을 제안하는 것을 목표로 한다.  
- Motivation: 기존 연구들은 스타일 주도 생성과 주제 주도 생성을 별도의 과제로 취급하여 각기 상충하는 요구사항을 해결하지 못했으나, 양자를 콘텐츠와 스타일의 분리 및 재조합으로 통합할 수 있음을 발견하였다.  
- Contribution: 본 연구는 스타일, 콘텐츠, 스타일화된 콘텐츠 이미지의 삼중 쌍 데이터 구축, 교차 작업 공동 분리를 이룬 USO 커스터마이즈 모델, 스타일 보상 학습 기법 및 스타일과 주제의 일관성을 평가하는 최초의 USO-Bench 벤치마크를 제안하였다.  

## Method  
USO 모델은 스타일 이미지와 콘텐츠 이미지를 분리 인코딩하고, 단계별 스타일 정렬 학습과 콘텐츠-스타일 분리 학습을 수행한다. 삼중쌍 데이터셋을 활용하여 양 작업에 필요한 특징을 분리하고 조합하는 공동분리(co-disentanglement) 패러다임을 도입하였다. 또한 스타일 보상 학습(SRL)을 통해 스타일 일관성을 명시적으로 강화하며, 스타일과 주제가 동시에 반영된 이미지 생성을 가능하게 하였다.  

## Results  
USO는 USO-Bench 및 DreamBench에서 주제 일관성, 스타일 유사도, 텍스트-이미지 정렬성 모두에서 기존 공개 소스 모델 대비 최고 성능을 달성하였다.  

## Limitations  
정보 부족.  

## Conclusion  
USO 프레임워크는 스타일 주도, 주제 주도 및 이들의 복합 생성 작업을 성공적으로 통합하며, 공동분리 및 보상 학습을 통해 각 작업의 성능을 상호 향상시키는 새로운 기준을 제시하였다.

# 4. [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World   Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)

## Introduction  
- Goal: MCP-Bench는 복잡한 실제 도메인에서 도구 사용이 요구되는 다단계 작업에 대한 대형 언어 모델(LLM) 에이전트를 평가하는 벤치마크를 제시하는 것이다.  
- Motivation: 기존의 도구 사용 벤치마크들은 제한된 도구 집합, 짧은 작업 흐름, 명확한 도구 지시 등으로 인해 현실적인 복잡성, 흐릿한 지시, 다중 목표 및 도메인 간 조정을 적절히 평가하지 못하였다.  
- Contribution: MCP-Bench는 28개의 MCP 서버에서 250개의 실시간 도구를 활용해 실제 생태계를 반영한 복잡한 다중 단계 작업을 자동 생성 및 평가하는 대규모 벤치마크와 평가 체계를 제안하였다.  

## Method  
MCP-Bench는 MCP 프로토콜 기반 서버들을 통해 다양한 도구 집합을 연결하고, 도구 간 의존성 체인을 분석하여 LLM이 다단계 작업을 수행하도록 자연어 작업 지시를 생성한다.  
평가는 도구 사용의 유효성, 스키마 준수, 실행 성공 여부를 확인하는 규칙 기반 평가와, LLM judge를 활용한 작업 완성도, 도구 선정, 계획 효율성 평가를 병합하여 진행한다.  
또한, 실제적 도전 과제인 흐릿한 지시사항 및 다중 서버 간 조정을 통해 LLM의 계획 및 추론 능력을 심층적으로 검증한다.  

## Results  
20개 최첨단 LLM을 대상으로 한 실험 결과, 고성능 모델들(gpt-5, o3 등)은 스키마 이해와 도구 사용 정확도가 98% 이상으로 뛰어났으나, 장기 계획 및 다중 서버 조정 등 상위 수준의 추론에서는 여전히 큰 성능 차이를 보였다.  

## Limitations  
현재 MCP-Bench는 복잡한 도구 생태계와 작업 구조를 제공하지만, 인간 평가의 개입이 필요하고 일부 작업의 자동 생성 품질 향상이 요구된다.  

## Conclusion  
MCP-Bench는 현실적인 도구 사용 시나리오에서 LLM 에이전트의 도구 이해, 계획, 실행 능력을 표준화하여 종합적으로 평가할 수 있는 확장성 높은 플랫폼임이 입증되었다.

# 5. [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)

## Introduction
- Goal: 본 논문은 Agentic AI의 “learning from practice” 패러다임을 실현하기 위한 AWORLD라는 분산형 오픈소스 프레임워크를 제안하는 데 목적이 있다.  
- Motivation: 복잡한 벤치마크 환경에서 경험 생성의 비효율성이 Agentic AI 학습의 주요 병목 현상으로 작용함에 따라 이를 극복할 필요가 존재한다.  
- Contribution: AWORLD 프레임워크를 설계 및 구현하여 경험 생성 단계의 14.6배 가속화를 달성하고, 이를 바탕으로 Qwen3-32B 기반 에이전트를 훈련시켜 기존 모델 대비 GAIA 벤치마크에서 우수한 성능을 입증하였다.  

## Method  
AWORLD는 에이전트 구축, 통신 프로토콜, 분산 실행 및 학습 조율을 통합하여 복잡하고 장기적인 환경과의 상호작용을 지원하는 아키텍처이다.  
분산 클러스터를 통해 다수의 에이전트가 동시에 환경과 상호작용하며 경험 데이터를 대규모로 수집할 수 있게 하였다.  
또한 외부 강화학습 프레임워크와 연동하여 경험 생성(rollout)과 모델 업데이트를 효율적으로 수행한다.  

## Results  
AWORLD 기반으로 강화학습된 Qwen3-32B 에이전트는 GAIA 벤치마크에서 베이스 모델 대비 정확도를 21.59%에서 32.23%로 향상시켰으며, 가장 어려운 문제에서는 최고 16.33%의 성과로 최첨단 폐쇄형 모델을 능가하였다.  

## Limitations  
AWORLD 프레임워크 자체는 트레이닝 프레임워크가 아니며, 강화학습 기능은 외부 시스템과의 통합에 의존한다.  

## Conclusion  
본 연구는 경험 생성의 병목을 분산처리로 해결하여 Agentic AI의 실질적 학습 가능성을 제고한 AWORLD 프레임워크와 이를 활용한 에이전트 훈련법을 제시함으로써 “learning from practice” 패러다임의 실현에 중요한 이정표를 마련하였다.

# 6. [TCIA: A Task-Centric Instruction Augmentation Method for Instruction   Finetuning](https://arxiv.org/abs/2508.20374)

## Introduction  
- Goal: 본 연구의 목표는 실세계 작업에 최적화된 다양하면서도 작업 관련성이 높은 명령어 데이터를 생성하는 Task-Centric Instruction Augmentation (TCIA) 방법을 제안하는 것이다.  
- Motivation: 기존 자동 명령어 생성 방법이 다양성 확보에는 성공했으나 실제 작업과의 관련성이 떨어지고 명령어 반복 및 작업 이탈 문제가 존재하여, 작업 특화 학습에 한계가 있다.  
- Contribution: TCIA는 명령어를 구조화하여 분해하고, BFS 기반 탐색을 통한 제약조건 조작으로 다양한 작업 중심 명령어를 체계적으로 생성하며, 이를 통해 개방형 LLM에서 작업별 성능을 평균 8.7% 개선하였다.  

## Method  
명령어를 기본 쿼리와 명확히 분류된 제약조건으로 분해하고, 유사 작업 데이터베이스에서 조건을 검색하여 BFS 탐색으로 명령어 제약조건을 추가·교체·삭제하며 다양성과 작업 일치도를 유지한다. 이후 LLM을 활용해 자연어 명령어로 재변환하고 엄격한 검증을 거쳐 질 높은 학습 데이터를 확보한다. 최종적으로 다중 LLM을 통해 응답을 생성 및 평가하여 고품질 명령어-응답 쌍을 학습 데이터로 사용한다.  

## Results  
TCIA로 학습한 모델은 4가지 실제 업무 중심 태스크에서 평균 8.7% 성능 향상을 보였으며, GPT-4o와 같은 최첨단 폐쇄형 모델을 능가하는 결과를 나타내었고, 공용 벤치마크에서도 일반 목적 능력을 유지하였다.  

## Limitations  
본 연구는 다중 턴 대화나 멀티모달 시나리오에 대한 적용 및 자동 프롬프트 정제에 관한 부분은 아직 구현 및 평가가 이루어지지 않았다.  

## Conclusion  
TCIA는 명령어의 구조적 분해와 제약조건 기반 다양성 확장을 통해 작업 특화 명령어 학습을 효과적으로 수행하며, 실제 작업 환경에 적합한 LLM 적응을 위한 확장 가능하고 효율적인 프레임워크임을 입증하였다.

# 7. [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)

## Introduction
- Goal: 본 연구는 긴 영상 생성 문제를 내부 정보 검색 문제로 재구성하여 장기 기억과 일관성을 유지하는 새로운 학습 가능한 희소 주의 라우팅 모듈인 Mixture of Contexts (MoC)를 제안하는 것이다.  
- Motivation: 기존 확산 변환기 기반 긴 영상 생성은 자기 주의 계산의 제곱 비용으로 인해 메모리와 계산이 비효율적이고 장기적 문맥 추적이 어려운 문제를 가지고 있다.  
- Contribution: MoC는 각 쿼리가 몇 개의 관련 청크와 필수 앵커(자막, 지역 윈도우)만을 선택해 주의를 집중하며 인과적 라우팅을 통해 루프 폐쇄를 방지, 긴 시퀀스에서 중요한 문맥을 효율적으로 검색하도록 설계되었다.  

## Method  
MoC는 영상 시퀀스를 의미론적 단위인 프레임, 샷, 자막 기준으로 청크로 분할하여 각 쿼리가 동적으로 관련 청크만 선택하는 탑-k 라우터를 적용한다.  
모든 영상 토큰은 텍스트 토큰 전체와 자기 샷 윈도우를 반드시 참조하며, 인과적 마스크로 시간 순서에 반하는 루프를 차단한다.  
선택된 청크들에 대한 주의 연산은 Flash Attention 커널로 고속 수행되어 계산 효율과 메모리 사용을 크게 개선한다.  

## Results  
긴 멀티샷 영상(약 180k 토큰) 생성에서 85% 이상의 희소성을 유지하며, FLOPs를 7배 이상 감소시키고 2.2배의 속도 개선과 함께 모션 다양성과 일관성 지표가 기존 밀집 주의 모델 대비 향상되었다.  

## Limitations  
MoC는 현재 LCT와 동일한 설정에서만 평가되었으며, 보다 긴 시퀀스에서의 성능과 효율성은 추가 연구가 필요하다.  

## Conclusion  
학습 가능한 희소 주의 라우팅을 통해 MoC는 복잡한 장기 문맥 기억 문제를 효율적으로 해결하며, 짧은 영상 수준의 계산 비용으로 분 단위 긴 영상 생성에서 탁월한 일관성과 메모리 성능을 달성한다.

# 8. [Turning the Spell Around: Lightweight Alignment Amplification via   Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)

## Introduction
- Goal: 본 논문은 대형 언어 모델(LLM)의 안전성 정렬을 강화하기 위한 경량의 백색박스(weight-level) 방법인 RANK-ONE SAFETY INJECTION(ROSI)를 제안하는 것을 목표로 한다.  
- Motivation: 기존 안전 메커니즘이 모델 내 특정 방향을 제거하여 우회될 수 있음을 보여주는 최근 연구에 대응하여, 안전 방향을 영구적으로 증폭시키는 방안을 모색하였다.  
- Contribution: 안전과 거부 행동에 대응하는 선형 방향을 추출해 모든 잔차 스트림 쓰기 행렬에 단일 랭크 수정으로 영구 주입하는 ROSI 기법을 도입하고, 광범위한 모델과 벤치마크에서 안전성 강화 및 기능 보존을 입증하였다.

## Method  
ROSI는 해로운 요청에 대한 거부를 매개하는 안전 방향을 해로운/무해한 명령어 쌍에서 추출하고, 이 방향을 정규화하여 각 레지듀얼 스트림 출력 가중치 행렬에 랭크-원(rank-one) 방식으로 가중치 수정으로 주입한다.  
이 수정은 α라는 스칼라 계수를 곱한 안전 방향 벡터와 원본 가중치 행렬 행들의 평균 벡터의 외적 형태로, 활성화가 거부 방향으로 항시 유도되도록 한다.  
해당 절차는 미세조정 없이 간단히 모든 층에 적용 가능하다.

## Results  
ROSI는 안전하게 정렬된 여러 LLM에서 해로운 요청 거부율을 크게 향상시키고, 다양한 우회 공격에 대한 견고성을 높이며, 제한된 성능 저하로 주요 벤치마크상의 유용성을 유지하는 것을 실험적으로 입증하였다.

## Limitations  
정보 부족.

## Conclusion  
ROSI는 모델 내부 안전 표현에 기반한 간결하고 효과적인 가중치 조작 기법으로, 저비용으로 기존 및 미정렬 모델의 안전성을 증폭시켜 보다 견고한 AI 안전 솔루션을 제공한다.

# 9. [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)

## Introduction
- 본 연구의 목표는 다중 카메라 영상을 활용하여 임의의 3D 점을 실시간으로 정확하고 강인하게 추적하는 데이터 기반 다중 시점 3D 점 추적기(MVTracker)를 개발하는 것이다.  
- 기존 단안 추적기가 깊이 불확실성과 폐색 문제에 취약하며, 다중 카메라 추적은 많은 카메라 수와 반복적인 최적화가 필요하다는 한계에서 동기를 얻었다.  
- 본 연구는 소수의 카메라(예: 4대)를 활용하여 다중 시점 특징을 통합한 3D 특징 점군과 kNN 상관관계를 기반으로 장거리 3D 점 궤적을 예측하는 최초의 피드포워드 다중 시점 3D 점 추적 모델을 제안한다.  

## Method  
입력된 다중 시점 RGB 및 깊이 데이터를 바탕으로, 각 시점의 특징 지도 및 깊이 맵을 3D 점군으로 융합하여 전역적인 공간 표현을 구성한다.  
이 융합된 3D 점군에서 각 추적 점 주변의 k-최근접 이웃과 다중 스케일 상관관계를 계산하며, 이를 활용한 변환기(Transformer) 기반 업데이트 모듈이 각 점의 위치 및 특징을 반복적으로 정제하고 폐색과 시점 변화에 적응한다.  
추적은 중첩된 시간 창 방식으로 수행되며, 각 연속 창의 최종 추정값을 다음 창의 초기값으로 활용하여 긴 영상에서도 일관성 있는 3D 궤적을 획득한다.  

## Results  
MVTracker는 Panoptic Studio, DexYCB, MV-Kubric 세 벤치마크에서 기존 단안 및 다중 시점 기반 기법들 대비 궤적 오차(MTE)를 3.1cm, 2.0cm, 0.7cm로 크게 낮추고, 위치 정확도(δavg)와 폐색 인식 정확도(AJ)에서도 우수한 성능을 기록하였다.  

## Limitations  
본 연구는 깊이 추정이 가능한 환경에서 동작하며, 깊이 정보의 정확도에 따라 성능이 다소 영향받을 수 있다는 한계가 존재한다.  

## Conclusion  
MVTracker는 소수의 다중 카메라 환경에서도 효율적이고 정확한 3D 점 추적을 가능하게 하는 최초의 데이터 주도 피드포워드 모델로서, 다중 시점 3D 추적 연구에 새로운 기준을 제시하고 실세계 응용에 실용적 도구를 제공한다.

# 10. [CogVLA: Cognition-Aligned Vision-Language-Action Model via   Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)

## Introduction
- Goal: 본 연구는 인간 인지 메커니즘에 영감을 받아 효율성과 성능을 동시에 개선하는 Cognition-Aligned Vision-Language-Action (CogVLA) 모델을 제안하는 것이다.  
- Motivation: 기존 VLA 모델들은 고비용의 후속 학습과 모달 간 의미적 결합 부족으로 인한 효율성 및 일관성 문제에 직면하였다.  
- Contribution: 본 논문은 EFA-Routing, LFP-Routing, CAtten 모듈을 포함하는 3단계 점진적 아키텍처를 설계하여, 명령어 기반의 시각 입력 희소화 및 일관된 행동 생성을 구현하였다.  

## Method  
CogVLA는 시각 인코더에서 명령어를 이용해 시각 토큰을 압축 및 집중시키는 EFA-Routing, 대형 언어 모델 내부에서 명령과 무관한 시각 토큰을 제거하는 LFP-Routing, 그리고 시각-언어-행동 간 인과적 및 양방향 연관성을 유지하는 CAtten으로 구성된 3단계 구조로 설계되었다. 이러한 모듈들은 인간의 시각-주의-운동 체계를 모방하여 멀티모달 인지 일관성을 확보하면서도 계산량을 크게 줄인다. 최종적으로 병렬 디코딩 방식을 채용해 효율적이고 정확한 행동 시퀀스 생성을 지원한다.  

## Results  
LIBERO 벤치마크에서 CogVLA는 97.4%의 최고 성공률을 달성하였으며, 실제 로봇 작업에서는 70.0%의 우수한 성능과 더불어 기존 모델 대비 훈련 비용을 2.5배, 추론 지연 시간을 2.8배 감소시키는 효율성을 입증하였다.  

## Limitations  
본 연구에서는 대용량 GPU 환경에서 원격 통신 지연이 존재하며, 이를 극복하기 위한 하드웨어 현지 실행 방안이 향후 과제로 남아있다.  

## Conclusion  
CogVLA는 인간 인지 메커니즘에 기반한 명령어 주도형 멀티모달 희소화 및 일관성 유지 기술로서, 차세대 확장 가능하고 효율적인 임베디드 AI 시스템 구축에 중요한 방향을 제시한다.
