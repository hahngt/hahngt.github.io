---
layout: post
title: "Daily Papers — 2025-12-02"
date: 2025-12-02 08:15:00
tags: [papers, hugginface]
categories: []
---


# 1. [TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models](https://arxiv.org/abs/2512.02014)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.02014)

## Introduction
- Goal: 본 논문은 이미지와 비디오의 이해 및 생성 작업을 하나의 통합된 연속적 시각 표현 공간에서 수행하는 네이티브 통합 멀티모달 모델 Tuna를 제안하는 데 목적이 있다.  
- Motivation: 기존 멀티모달 모델들은 이해와 생성 과제를 위한 시각 표현을 분리하여 표현 형식 불일치와 효율성 저하 문제를 겪는 반면, 통합된 시각 표현은 이 문제들을 해소하고 두 작업 간의 상호 향상을 가능하게 한다.  
- Contribution: Tuna는 VAE 인코더와 표현 인코더를 직렬로 연결하여 하나의 통합 시각 표현을 구축하고, 다양한 멀티모달 작업에서 기존 분리 표현 기반 모델들보다 우수한 성능을 입증하였다.  

## Method
Tuna는 3D 인과적 VAE 인코더로 입력 영상의 잠재 표현을 얻은 뒤, 이를 수정한 시그립(SigLIP) 표현 인코더에 입력해 통합된 시각 표현을 생성한다. 이 표현은 텍스트 토큰과 결합되어 대형 언어 모델(LLM) 디코더에 투입되며, 디코더는 이해 과제를 위한 자기회귀 텍스트 생성과 생성 과제를 위한 흐름 매칭 기반 영상 생성 헤드를 동시에 수행한다. 모델은 이미지 캡셔닝, 텍스트-이미지 생성, 영상 자막, 이미지 편집 등 다양한 데이터로 세 단계에 걸쳐 단계적 학습을 진행한다.  

## Results
Tuna는 여러 벤치마크에서 이미지 및 비디오 이해, 생성, 편집 분야 모두에서 현존 최고 수준의 성능을 달성하였으며, 특히 일관된 통합 표현 설계 덕분에 분리 표현 기반 모델 대비 모드별 성능 저하 없이 우수한 결과를 보였다.  

## Limitations
Tuna는 7B 파라미터 버전 학습 시 영상 데이터 포함에 높은 계산 비용이 소요되어 일부 영상 학습이 제한되었다는 한계가 존재한다.  

## Conclusion
통합 시각 표현을 기반으로 하는 Tuna는 멀티모달 이해와 생성 작업을 단일 프레임워크 내에서 성공적으로 통합하여 우수한 성능과 효율성을 실현하였다.

# 2. [LFM2 Technical Report](https://arxiv.org/abs/2511.23404)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.23404)

## Introduction
- Goal: LFM2는 엣지 디바이스에서 빠르고 메모리 효율적인 추론과 높은 작업 성능을 구현하기 위해 설계된 Liquid Foundation Models 패밀리이다.  
- Motivation: 휴대폰, 태블릿 등 엣지 환경에서는 엄격한 레이턴시와 메모리 제약 조건 아래에서 실시간 언어 모델 실행이 필수적임에도 불구하고 기존 대형 모델들은 엣지 디바이스에 적합하지 않았다.  
- Contribution: 하드웨어 인 더 루프 아키텍처 탐색을 통한 하이브리드 백본 설계, 온디바이스 맞춤 프리트레이닝 및 사후학습 파이프라인, 그리고 다양한 멀티모달 및 정보 검색 모델 변종을 공개하였다.  

## Method  
LFM2는 입력 의존적 게이트 단기 합성곱과 그룹 쿼리 어텐션을 조합한 최소한의 하이브리드 백본을 사용하여 CPU에서 전 모델 대비 최대 2배 빠른 처리 속도를 달성한다. 10~12조 토큰 크기의 대규모 사전학습 및 중간 학습, 템퍼드 분리형 Top-K 지식 증류, 단계별 사후학습(지도 미세조정, 선호도 최적화, 모델 병합)을 통해 작은 모델에서도 높은 성능을 보장한다. 또한 비전-언어, 오디오-언어 및 다중언어 정보 검색용 모델 등 멀티모달 변종을 개발하였다.  

## Results  
LFM2-2.6B 모델은 IFEval에서 79.56%, GSM8K에서 82.41%의 우수한 성능을 기록하며, CPU 기준 동급 모델 대비 최대 2배 빠른 프리필 및 디코딩 속도를 제공한다.  

## Limitations  
향후 CPU에서 MoE 모델의 하드웨어 활용 개선과 더 큰 규모 모델에서의 성능 및 효율성 향상이 필요하다.  

## Conclusion  
LFM2는 엣지 디바이스에서 실시간 고성능 자연어 처리 및 멀티모달 작업을 위한 효율적이고 확장 가능한 토대 모델 패밀리로서 실용적 기반을 마련하였다.

# 3. [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.00590)

## Introduction
- Goal: 본 연구는 대규모 언어 모델을 활용하여 위키데이터 기반의 온톨로지 제약을 반영한 고품질 지식 그래프를 자동으로 구축하는 파이프라인인 Wikontic을 제안하는 데 목적이 있다.  
- Motivation: 기존 LLM 기반 시스템들이 지식 그래프를 보조 구조로만 사용하여 지식 그래프의 본질적 품질과 추론 능력을 충분히 활용하지 못하는 문제점을 해결하고자 하였다.  
- Contribution: Wikontic은 후보 삼중항 추출, 온톨로지 제약 검증, 엔티티 정규화 및 중복 제거를 포함한 다단계 절차를 통해 위키데이터 정렬 및 온톨로지 일치 지식 그래프를 생성하며, 이를 통해 다중 홉 질의응답에서 최첨단 성능을 달성함을 입증하였다.  

## Method  
Wikontic은 위키데이터에서 추출한 온톨로지를 기반으로 후보 삼중항을 LLM으로부터 추출한 후, 유형과 관계에 대한 온톨로지 제약을 적용하여 검증 및 수정한다. 이후 엔티티 이름 정규화 및 동의어 처리를 통해 중복을 제거하여 일관되고 컴팩트한 지식 그래프를 완성한다. 생성된 지식 그래프는 다중 홉 질의응답을 위한 검색 및 추론에 활용된다.  

## Results  
Wikontic은 MINE-1 벤치마크에서 86% 이상의 정보 유지율을 기록하며 이전 방법들을 크게 능가하였고, HotpotQA와 MuSiQue 질의응답 데이터셋에서 텍스트 없이도 최고 수준의 F1 점수를 달성하였다.  

## Limitations  
본 연구는 주로 OpenAI와 Llama 등 제한된 대형 언어 모델을 사용하였으며, 파이프라인의 실시간 처리 지연 시간과 처리량에 대한 분석은 이루어지지 않았다.  

## Conclusion  
Wikontic은 위키데이터 온톨로지 기반의 LLM 추출을 결합하여 정확하고 효율적인 지식 그래프를 구축함으로써, 비구조적 텍스트를 해석 가능한 구조화 지식으로 변환하는 확장 가능하고 신뢰할 수 있는 접근법임을 입증하였다.

# 4. [Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.00891)

## Introduction
- 본 논문은 스트리밍 영상에서 대형 언어 모델(VideoLLMs)의 처리 속도를 향상시키는 방법을 제안하는 데 목적이 있다.  
- 연속되는 영상 프레임의 시각 토큰 처리에서 발생하는 높은 계산 비용과 지연 시간을 극복하는 것이 필요하기 때문이다.  
- 이를 위해 ViT 인코딩과 LLM 프리필(pre-filling) 단계를 모두 최적화하는 계층적 토큰 압축 프레임워크인 Streaming Token Compression(STC)를 제안하였다.  

## Method  
- STC는 두 가지 모듈로 구성되는데, STC-Cacher는 인접 프레임의 유사한 특징을 캐싱하여 ViT 인코딩에서 중복 계산을 줄이고, STC-Pruner는 시공간적 중요도에 기반해 LLM 입력 토큰을 선택적으로 압축하여 프리필 단계 부담을 감소시킨다.  
- STC-Cacher는 키 프로젝션의 코사인 유사도를 이용해 동적인 토큰을 판단하고, 정적인 토큰은 캐시된 정보를 재사용하여 계산 효율성을 높인다.  
- STC-Pruner는 과거 문맥을 대표하는 시공간 앵커(Temporal and Spatial Context Anchor)를 활용하여 토큰 중요도를 평가하고, 중요도가 낮은 토큰을 제거한다.  

## Results  
- 다양한 스트리밍 및 오프라인 장기 영상 이해 벤치마크에서 STC는 기존 방법 대비 최대 99%의 정확도를 유지하면서 ViT 인코딩과 LLM 프리필 단계의 지연을 각각 24.5%와 45.3%까지 줄여 실시간 처리 능력을 크게 향상시켰다.  

## Limitations  
- STC는 스트리밍 시나리오에 최적화되었으나, 완전한 전역 비디오 정보나 미래 사용자 쿼리를 활용할 수 없는 점에서 전통적인 오프라인 방식과는 차이를 보인다.  

## Conclusion  
- STC는 스트리밍 영상 처리에서 중복 계산과 긴 토큰 시퀀스 문제를 효과적으로 해결하며, 별도의 재학습 없이 기존 VideoLLMs에 플러그인 방식으로 적용 가능한 실시간 영상 이해 가속화 솔루션임을 입증하였다.

# 5. [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.00466)

## Introduction  
- Goal: 본 연구는 수학적 추론 문제 해결 시 서브 문제의 난이도에 따라 계산 자원을 선택적으로 할당하여 성능 병목 현상을 극복하는 SCALE 프레임워크를 제안하는 것이다.  
- Motivation: 기존의 테스트 시점 시간 확장 방법들은 모든 서브 문제에 균일하게 자원을 배분하여 간단한 문제에 과도한 자원이 할당되고 복잡한 문제에 자원이 부족하여 성능 향상이 제한되는 근본적인 문제가 존재하였다.  
- Contribution: 본 연구는 문제 분해, 난이도 평가, 난이도 기반 처리 모드 선택, 순차 실행의 네 단계로 구성된 SCALE을 통해 서브 문제 단위의 선택적 자원 분배를 실현하고, 이를 통해 정확도 향상과 자원 효율성 개선이라는 두 가지 목표를 동시에 달성하였다.  

## Method  
SCALE은 먼저 수학 문제를 논리적으로 순차화된 서브 문제들로 분해하고, 각 서브 문제의 난이도를 평가하여 단순 문제는 빠른 시스템 1 모드로, 복잡 문제는 심층적 시스템 2 모드로 처리한다. 이후 이전 문제 해결 결과를 상황 정보로 활용하며 서브 문제들을 순차적으로 해결하여 일관된 추론 체인을 유지한다. 이 과정에서 난이도 기준값을 설정하여 자원 할당의 선별성과 효율성을 조절한다.  

## Results  
SCALE은 AIME24, AIME25, AMC23 벤치마크에서 기존 CoT, InftyThink, Majority Voting 대비 최대 13.75% 포인트 정확도 향상을 달성하면서 계산 자원 사용을 33-53% 절감하였다.  

## Limitations  
난이도 기준값 설정에 따라 성능과 자원 사용량 간 트레이드오프가 존재하며, 적절한 임계값 탐색이 필요하다는 제약이 존재한다.  

## Conclusion  
SCALE은 수학적 추론 과정에서 서브 문제 난이도에 따른 선택적 자원 배분으로 성능 병목을 극복하고 높은 정확도와 효율성을 동시에 실현하는 테스트 시점 시간 확장 기법임이 실험을 통해 입증되었다.

# 6. [Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation](https://arxiv.org/abs/2511.17282)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.17282)

## Introduction
- Goal: 본 연구는 다국어 텍스트-이미지 생성 모델에서 문화적 일관성 부족 문제를 규명하고 이를 개선하는 방법을 제안하는 것이다.  
- Motivation: 텍스트 프롬프트가 각 언어의 문화적 함의를 반영하지 못해 생성된 이미지가 문화적으로 중립적이거나 영어 편향적 결과를 보이는 문제가 존재한다.  
- Contribution: 문화에 민감한 뉴런을 특정 계층 내에서 탐지하고, 이를 활용한 추론 시 활성화 및 계층별 미세조정 기법을 통해 문화적 일관성을 높이는 두 가지 정렬 전략과 다국가 문화 벤치마크(CultureBench)를 제안하였다.  

## Method  
문화 관련 신호가 소수 뉴런과 특정 계층에 집중됨을 확인 후, 첫째로 추론 시 이들 뉴런의 반응을 증폭시키는 무학습 방식, 둘째로 문화 민감 계층만 선택적으로 미세조정하는 방식을 도입하였다.  
문화 및 명사 매칭 기반 주의(attention) 분석과 Top-K SAE 기법으로 문화 민감 뉴런을 검출하였다.  
CultureBench를 이용한 평가 실험으로 제안 방법들의 문화적 일관성 향상을 검증하였다.  

## Results  
문화 민감 뉴런 활성화 및 계층별 미세조정 방식은 기존 최첨단 모델 대비 CultureVQA 점수를 최대 36.63으로 대폭 향상시키면서 시각적 품질과 텍스트-이미지 정합성도 유지하였다.  

## Limitations  
본 연구는 제한된 문화 및 언어권 데이터셋에 기반하여 실험을 수행하였으므로 광범위한 문화적 다양성을 완전히 포괄하지 못한다.  

## Conclusion  
문화 지식이 부족한 것이 아니라 문화 민감 표현의 약한 활성화가 문제임을 밝히고, 소수 뉴런과 계층 타겟팅을 통한 효율적인 문화 정렬 기법으로 다국어 텍스트-이미지 생성의 문화적 일관성 문제를 실용적으로 해결하였다.

# 7. [Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2512.01949)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.01949)

## Introduction
- Goal: 본 연구는 멀티모달 대형 언어 모델(MLLM)의 시각적 토큰 수 증가에 따른 메모리 소비와 추론 지연 문제를 완화하기 위한 효과적이고 쿼리 조건화된 토큰 프루닝 기법을 제안하는 데 목적이 있다.  
- Motivation: 기존 토큰 프루닝 방법들은 사용자 쿼리에 대한 적응성이 부족하거나 주의(attention) 메커니즘의 한계로 인해 쿼리 관련 토큰을 적절히 유지하지 못하는 문제점이 존재한다.  
- Contribution: Script라 명명한 학습 불필요하고 모델-범용적인 토큰 프루닝 방법을 제안하며, 이는 그래프 구조의 시각적 중복 제거와 쿼리 조건화된 의미 기반 토큰 선택 모듈을 결합하여 효율성과 정확도를 동시에 향상시킨다.  

## Method  
Script은 두 개의 주요 모듈로 구성된다. 첫째, 그래프 구조 프루닝(GSP)은 시각 토큰 간의 코사인 유사도를 이용해 중복된 토큰을 효율적으로 탐지하고 제거한다. 둘째, 쿼리 조건화 의미 프루닝(QCSP)은 입력 쿼리와 시각 토큰 간의 의미적 관련도를 측정하고 다양성을 유지하는 방식으로 중요 토큰을 선별한다. 최종 선택은 두 모듈의 결과를 교차하여 시각적 중복을 줄이는 동시에 쿼리에 적합한 토큰만을 남긴다.  

## Results  
14개의 이미지 및 영상 이해 벤치마크 실험에서 Script는 기존 프루닝 기법 대비 최대 10배 속도 향상과 96.88% 이상의 정확도 유지 성능을 보였다.  

## Limitations  
정보 부족.  

## Conclusion  
Script는 그래프 기반 중복 제거와 쿼리 조건화 토큰 선별을 결합한 혁신적 토큰 프루닝 기법으로, 멀티모달 대형 언어 모델의 효율성과 정확도를 동시에 향상시키는 데 성공하였다.

# 8. [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.00369)

## Introduction
- 본 연구의 목표는 확산 모델에서 DDIM 역변환 과정의 근사 잡음 오류를 근본적으로 최소화하는 적응형 가이드 스케일 스케줄링 기법을 제안하는 것이다.  
- 기존 역변환 방법에서는 인접 단계 간 잡음 예측의 차이로 누적되는 오류가 존재하며, 고정된 가이드 스케일 사용이 이 문제를 악화시킨다는 점에서 동기가 부여되었다.  
- 본 논문은 역변환을 단순한 오류 보상 문제에서 오류 발생 원점 최적화 문제로 재구성하고, 각 단계별로 최적의 가이드 스케일을 계산하는 POLARIS 기법을 제안하였다.  

## Method  
POLARIS는 역변환 시 단계별 가이드 스케일을 동적으로 조정하는 방식을 채택하여, 인접 시간 단계 잡음 예측의 차이를 최소화하는 정규 방정식에서 유도된 닫힌 해를 활용한다. 기존의 누적 히스토리 영향으로 인한 계산 불안정을 해소하기 위해 히스토리 항을 무시하고 단계별 오차를 직교 투영 법칙으로 최적화한다. 이 방법은 단 한 줄의 코드 추가만으로 DDIM 역변환 과정에 효율적으로 통합 가능하다.  

## Results  
COCO2017, Pick-a-Pic 그리고 EditBench를 포함하는 다양한 벤치마크에서 POLARIS는 고정 가이드 스케일 대비 평균 제곱오차(MSE) 감소, PSNR 및 SSIM 향상을 일관되게 나타냄으로써 복원과 편집 품질을 유의미하게 개선하였다.  

## Limitations  
POLARIS는 복잡한 비강체 객체 편집과 같은 고차원 비선형 변환에 대해서는 아직 적용 및 안정성 확보가 미흡한 한계가 존재한다.  

## Conclusion  
본 연구는 DDIM 역변환의 근사 잡음 오류를 근본적으로 줄이는 POLARIS를 제안하여, 고품질 이미지 복원과 편집을 위한 내재적 오류 최소화 기법을 실질적이고 효율적으로 구현하였으며, 향후 비강체 편집 및 동영상/3D 분야로의 확장 가능성을 제시한다.

# 9. [Generative Video Motion Editing with 3D Point Tracks](https://arxiv.org/abs/2512.02015)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.02015)

## Introduction
- 본 연구의 목표는 3D 포인트 트랙을 활용하여 카메라와 객체의 동작을 정밀하게 편집할 수 있는 비디오 모션 편집 프레임워크를 개발하는 것이다.  
- 기존의 모션 제어 이미지-비디오 변환(I2V) 방법은 장면 전체 컨텍스트가 부족하고, 비디오-비디오 변환(V2V) 방법은 세밀한 객체 모션 제어가 제한적이어서 복잡한 객체 움직임을 정확히 편집하는 데 한계가 존재한다.  
- 본 논문에서는 소스 비디오와 페어링된 3D 포인트 트랙을 조건으로 하는 트랙-컨디셔닝 V2V 모델을 제안하여, 카메라와 객체 모션의 공동 편집이 가능하며 깊이 정보 활용을 통해 가림 현상과 깊이 순서 문제를 효과적으로 해결하는 점을 기여로 제시한다.

## Method  
입력 비디오에서 카메라 자세와 3D 포인트 트랙을 추정하고, 사용자가 원하는 3D 궤적을 편집하여 목표 모션을 정의한다.  
소스 비디오 및 소스와 타겟 3D 트랙을 조건으로 하여, 텍스트-비디오 확산모델을 기반으로 하는 V2V 프레임워크가 목표 비디오를 생성한다.  
세밀한 3D 트랙 조건 인코딩을 위해 교차 어텐션 방식의 3D 트랙 컨디셔너를 도입하고, 합성 데이터와 실제 데이터를 이용한 2단계 세밀 튜닝으로 모델의 정밀도와 범용성을 확보한다.

## Results  
DyCheck, MiraData 등 실제 및 합성 데이터셋에서 제안 방법이 기존 트랙-컨디셔닝 I2V, 카메라 제어 V2V, 인페인팅 기반 V2V 기법들을 능가하여, 조인트 카메라 및 객체 모션 편집에서 정량적 및 정성적으로 우수한 성능을 보였다.

## Limitations  
제안 방법은 3D 포인트 트랙 추정의 노이즈와 불완전성에 영향을 받을 수 있으며, 일부 장면에서는 배경 트랙의 시간적 요동에 따른 왜곡이 발생할 수 있다.

## Conclusion  
본 연구는 3D 포인트 트랙 조건화 기반 V2V 모델을 통해 카메라와 객체 모션의 정밀한 공동 편집을 지원하며, 다양한 모션 편집 응용을 가능하게 하는 획기적 비디오 편집 기술을 제안하였다.

# 10. [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2512.00234)

## Introduction
- Goal: 본 논문은 멀티모달 기초 모델(MMFM)과 다국어 번역 대형언어모델(LLM)을 융합하여 동시에 멀티링구얼 멀티모달 번역을 수행하는 종단간 시스템을 제안하는 것을 목표로 한다.  
- Motivation: 기존의 음성 번역 모델들은 자동 음성 인식과 번역을 단계별로 수행하는 파이프라인 구조로 인해 지연이 크고, 이미지 등 멀티모달 문맥을 활용하지 못하는 한계가 존재하였다.  
- Contribution: 여러 층의 MMFM 은닉 상태를 번역 LLM과 융합하는 새로운 게이트 융합 방식을 도입하여, 음성-텍스트, 음성-이미지-텍스트, 텍스트-이미지-텍스트 번역을 동시에 처리하는 OmniFusion 모델을 구현하였다.  

## Method  
사전에 학습된 MMFM의 낮은 층부터 높은 층까지 은닉 상태를 추출하여 게이트 융합 레이어를 통해 가중치를 조절하며 번역 LLM에 통합하였다.  
이미지 모달리티에 대해서는 OCR을 중간 작업으로 도입하여 시각-음성 간 표현 정렬을 강화하였다.  
학습은 MMFM을 고정시키고 번역 LLM만 LoRA 방식으로 미세조정하여 계산 자원 효율성을 확보하였다.  

## Results  
SimulST(실시간 동시 음성 번역)에서 기존의 단계적 파이프라인 대비 약 1초의 지연 감소와 향상된 품질을 달성하였으며, 멀티모달과 다국어 환경에서 오프라인 및 캡션 번역 벤치마크 데이터셋에서도 최첨단 성능을 보였다.  

## Limitations  
본 연구는 고정된 작업별 프롬프트만 지원하여 범용적인 명령 수행 능력은 미흡하며, 주로 영어를 소스 언어로 사용하여 다국어 영상-음성 입력에 대한 제로샷 실험이 부족하였다.  

## Conclusion  
본 연구는 MMFM과 전문 번역 LLM의 층별 은닉 정보 융합을 통해 멀티모달 문맥을 효율적으로 활용하는 종단간 다국어 동시 번역 시스템을 제안하고, 이를 기반으로 시연된 실험에서 실시간성, 번역 품질 및 시각 정보 활용 효용성을 모두 향상시켰음을 확인하였다.

# 11. [Structured Extraction from Business Process Diagrams Using Vision-Language Models](https://arxiv.org/abs/2511.22448)

> [Alphaxiv](https://www.alphaxiv.org/ko/overview/2511.22448)

## Introduction
- Goal: 본 연구의 목표는 비전-언어 모델(Vision-Language Models, VLMs)을 활용하여 BPMN 비즈니스 프로세스 다이어그램 이미지로부터 원본 파일이나 주석없이 직접 구조화된 JSON 표현을 추출하는 것이다.  
- Motivation: BPMN 다이어그램이 주로 시각적 이미지로 교환되지만, 기존 방법은 XML 표현에 의존하여 원본 소스 파일이 없는 경우가 많아 의미 기반 프로세스 데이터 활용에 어려움이 존재한다.  
- Contribution: VLM 기반 프롬프트를 통한 BPMN 요소 인식 및 분류 파이프라인을 제안하고, OCR을 통한 텍스트 보강 기법을 결합하여 다양한 모델들을 벤치마킹하며 자세한 성능 분석과 데이터셋을 공개하였다.  

## Method  
BPMN 다이어그램 이미지를 입력으로 하여, 다이어그램의 시각적 및 의미적 특성을 설명하는 프롬프트와 함께 VLM이 JSON 형태의 구조화된 결과를 생성하도록 하였다.  
OCR 기법을 활용해 텍스트 누락 문제를 보완하고, 추출된 JSON의 일관성과 정확성을 유지하기 위해 후처리 및 보강 과정을 적용하였다.  
여러 VLM 및 OCR 방법을 조합한 하이브리드 방식과 단독 VLM 방식을 비교 평가하였다.  

## Results  
GPT-4.1, Mistral-Small-3.1 등 최상위 모델이 OCR 없이도 높은 F1 점수를 달성하였고, 중간 수준 모델들은 OCR 보강 시 성능이 유의미하게 향상되었으며, 낮은 등급 모델은 OCR 활용 시 노이즈로 인해 오히려 성능 저하가 관찰되었다.  

## Limitations  
관계 및 방향성 인식이 필요한 게이트웨이, 시퀀스 플로우 등 복잡한 제어 흐름 구성요소에서는 여전히 높은 오류율을 보여 모델의 시각적 정보만으로는 한계가 존재한다.  

## Conclusion  
본 연구는 VLM과 OCR을 결합한 프롬프트 기반 방법이 원본 BPMN 파일 없이도 이미지에서 구조화된 프로세스 정보를 효과적으로 추출할 수 있음을 입증하며 BPMN 다이어그램 해석 연구에 실질적인 기여를 제공한다.
