---
layout: post
title: Daily Papers — 2025-09-12"
date: 2025-09-12 08:15:00
tags: [papers, hugginface]
categories: []
---

# 1. [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)

## Introduction

- Goal: 본 연구는 소형 규모의 비전-언어-행동(Vision-Language-Action, VLA) 모델에서 비전-언어 표현을 효과적으로 행동으로 연결하는 새로운 패러다임인 VLA-Adapter를 제안하는 데 목적이 있다.
- Motivation: 기존 VLA 모델은 대규모 비전-언어 모델(VLM)에 의존하며 높은 학습 비용, 느린 미세조정 속도, 많은 GPU 메모리 사용 및 낮은 추론 효율성 등의 문제를 갖는다.
- Contribution: VLA-Adapter는 소형 백본만으로 로봇 데이터 사전학습 없이도 최적 조건을 정책 네트워크에 주입하는 Bridge Attention 모듈을 도입하여 뛰어난 성능과 효율성을 보임을 증명하였다.

## Method

VLA-Adapter는 VLM에서 추출한 다중 층의 Raw latent와 ActionQuery latent를 통합하여 정책 네트워크에 조건으로 주입하며, Bridge Attention 구조를 통해 행동 생성에 필요한 멀티모달 지식을 효과적으로 전파한다. 정책 네트워크는 LayerNorm과 MLP를 포함하는 L1 기반 구조로 구성되어 있으며, 자동 학습 가능한 파라미터를 통해 Raw latent 정보의 주입 정도를 조절한다. 이를 통해 소형 모델로도 대규모 사전학습 없이 높은 성능을 달성할 수 있다.

## Results

LIBERO 및 CALVIN 벤치마크와 실제 로봇 작업에서 VLA-Adapter는 0.5B 파라미터 백본을 사용함에도 불구하고 기존 최고 수준 기법 대비 최대 9.2% 높은 성공률과 3배 이상 빠른 추론 속도를 보여주었다.

## Limitations

VLA-Adapter는 대규모 임베디드 데이터 사전학습이 없고 백본 크기가 작아 현실 시스템에서의 일반화 능력과 행동 품질 향상 측면에서 추가 연구가 필요하다.

## Conclusion

본 연구는 VLA-Adapter를 통해 대규모 VLM 및 고비용 학습에 의존하지 않고도 소형 모델로 우수한 VLA 성능과 효율성을 실현하여 VLA 모델 보급의 장벽을 크게 낮춘 혁신적인 방법론을 제시하였다.

# 2. [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)

## Introduction

- Goal: 본 연구는 텍스트, 참조 이미지, 오디오의 삼중 조건을 활용하여 인간 중심의 영상 생성에서 다중 모달 조건을 협력적으로 제어하는 통합 프레임워크 HuMo를 제안하는 것이다.
- Motivation: 기존 인간 중심 영상 생성 기법은 삼중 입력 조건의 부족한 데이터와 주제 고정 및 음성-영상 동기화 과제 간 협력의 어려움으로 인해 모달 간 효과적 통합에 한계가 존재하였다.
- Contribution: HuMo는 고품질 삼중 조건 데이터셋 구축과 단계적 다중 모달 학습 전략, 시간 적응형 Classifier-Free Guidance 기법을 도입하여 텍스트, 이미지, 오디오 모달의 협력적 통제 성능을 획기적으로 향상시켰다.

## Method

HuMo는 대규모 비디오와 이미지 데이터에서 일관된 주제의 다양한 참조 이미지와 정렬된 음성 데이터를 추출하는 두 단계 데이터 처리 파이프라인을 구축하였다.  
DiT 기반 텍스트-투-비디오(T2V) 모델에 최소 침습 이미지 삽입과 오디오 교차 어텐션 모듈, 얼굴 지역 예측 기반 집중 전략을 결합한 점진적 다중 모달 학습법을 적용하여 주제 보존과 음성-영상 동기화를 공동 학습하도록 설계하였다.  
추론 시에는 각 모달별 가중치를 시간에 따라 동적으로 조정하는 시간 기반 CFG 방식을 도입해 세 가지 모달 조건의 유연하고 정밀한 협력적 컨트롤을 가능하게 하였다.

## Results

HuMo는 주제 보존 및 음성-영상 동기화 주요 하위 과제에서 기존 최첨단 기법들보다 우수한 정성·정량 성능을 달성했으며, 1.7B 및 17B 파라미터 모델 모두에서 뛰어난 확장성과 효율성을 보였다.

## Limitations

음성-aligned 데이터가 인간 중심 비디오에 주로 존재하여 비인간 객체에 대한 오디오 동기화 제어는 데이터 측면에서 제한적이다.

## Conclusion

HuMo는 대규모 다중 모달 데이터 처리와 점진적 학습, 동적 컨트롤 기법을 통합하여 인간 중심 영상 생성 분야에서 텍스트, 이미지, 오디오 모달 공동 제어의 새로운 표준을 제시하였다.

# 3. [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)

## Introduction

- Goal: 본 논문은 Vision-Language-Action (VLA) 모델의 훈련을 강화학습(Reinforcement Learning, RL) 기반으로 확장하는 방법을 제안하는 것이다.
- Motivation: 기존의 감독학습 기반 미세조정은 대규모 인간 조작 궤적 데이터의 희소성과 비용 문제, 그리고 분포 변화에 따른 일반화 한계에 직면한다.
- Contribution: SimpleVLA-RL이라는 효율적이고 확장 가능한 VLA 전용 RL 프레임워크를 개발하여 데이터 부족 문제 해결과 장기 계획 능력 향상에 기여하였다.

## Method

SimpleVLA-RL은 veRL 프레임워크를 확장하여 VLA 특화된 궤적 샘플링, 병렬 환경 렌더링 및 최적화된 손실 계산 방식을 도입하였다.  
복수의 시뮬레이션 환경을 동시에 운용하며 결과 기반 이진 보상으로 정책을 학습하고, 탐험 강화를 위한 동적 샘플링, 클리핑 범위 확대 및 높은 온도 샘플링 기법을 적용하였다.  
이와 같은 기법들은 PPO 계열의 Group Relative Policy Optimization (GRPO) 알고리즘을 통해 안정적이고 효율적인 온라인 RL 훈련을 가능하게 하였다.

## Results

LIBERO, RoboTwin1.0 및 RoboTwin2.0 시뮬레이션 벤치마크에서 SimpleVLA-RL은 SFT 대비 최대 30.5% 높은 성공률을 기록하며 SoTA 성능을 달성하고, 극히 제한된 시연 데이터에서도 기존 방법보다 뛰어난 일반화 및 장기 과제 수행 능력을 입증하였다.

## Limitations

본 연구는 RL 학습 과정에서 나타나는 일부 실패 사례 및 복잡한 보상 설계 부재에 따른 한계점을 논의하고 있으나, 구체적 해결책은 추가 연구가 필요하다.

## Conclusion

SimpleVLA-RL은 소규모 시연 데이터에서도 강인한 일반화와 실환경 적용 가능성을 보이며, VLA 모델 훈련의 데이터 및 일반화 문제를 극복할 수 있는 효과적인 RL 기반 접근법임을 확인하였다.

# 4. [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)

## Introduction

- Goal: 본 연구는 음성-음성 대규모 언어 모델(Speech-to-Speech LLM, SLLM)에서 음향-의미 간극(acoustic-semantic gap)을 완화하는 방법을 제안하는데 목적이 있다.
- Motivation: 기존 SLLM은 텍스트 기반 LLM에 비해 지식과 추론 능력이 저하되는 문제가 음향과 의미 표현 간 불일치에서 비롯된다는 가설에 근거한다.
- Contribution: 본 논문은 의미 표현을 활용하고 동적으로 음성 학습 목표를 생성하는 EchoX라는 훈련 프레임워크를 제안하여 SLLM의 지능 저하 문제를 개선하였다.

## Method

EchoX는 (1) 텍스트 기반 LLM을 음성-텍스트 대화 LLM으로 전환하고, (2) 텍스트를 음성 토큰으로 변환하는 텍스트-투-코덱(Text-to-Codec) 모듈을 사전학습하며, (3) 두 모듈을 통합해 의미적 은닉 상태에서 음성 토큰을 생성하는 Echo 훈련 전략을 적용한다. 이러한 접근은 음향적 정확성에 치우친 기존 SLLM 훈련의 문제를 극복하고, 단위 언어(unit language)를 이용한 압축된 음성 토큰과 스트리밍 생성 기법으로 긴 음성 시퀀스 문제도 해결하였다.

## Results

약 6천 시간의 학습 데이터만으로 EchoX는 다양한 지식 기반 질문 응답 벤치마크에서 기존 모델과 비교해 경쟁력 있는 성능을 달성하였다.

## Limitations

정보 부족.

## Conclusion

EchoX는 음향-의미 간극 문제를 완화하는 새로운 훈련 방법과 음성 토큰 표현을 도입하여 적은 데이터로도 SLLM의 지적 능력을 효과적으로 유지할 수 있음을 입증하였다.

# 5. [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)

## Introduction

- Goal: 본 논문은 멀티모달 지시문을 통합하여 장기간 고품질의 사실적인 아바타 애니메이션을 생성하는 Kling-Avatar 프레임워크를 제안하는 것을 목표로 한다.
- Motivation: 기존 오디오 기반 아바타 영상 생성 방법은 단순한 저수준 추적에 그쳐 지시문의 의도와 내러티브 일관성 및 캐릭터 표현력에 한계가 존재하였다.
- Contribution: 본 연구는 멀티모달 대형언어모델(MLLM)을 활용한 지시문 해석과 스토리라인 계획을 통해 전역적 의미를 반영하는 2단계 병렬 생성 구조를 제안하였다.

## Method

Kling-Avatar는 MLLM 디렉터가 오디오, 이미지, 텍스트 지시문을 해석하여 고수준의 스토리라인을 생성한다. 1단계에서는 이를 기반으로 청사진 영상이 제작되고, 2단계에서는 청사진의 핵심 프레임을 활용하여 병렬로 장시간 영상을 세밀하게 합성한다. 영상 데이터는 엄격한 품질 필터링 과정을 거쳐 선별되며, 훈련과 추론 시에는 입술 동기화와 정체성 보존을 강화하는 전략이 적용된다.

## Results

375개의 다양한 시나리오 샘플로 구성된 벤치마크 평가에서 Kling-Avatar는 OmniHuman-1과 HeyGen 대비 전반적인 품질과 입술 싱크 정확도, 시각적 표현, 명령어 반응성 및 정체성 일관성 등 대부분 평가 지표에서 우수한 성능을 입증하였다.

## Limitations

한계점에 대한 구체적인 언급은 존재하지 않는다.

## Conclusion

본 연구는 멀티모달 지시문 기반의 고품질 장시간 아바타 영상 생성에 있어 새로운 기준을 제시하며 실제 디지털 휴먼 응용에 강력한 가능성을 보여준다.

# 6. [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)

## Introduction

- Goal: 본 논문은 장기간의 다단계 작업에서 희박한 결과 기반 보상으로 인한 중간 단계에 대한 크레딧 할당 문제를 해결하는 것을 목표로 한다.
- Motivation: 기존 강화학습 기법들은 정책 그래디언트 크기가 정책 엔트로피와 결합되어 있어 확신 있는 행동은 업데이트가 작아 학습 속도가 느리고, 불확실한 행동에서는 불안정한 큰 업데이트가 발생하는 근본적 문제를 갖고 있었다.
- Contribution: 본 연구는 단계별 불확실성과 최종 결과를 고려하여 정책 그래디언트를 재조정하는 Entropy-Modulated Policy Gradients(EMPG) 프레임워크를 제안하였다.

## Method

EMPG는 단계별 토큰 수준 엔트로피를 불확실성 척도로 활용하여 정책 그래디언트에 자기 보정 스케일링을 적용하고, 확신 있는 행동 시 업데이트를 증폭시키고 불확실한 행동에서는 완화시키는 방식을 도입하였다. 또한 미래 상태의 엔트로피를 최소화하도록 유도하는 미래 명확성 보너스를 포함하여 예측 가능하고 안정적인 해결 경로 탐색을 촉진한다. 이 두 가지 구성 요소가 결합되어 희박한 보상 신호에서 밀도 있고 정보량 높은 학습 신호를 생성한다.

## Results

WebShop, ALFWorld, Deep Search와 같은 장기간 작업 벤치마크에서 EMPG는 GRPO와 DAPO 같은 강력한 정책 그래디언트 기반 알고리즘 대비 최대 8.1포인트 성공률 향상을 포함한 유의미한 성능 개선을 보였다.

## Limitations

과도한 인간 주석 비용과 합성 데이터 상의 노이즈 문제 등 기존 프로세스 보상 모델의 한계를 여전히 완전히 극복하지는 못하였다.

## Conclusion

EMPG는 LLM 기반 장기간 작업 에이전트의 학습에서 내재된 불확실성을 효과적으로 활용하여 희박한 보상 문제를 완화하며, 다단계 크레딧 할당 및 학습 안정성을 크게 향상시키는 일반화 가능한 정책 그래디언트 재조정 기법임이 입증되었다.

# 7. [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)

## Introduction

- Goal: 본 연구는 대규모 추론 중심 텍스트-이미지 생성 모델 학습과 평가를 위한 FLUX-Reason-6M 데이터셋과 PRISM-Bench 벤치마크를 제안하는 데 목적이 있다.
- Motivation: 기존 텍스트-이미지 생성 연구는 대규모, 고품질 추론 데이터셋과 인간 평가에 부합하는 종합적 벤치마크 부재로 인해 최신 폐쇄형 모델과 성능 격차가 존재하였다.
- Contribution: 600만 개 이미지와 2천만 개의 중국어-영어 이중언어 캡션, 6가지 핵심 특성 및 Generation Chain-of-Thought(GCoT)를 포함한 대규모 추론 데이터셋과, 7개 트랙으로 구성된 세밀하고 신뢰성 높은 평가 프레임워크를 제공한다.

## Method

FLUX-Reason-6M은 고성능 FLUX.1-dev 모델과 비전-언어 모델을 통해 데이터 합성, 필터링 및 다차원 분류, 밀집 캡션 생성, GCoT 설계로 구성된 체계적 데이터 구축 파이프라인을 수행한다.  
6가지 특징(Imagination, Entity, Text rendering, Style, Affection, Composition)을 다중 라벨 방식으로 부여하여 복합적 장면 구성과 추론 능력 학습을 지원하며, 각 이미지에 대해 상세한 생성 과정과 논리를 서술한 GCoT 캡션을 포함한다.  
PRISM-Bench는 7개 평가 트랙별로 700개의 대표 및 도전적 프롬프트를 수집, GPT-4.1 및 Qwen2.5-VL-72B의 고도화된 시각-언어 이해 능력을 활용해 세밀한 정렬도와 미적 완성도를 평가하는 프로토콜을 제안한다.

## Results

PRISM-Bench를 통해 19개 선도 텍스트-이미지 모델의 평가를 진행한 결과, 폐쇄형 모델과 오픈 소스 모델 간 성능 격차가 확인되었으나 모든 모델이 특정 차원에서는 개선 여지가 있음을 시사하였다.

## Limitations

현재 데이터셋과 벤치마크는 복잡한 장면 및 길이 긴 텍스트 기반 생성 능력을 평가하나, 실제 응용에서 발생할 수 있는 모든 추론 시나리오를 포괄하지 못한다.

## Conclusion

본 연구의 FLUX-Reason-6M 데이터셋과 PRISM-Bench는 추론 중심 텍스트-이미지 생성 모델 개발 및 평가를 위한 새로운 표준을 제시하며, 오픈 소스로 공개되어 연구자들의 접근성과 활용성을 크게 증진시킨다.

# 8. [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)

## Introduction

- Goal: 본 논문은 멀티모달 이해와 생성이 단순 공존이 아닌 상호 시너지를 낼 수 있는 통합 모델 학습을 제안하는 데 목적이 있다.
- Motivation: 기존 연구들은 이해와 생성을 별개의 과제로 분리하여 학습함으로써 두 작업 간 상호 이익을 놓치는 한계가 존재하였다.
- Contribution: 본 연구에서는 오토인코더 관점에서 이해를 인코더(I2T), 생성을 디코더(T2I)로 통합하고, 재구성 정확도를 단일 목적함수로 삼는 UAE 프레임워크와 강화학습 기반 Unified-GRPO를 제안하였다.

## Method

UAE는 700K 이상의 고품질 장문 이미지-캡션 데이터로 디코더를 사전학습하고, 세 단계 강화학습으로 인코더와 디코더를 상호 보완적으로 최적화한다. Generation for Understanding 단계에서는 인코더가 디코더 재구성 품질을 극대화하는 설명을 생성하도록 학습하며, Understanding for Generation 단계에서는 디코더가 상세 캡션으로부터 정확히 재구성하도록 개선한다. 강화학습은 CLIP 기반 재구성 유사도를 보상으로 사용하여 양방향 정보 흐름을 강화한다.

## Results

Unified-Bench에서 UAE는 전체 통합 점수 86.09로 최상위 성능을 기록했으며, GenEval, GenEval++, DPG-Bench 등에서 구성 이해 및 명령 수행 능력 모두에서 우수한 결과를 보였다.

## Limitations

텍스트 렌더링 부분에서는 OCR 인지 강화 학습이 미흡하여 고해상도 텍스트 정확성에 한계가 존재한다.

## Conclusion

본 연구는 오토인코더 원칙에 기반한 단일 재구성 목표가 이해와 생성의 진정한 통합과 상호 이익 달성에 효과적임을 실증하였다.

# 9. [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)

## Introduction

- Goal: 본 논문은 대규모 언어 모델에 다수의 인-컨텍스트 예시를 학습시켜 표 형식의 예측 작업에서 강인한 성능을 갖는 머신러닝 능력을 부여하는 지속적 사전학습 프레임워크인 MACHINELEARNINGLM을 제안하는 데 목표를 둔다.
- Motivation: 기존 대형 언어 모델들은 많은 샷의 인-컨텍스트 학습에서 정확도 향상이 제한적이며 표 형식 머신러닝 과제에서 성능이 저하되는 문제가 존재한다.
- Contribution: MACHINELEARNINGLM은 수백만 개의 합성 표 형식 예측 작업과 랜덤 포레스트 교사 모델의 지식을 활용한 토큰 효율적 프롬프트 설계를 통해 다수 샷 학습의 확장성과 범용적인 추론능력을 동시에 달성하였다.

## Method

MACHINELEARNINGLM은 구조적 인과 모델(SCM) 기반 합성 데이터셋으로 표 형식 다중 클래스 및 이진 분류 과제를 생성하고 약 3백만 개의 작업을 활용하여 지속적 사전학습을 수행한다. 랜덤 포레스트 교사 모델을 통한 초기 학습 안정화와 셀프 컨시스턴시 기반 예측 집계 기법을 도입하여 모델의 수치적 강건성과 순서 민감도를 개선한다. 또한, 토큰 효율이 높은 표 형식 인코딩과 정수 형태의 수치값 표현, 시퀀스 단위 배치 추론을 통하여 훈련 및 추론에서의 문맥 길이 제약을 완화한다.

## Results

MACHINELEARNINGLM은 Qwen-2.5-7B-Instruct 백본 대비 평균 약 15% 향상된 out-of-distribution 탭률 분류 정확도를 도메인 전반에 걸쳐 달성하며, 최대 1,024샷까지 정확도가 지속적 증가하는 뚜렷한 다수 샷 확장 법칙을 보였다.

## Limitations

본 연구는 현재 표 형식 머신러닝에만 초점을 맞추고 있으며, 기타 머신러닝 분야로의 확장은 향후 연구 과제로 남겨두었다.

## Conclusion

MACHINELEARNINGLM은 합성 데이터 기반 지속적 사전학습과 효율적 토큰 프롬프트 설계를 통해 일반 목적 대형 언어 모델에 강력한 다수 샷 인-컨텍스트 머신러닝 능력을 성공적으로 확장하였다.

# 10. [AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs](https://arxiv.org/abs/2509.08031)

## Introduction

- Goal: 본 논문은 대규모 오디오 언어 모델(Audio LLMs, LALMs)의 포괄적이고 효율적인 평가를 위한 오픈소스 툴킷인 AU-Harness를 제안하는 것을 목표로 한다.
- Motivation: 현재 오디오 LLM 평가 툴킷은 처리 속도가 느리고, 불일치한 프롬프트와 제한적인 평가 과제 범위로 인해 공정하고 체계적인 비교가 어렵다는 문제점이 존재한다.
- Contribution: AU-Harness는 병렬 처리와 배치 최적화를 통해 기존 대비 최대 127% 향상된 처리 속도를 제공하며, 표준화된 프롬프트와 확장된 평가 과제(LLM-적응형 분할 및 음성 언어 추론)를 포함한 포괄적인 평가 프레임워크를 구축하였다.

## Method

AU-Harness는 토큰 기반 요청 스케줄링과 데이터 샤딩을 활용해 멀티 노드 및 하드웨어 아키텍처에서 평가를 масштаб화하며, 다양한 모델과 작업에 대해 표준화된 프롬프트와 사용자 맞춤형 구성을 지원한다. LLM-적응형 분할은 스피커 식별과 타임스탬프를 포함한 프롬프트 기반 접근을 사용하고, 음성 언어 추론 과제는 복잡한 오디오 지시문을 텍스트-음성 변환을 통해 구성하여 LALMs의 심층적 인지 능력을 평가한다. 시스템은 다중 엔진 동시 실행과 요청 오류 재시도 기능을 갖추어 신뢰성과 처리량을 향상시킨다.

## Results

AU-Harness는 다양한 음성 데이터셋과 380개 이상의 과제를 대상으로 평가를 수행한 결과, 기존 평가 툴킷 대비 처리량을 최대 127% 개선하고 실시간 처리 시간은 약 59% 단축하여 대규모 LALM 평가에 이상적인 프레임워크임을 검증하였다.

## Limitations

본 프레임워크는 vLLM 등 특정 백엔드 의존성이 존재하며, 개방형 프롬프트 표준화에도 불구하고 완전한 재현성과 시간 정밀도 평가에는 한계가 존재한다.

## Conclusion

AU-Harness는 효율성과 확장성을 갖춘 표준화된 평가 환경을 제공하여 LALMs의 체계적 연구와 실제 적용을 촉진하며, 오디오 언어 모델 평가 생태계 발전에 기여한다.

# 11. [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)

## Introduction

- 목표는 대규모 실제 영상 데이터를 기반으로 다채로운 장면과 카메라 움직임, 밀도 높은 3D 주석 정보를 포함한 SpatialVID 데이터셋을 구축하는 것이다.
- 기존 데이터셋들이 실제 동적 장면의 지리학적 정보와 태깅의 풍부함에서 한계가 있어 공간 인공지능 연구 확장에 제약이 존재한다는 점에서 동기가 부여되었다.
- 본 연구는 7,089시간 분량의 고품질 동적 영상과 카메라 자세, 깊이 맵, 구조화된 캡션, 동작 지시문 등 상세한 공간 주석을 포함하는 세계 최대 규모의 영상 데이터셋 SpatialVID를 새롭게 제안하였다.

## Method

- 2만 시간 이상의 원천 영상을 유튜브에서 모은 뒤, 미적 품질, 조도, OCR, 모션 강도 기반의 다차원 필터링 과정을 통해 7,089시간 분량의 핵심 클립을 계층적으로 선별하였다.
- MegaSaM 기법과 최첨단 모노큘러 깊이 추정 모델을 활용하여 각 클립별 카메라 자세와 깊이 맵을 정확하게 추출하고, SAM2 모델을 이용한 동적 객체 분할을 통해 영상 내 움직임 정보를 정밀하게 주석화하였다.
- 시각-언어 모델과 대규모 언어 모델을 결합해 공간 정보를 통합한 구조화된 캡션과 카메라 운동을 텍스트화한 동작 지시문 생성 파이프라인을 구축하였다.

## Results

- SpatialVID는 270만 개 이상의 고품질 영상 클립과 1,276억 개의 캡션 단어를 포함하며, 기존 공간 정보 주석 영상 데이터셋 대비 압도적인 스케일과 다양성을 갖춰 공간 인공지능 및 3D 비전 연구에서 뛰어난 일반화 성능을 확보한다.

## Limitations

- MegaSaM 기반의 카메라 추정은 극단적인 움직임이나 컬리니어 동작 상황, 다양한 초점 거리 및 왜곡 문제에서는 여전히 성능 저하가 발생한다.

## Conclusion

- SpatialVID는 실제 동적 장면에 정밀한 3D 공간 및 의미 주석을 결합한 대규모 영상 데이터셋으로, 공간 인공지능과 영상 생성 분야의 모델 학습과 평가를 위한 핵심 자원으로 자리매김할 것이다.

# 12. [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)

## Introduction

- Goal: 본 연구는 차트 이해에서 시각적 프로그램화(Visual Programmability)를 활용하여 작업에 최적화된 추론 경로를 동적으로 선택하는 적응적 코드 기반 추론 프레임워크를 제안하는 것이다.
- Motivation: 기존 차트 이해 접근법은 외부 도구 의존성 또는 단일 추론 전략 사용으로 인해 복잡한 실세계 차트에 대한 일반화 성능이 저하되는 한계를 가진다.
- Contribution: 시각적 프로그램화 개념을 도입하고, 강화학습 기반의 이중 보상 체계를 적용하여 차트별로 코드 활용 여부를 판단하는 적응형 VLM 학습 방식을 개발하였다.

## Method

본 연구는 차트와 질문 쌍의 시각적 프로그램화 수준을 학습하고, 이에 따라 코드 기반(Code-as-Thought) 또는 직접 시각 추론 경로를 선택하는 모델 정책을 강화학습으로 학습한다.  
데이터 정확도 보상과 선택 전략 보상을 포함하는 다중 보상 함수를 이용하여 모델이 명확하고 신뢰성 있는 코드 생성과 적절한 전략 판단을 병행하도록 설계하였다.  
이 접근법은 Qwen2.5-VL 모델을 기반으로 하여 다양한 데이터셋에 걸쳐 적응적 추론 경로 선택을 가능하게 한다.

## Results

제안된 적응형 프레임워크는 ChartX, ChartBench, ChartQA, CharXiv 등 다양한 벤치마크에서 최고 평균 정확도 62.8%를 달성하며, 기존의 고정 전략 모델을 능가하였다.

## Limitations

본 연구는 모델 크기 및 복잡성에 따른 적응형 전략 선택 성능 차이가 존재하며, 소형 모델에서는 추론 전략 선택 능력이 다소 제한되는 한계가 확인되었다.

## Conclusion

본 연구는 차트 이해 문제에서 단일 추론 전략의 한계를 극복하기 위해 시각적 프로그램화에 기반한 적응형 추론 전략 선택을 통해 보다 유연하고 강건한 멀티모달 추론 시스템 구축이 가능함을 입증하였다.

# 13. [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)

## Introduction

- 본 연구는 3조 토큰과 1833개 언어를 대상으로 사전학습한 최신 다국어 인코더 모델 MMBERT를 제안하는 것을 목표로 한다.
- 기존 인코더 전용 다국어 모델에 대한 최근 연구가 부족하고, 특히 대규모 다국어 지원과 성능 향상을 위한 새로운 학습 기술의 필요성이 존재한다는 점에서 동기가 부여되었다.
- 본 논문은 역마스킹 비율 스케줄과 언어 샘플링 온도 조절, 그리고 학습 단계별 언어 수 증가를 포함하는 annealed language learning을 도입하여 MMBERT를 개발하고 기존 모델 대비 뛰어난 성능을 보임을 기여한다.

## Method

MMBERT는 ModernBERT 기반 아키텍처를 사용하며 Gemma 2 토크나이저로 다국어 입력을 처리한다. 학습은 세 단계로 진행되며, 초기에는 고자원 언어 60개를 중심으로 학습하고 중간 및 감소(Decay) 단계에서 점차 110개, 최종적으로 1833개 언어를 포함해 정규화된 온도 샘플링 방식을 적용한다. 역마스킹률을 점차 낮춤으로써 최적화된 사전학습 조건을 구현하며, 다양한 고품질 데이터셋을 활용하여 학습한다.

## Results

MMBERT는 영어 및 다국어 이해, 검색 과제에서 기존 최고 성능 모델인 XLM-R은 물론 OpenAI의 o3와 구글 Gemini 2.5 Pro를 능가하며 특히 저자원 언어에서 2배 이상의 성능 향상을 기록하였다.

## Limitations

고품질 데이터가 부족하거나 전혀 없는 저자원 언어들이 여전히 다수 존재하며, 이들에 대한 추가 데이터 확보 및 학습법 개발이 필요하다.

## Conclusion

MMBERT는 신경망 인코더 모델의 현대적 학습 기법과 대규모 다국어 확장을 결합하여 기존 다국어 인코더 모델 대비 전반적인 성능과 효율성을 크게 향상시켰으며 공개 자원으로 기여한다.

# 14. [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)

## Introduction

- 본 연구의 목표는 텍스트 기반 인물 검색에서 강인한 표현 학습을 위해 CLIP 모델을 개선하는 것이다.
- 기존 CLIP은 인물 중심 이미지-텍스트 데이터의 부족과 전역 대조 학습의 한계로 세밀한 특징 학습과 노이즈 텍스트 처리에 취약하다는 문제가 존재한다.
- 본 연구는 대규모 고품질 데이터셋 구축과 새로운 이중 마스킹 학습 프레임워크를 통해 텍스트-이미지 모달 정렬과 세밀한 의미 표현 학습을 향상시킨다.

## Method

본 연구에서는 웹에서 수집한 데이터를 MLLM을 활용해 자동 필터링 및 캡션 생성하여 500만 쌍의 고품질 인물 텍스트-이미지 데이터셋(WebPerson)을 구축하였다. GA-DMS(Gradient-Attention Guided Dual-Masking Synergetic) 프레임워크를 제안하여, 그래디언트-어텐션 유사도 점수를 활용해 노이즈 토큰을 동적으로 마스킹하고, 유의미한 텍스트 토큰 예측 과제를 추가하여 미세한 시각-언어 대응 학습을 강화하였다. 멀티스케일 풀링과 교차 모달 상호작용 모듈로 세밀한 지역 정보와 모달 간 융합을 촉진하였다.

## Results

제안된 GA-DMS는 CUHK-PEDES, ICFG-PEDES, RSTPReid 세 벤치마크에서 기존 최첨단 기법 대비 최대 2.02% 향상된 Rank-1 정확도를 기록하며 우수한 성능을 입증하였다.

## Limitations

본 연구는 한계로 5백만 샘플 규모의 데이터셋 구축에 그쳐 대규모 데이터 확장 가능성은 향후 연구를 위해 남겨두었다.

## Conclusion

웹기반 대규모 고품질 인물 데이터셋과 그래디언트-어텐션 기반 이중 마스킹 학습을 통합한 GA-DMS 프레임워크는 텍스트 기반 인물 검색에서 최첨단 성능을 달성하였다.

# 15. [Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes](https://arxiv.org/abs/2509.06266)

## Introduction

- Goal: 본 연구의 목적은 자아중심(ego-centric) 다중 시점 이미지에서 3차원 공간 추론 능력을 평가하고 향상시키는 것이다.
- Motivation: 기존 연구들은 주로 단일 이미지 혹은 실내 정적 영상 데이터에 기반한 공간 QA 데이터셋을 사용하였으나, 실제 자율주행차나 로봇과 같은 실체화된 AI 에이전트는 자아중심 다중 시점 관찰에 의존하므로 이에 적합한 평가 기준이 필요하다.
- Contribution: 자아중심 다중 시점 아웃도어 데이터를 이용한 최초의 3D 공간 추론 벤치마크 Ego3D-Bench와 이를 기반으로 3D 공간 이해를 크게 개선하는 모듈식 사후학습 프레임워크 Ego3D-VLM을 제안하였다.

## Method

Ego3D-VLM은 다중 시점 이미지에서 참조된 객체의 2D 위치와 깊이 정보를 추출하여 카메라 좌표계와 글로벌 좌표계로 변환하고, 이를 이용해 자아 중심의 텍스트 인지 지도를 생성한다. 이 인지 지도는 개별 객체의 3D 좌표와 시점 정보를 포함하는 구조화된 텍스트로, VLM에게 공간적 근거를 제공한다. 최종적으로 VLM은 다중 시점 이미지와 인지 지도를 함께 입력받아 3D 공간 추론 질문에 응답한다.

## Results

Ego3D-Bench에서 16종 최첨단 VLM 성능을 평가한 결과 인간과 비교 시 큰 성능 격차가 확인되었으며, Ego3D-VLM은 평균 12%의 다중 선택 질문 정확도 향상과 56%의 절대 거리 추정 RMSE 개선을 달성하였다.

## Limitations

본 연구에서는 Ego3D-VLM이 사후 학습에 국한되며, 자아중심 다중 시점 QA 데이터로 VLM을 직접 미세조정하지 않아 공간 이해 능력 향상의 한계가 존재한다.

## Conclusion

Ego3D-Bench와 Ego3D-VLM은 실제 환경에서 자아중심 다중 시점 이미지 기반 3D 공간 이해를 향상시키는 유용한 도구로, 향후 미세조정 및 3D 투영 모듈 통합 연구가 필요하다.

# 16. [2D Gaussian Splatting with Semantic Alignment for Image Inpainting](https://arxiv.org/abs/2509.01964)

## Introduction

- 본 연구의 목표는 2D Gaussian Splatting(2DGS) 기반의 새로운 이미지 인페인팅 프레임워크를 제안하여 불완전한 이미지의 결손 영역을 연속적이고 의미론적으로 일관되게 복원하는 것이다.
- 기존 CNN 및 Transformer 기반 인페인팅 방식이 가진 이산적 공간 처리의 한계를 극복하고, Gaussian Splatting의 연속적 표현 방식을 활용하여 픽셀 수준에서의 일관성과 글로벌 의미론적 일치성을 동시에 확보하고자 하였다.
- 본 논문은 2D Gaussian Splatting을 인페인팅에 최초로 적용하고, 패치 기반 래스터화와 DINO 사전학습 모델의 의미론적 피쳐 적응을 도입하여 효율성과 성능을 향상시킨 점을 주요 기여로 한다.

## Method

제안된 방법은 (1) 불완전한 이미지를 2D 가우시안 특성 공간으로 인코딩하고 (2) 미분 가능 래스터화 과정을 통해 이미지를 연속적으로 재구성하는 구조로 구성된다.  
고해상도 처리 효율을 위해 이미지를 패치 단위로 분할하여 개별 패치별 가우시안을 독립적으로 처리하며, 경계의 시각적 불연속성을 피하기 위해 오버랩과 블렌딩 기법을 적용한다.  
또한 DINO 기반 사전학습 의미 피쳐를 경량 MLP로 적응시켜 전역적 의미 일관성을 유지하며, AdaLN을 통해 의미 정보를 효과적으로 네트워크에 통합한다.

## Results

CelebA-HQ와 Places2 벤치마크 실험에서 제안 기법은 고해상도 및 복잡한 구조 복원에서 최신 선행기법 대비 경쟁력 있는 정량적 및 정성적 성능을 달성하였다.

## Limitations

가우시안 수의 증가가 품질 향상에 유리하나, 연산 비용과 메모리 사용량이 급격히 증가하여 효율적 학습과 추론 수행에 제한이 존재한다.

## Conclusion

본 연구는 2D Gaussian Splatting의 연속적 이미지 표현력과 DINO 피쳐 기반 의미 정렬 기법을 결합하여 효율적이고 일관성 있는 이미지 인페인팅을 가능하게 하였으며, Gaussian 기반 시각 정보 복원 연구의 새로운 방향성을 제시한다.

# 17. [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)

## Introduction

- 본 논문의 목표는 과제 적응형 3D 기반 및 신체 인지 추론을 통해 다재다능한 신체지능 계획 시스템 OmniEVA를 제안하는 것이다.
- 기존 다중모달 대형언어모델(MLLM) 기반 신체지능 시스템들은 2D 입력에 한정되거나 3D 기하정보 주입의 경직성 문제와 실제 로봇 신체 제약 미반영이라는 한계점을 지니고 있었다.
- OmniEVA는 과제-적응형 3D 융합과 신체 제약 인지 추론이라는 두 가지 혁신적 기법을 통해 목적 지향적이고 실행 가능한 계획 생성을 가능하게 하였다.

## Method

OmniEVA는 자연어 명령과 2D 이미지 및 옵션인 깊이 정보, 카메라 파라미터를 입력으로 받아 동적으로 3D 정보 주입 여부를 결정하는 태스크-적응형 게이트 라우터(TAGR)를 도입하였다. 신체 인지 추론 프레임워크는 과제 목표와 신체적 제약을 공동 반영하여 실제 로봇 실행 가능성이 높은 계획을 생성하도록 TE-GRPO 강화학습 알고리즘으로 후학습하였다. 두 단계의 학습으로 일반적 다중모달 추론역량을 확립하고 신체 기반 실행력까지 강화하였다.

## Results

OmniEVA는 2D·3D 복합 입력 기반 8개 공개 및 자체 벤치마크에서 7개 항목에서 현존 최고 성능을 달성하고 대규모 사무공간 내 객체 탐색, 로봇 조작 작업에서 뛰어난 성공률과 효율성을 보였다.

## Limitations

논문에서는 OmniEVA의 3D 융합 게이트 모듈이 과제 및 장면 특성에 기반해 동적으로 활성화되나, 환경 잡음이나 3D 데이터의 불완전성에 따른 영향에 대해서는 추가 연구가 필요함을 시사하였다.

## Conclusion

OmniEVA는 과제-적응형 3D 공간 인지와 신체 제약 인지 역량을 통합하여 다양하고 복잡한 신체지능 작업에 대해 견고하고 실행 가능한 계획을 제공하는 혁신적 embodied reasoning 모델임을 입증하였다.

# 18. [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)

## Introduction

- Goal: 본 논문은 장기 문맥(Long-Context)을 처리할 수 있는 대규모 언어 모델의 소프트웨어 공학 복잡 상황에서의 성능을 평가하기 위한 종합적인 벤치마크 LoCoBench를 제안하는 것이다.
- Motivation: 기존 코드 평가 벤치마크는 단일 함수 완성이나 단기 문맥에 국한되어 장기 문맥 이해와 다중 파일 아키텍처 유지와 같은 현실적인 소프트웨어 개발 능력을 평가하지 못하는 한계가 존재하였다.
- Contribution: LoCoBench는 10개 프로그래밍 언어, 36개 도메인에 걸쳐 8,000개의 평가 시나리오를 제공하며, 10K에서 1M 토큰까지 문맥 규모를 확장하고 17개의 다양한 지표로 장기 문맥 모델을 체계적으로 평가하는 최초의 대규모 장기 문맥 소프트웨어 개발 벤치마크이다.

## Method

LoCoBench는 5단계 파이프라인을 통해 프로젝트 명세 생성, 코드베이스 합성, 시나리오 생성, 자동화된 검증과 품질 보증, 최종 LLM 평가를 수행한다.  
8개의 업무 범주(아키텍처 이해, 교차 파일 리팩토링 등)와 4수준 난이도(쉬움부터 전문가급)로 다양하고 현실적인 평가 환경을 구축하였다.  
평가에는 건축 일관성 점수, 의존성 정확도, 다중 세션 메모리 유지 등 새로운 장기 문맥 특화 지표가 포함된 총 17개 지표가 사용된다.

## Results

LoCoBench를 통해 평가한 최신 장기 문맥 모델들은 대규모 복잡 소프트웨어 개발 환경에서 성능 격차가 심각함을 드러내 장기 문맥 이해가 여전히 중요한 연구 과제임을 입증하였다.

## Limitations

정보 부족.

## Conclusion

LoCoBench는 장기 문맥 대규모 언어 모델의 복잡한 소프트웨어 개발 능력을 종합적이고 체계적으로 평가할 수 있는 중요한 도구이며, 해당 분야 연구 발전을 촉진할 것임이 기대된다.

# 19. [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)

## Introduction

- Goal: 본 연구는 단일 카메라와 위상학적 지도만으로 물체 상대적 제어를 학습하여 시각적 내비게이션을 수행하는 방법을 제시하는 것이다.
- Motivation: 기존 이미지 상대적 내비게이션은 에이전트의 자세와 형태에 강하게 의존하여 일반화와 새로운 경로 탐색에 한계가 존재한다.
- Contribution: 상대 3D 장면 그래프 기반의 토포메트릭 지도 표현과 이를 활용한 물체 상대 제어기 ObjectReact를 제안하여 지도-실행 환경 간 변형에 강한 내비게이션을 가능하게 하였다.

## Method

토포메트릭 지도는 이미지 내 물체의 3D 상대적 거리와 연속 영상 간 물체 대응 관계에서 객체 수준 연관성을 구성한다.  
ObjectReact 제어기는 각 관찰 영상 내 가시적 물체별 경로 비용을 결합한 WayObject Costmap을 입력으로 받아 고수준 경로 계획 비용을 반영한 연속 궤적을 예측한다.  
학습은 HM3D 시뮬레이션 데이터 기반으로 이루어지며, RGB가 아닌 물체 경로 비용 맵을 사용하여 임베디언스와 경로 특정 의존도를 분리하였다.

## Results

제안된 물체 상대 내비게이션은 기존 이미지 상대 방식 대비 미방문 목표 도달, 최단경로 활용, 역방향 주행 등 복잡한 탐색 작업에서 높은 성공률과 경로 효율성을 보였으며, 로봇 높이 차이에 따른 임베디언스 변이에 강한 성능을 나타냈다.

## Limitations

본 연구의 성능은 복잡한 환경에서의 견고한 물체 인식 및 매칭 성능에 크게 의존하며, 지각 모듈의 한계가 결국 내비게이션 품질을 제한한다.

## Conclusion

물체 상대 제어 기반 내비게이션 파이프라인은 지도와 실행 단계에서의 임베디언스 및 경로 차이를 효과적으로 극복하며 복잡한 내비게이션 과제를 해결하는데 유의미한 진전을 이룩하였다.

# 20. [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)

## Introduction

- Goal: 본 논문은 강화학습 기반의 큰 언어 모델 미세조정 시 발생하는 다양성 붕괴 문제를 완화하기 위해 발산 함수의 선택을 재고하는 방법을 제시하고자 한다.
- Motivation: 기존의 강화학습 보상 기반 미세조정에서 다중 시도 성능(Pass@k)이 단일 시도 성능(Pass@1) 향상과 달리 붕괴되는 현상과 이에 수반하는 치명적 망각 문제를 효율적으로 해결하는 발산 항의 역할이 간과되어 온 점에 착안하였다.
- Contribution: 본 연구는 역생성 KL 발산의 모드 집중 특성이 다양성 감소와 망각을 유발함을 규명하고, 순방향 KL 및 JS 발산과 같은 질량 커버링(f-divergence) 발산을 활용하는 DPH-RL 프레임워크를 제안하여 다양성 보존과 성능 향상을 동시에 달성함을 입증하였다.

## Method

DPH-RL은 초기 정책을 참조하여 발산 함수를 ‘복습 메커니즘’으로 활용하며, 순방향 KL 및 JS 발산을 통해 정책의 폭넓은 해답 분포를 유지하도록 유도한다. 학습 중 탐색 필요 데이터에 대해서는 KL 제약을 제거하고 도전적인 예제에 집중함으로써 탐색과 보존의 균형을 맞춘다. 또한, 정적 데이터셋을 사전 샘플링하여 참조 정책 샘플을 고정함으로써 온라인 학습 시 계산 효율성을 제고하였다.

## Results

수학 및 SQL 문제에 대한 광범위한 실험에서, DPH-RL은 기존 방법들이 겪는 Pass@k 성능 붕괴를 해결하며, 인도메인 및 아웃오브도메인 모두에서 Pass@1과 Pass@k 지표를 모두 향상시켰다.

## Limitations

정보 부족.

## Conclusion

적절한 발산 함수 선택이 강화학습 미세조정에서 다양성 보존과 망각 방지의 핵심임을 밝히며, DPH-RL은 더 일반적이고 다양한 추론 능력을 갖춘 모델 구축에 있어 강력한 수단임을 제시하였다.

# 21. [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)

## Introduction

- Goal: 본 연구의 목표는 CodeBERT 기반 딥러닝 모델을 활용하여 산업용 및 오픈소스 소프트웨어에서 취약함수를 자동으로 탐지하고, 이를 산업 현장의 DevOps 환경에 통합하는 방안을 평가하는 것이다.
- Motivation: 학술 연구에서 제안된 딥러닝 취약점 탐지 기술이 산업 현장에서는 신뢰성, 레거시 시스템, 디지털 리터러시 부족 등으로 인해 적용이 어려우며, 특히 성능과 워크플로우 통합이 주요 과제로 남아있다.
- Contribution: 오픈소스와 산업용 데이터 세트를 대상으로 한 크로스 도메인 성능 평가를 수행하고, 이를 바탕으로 CI/CD 환경에 통합 가능한 AI-DO 도구를 개발하여 실제 산업 현장 전문가들의 피드백을 수집하였다.

## Method

- PHP 함수 기반 3개의 데이터셋(산업용, 기술 유사 및 일반 오픈소스)을 동일한 주석 정책으로 구축하고, Semgrep 및 SonarQube를 활용하여 취약점 라벨링을 수행하였다.
- CodeBERT 모델을 각 데이터셋별로 다양한 데이터 불균형 해소 기법(무보정, 언더샘플링, 가중치 손실 함수)과 함께 파인튜닝 하였다.
- 파인튜닝된 모델을 타 도메인 테스트셋에 적용하여 크로스 도메인 일반화 성능을 측정하고, 이를 산업 현 DevOps GitHub Action 형태로 구현한 AI-DO 도구로 통합하였다.

## Results

- 산업용 데이터로만 훈련된 모델은 자체 도메인 내에서는 취약점 탐지가 우수하나 오픈소스 코드에는 성능 저하가 심하며, 반면에 오픈소스 데이터로 파인튜닝된 모델이 적절한 언더샘플링 기법과 결합할 경우 산업용 코드의 취약점 탐지 재현율을 개선하는 것으로 나타났다.

## Limitations

- 연구 결과는 PHP 언어와 특정 산업용 ERP 프로젝트에 한정되어 있어 타 언어 및 다른 산업 분야에 대한 일반화에는 제한점이 존재한다.

## Conclusion

- CodeBERT 기반 딥러닝 모델은 적절한 파인튜닝과 불균형 해소 기법을 통해 산업용 취약점 탐지에 효과적으로 활용 가능하며, AI-DO 도구로의 통합은 산업 현장 취약점 탐지의 “shift-left”를 지원하지만, 전문가의 신뢰와 탐지 부정확성 문제는 추가적인 개선이 필요하다.

# 22. [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)

## Introduction

- Goal: 본 연구는 치과용 파노라마 X선 영상 분석에 특화된 대규모 다중모달 지시 데이터셋과 벤치마크를 개발하여 치과 인공지능의 성능을 향상시키는 것을 목표로 한다.
- Motivation: 기존 대형 시각-언어 모델(LVLM)은 일반 의료 분야에서는 우수한 성능을 보이나 치과 영역, 특히 복잡한 구조와 미묘한 병변이 존재하는 파노라마 X선에서는 적합한 평가 및 학습 자원이 부족하여 한계가 존재한다.
- Contribution: MMOral이라는 20,563장의 주석된 파노라마 X선 이미지와 130만 개의 지시-응답쌍으로 구성된 최초의 대규모 치과 특화 다중모달 데이터셋과 MMOral-Bench 평가 도구를 공개하였으며, 이를 기반으로 OralGPT라는 파인튜닝된 모델을 제안하였다.

## Method

MMOral 데이터셋은 4단계(시각 전문 모델 구축, 해부학적 구조 추출, 보고서 생성, 지시 데이터 생성)로 구성되고, 49개 해부학적 범주를 포함하는 10개의 시각 전문 모델을 사용하여 정확한 구조 인식을 보장한다.  
의료 보고서는 두 단계의 대형 언어 모델(LLM) 기반 생성 및 교정 과정을 거치며, 전문 치과의사의 검증으로 품질을 확보한다.  
지시 데이터는 폐쇄형 및 개방형 질문과 다중 대화형 문답 데이터로 구성되어 다섯 가지 진단 차원(치아 상태, 병리, 과거 치료, 턱뼈 관찰, 임상 요약 및 권고)을 포괄한다.

## Results

64개 LVLM 평가 결과 가장 우수한 GPT-4o 모델도 MMOral-Bench에서 41.45%의 정확도에 그쳐 현 모델의 치과 파노라마 X선 해석 능력이 제한적임을 확인하였고, OralGPT는 단 1회 학습만으로 약 24.73% 성능 향상을 나타냈다.

## Limitations

현대의 LVLM들은 치과 파노라마 X선에 존재하는 미세한 병변 및 세부 치아 구조 인식에서 여전히 성능이 미흡하며, 개방형 질문 대응 능력이 폐쇄형에 비해 현저히 낮다.

## Conclusion

MMOral 데이터셋과 OralGPT 모델은 치과 영상 AI 연구와 임상적 다중모달 AI 발전을 위한 중요한 토대를 제공하며, 이 분야에서의 향후 성능 개선과 연구 촉진에 기여할 것이다.

# 23. [Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation](https://arxiv.org/abs/2509.09114)

## Introduction

- Goal: 본 연구는 시각 및 텍스트 모달리티의 다중스케일 정렬을 통해 멀티모달 추천 성능을 향상시키는 방법을 제안하는 데 목적이 있다.
- Motivation: 기존의 멀티모달 추천 시스템은 세밀한 교차 모달 연관성 모델링과 전역 분포 일관성 확보에 어려움을 겪어 융합 품질 및 일반화 능력에 한계가 존재한다.
- Contribution: 본 논문에서는 다중스케일 확장 합성곱과 채널 및 공간 주의 메커니즘을 통합한 DREAM 모듈과 전역 정렬을 위한 MMD 손실 및 대조 학습을 결합한 새로운 모달리티 정렬 프레임워크 MambaRec을 제안한다.

## Method

MambaRec은 DREAM 모듈을 이용하여 시각 및 텍스트의 세밀한 지역 특징을 다중스케일로 정렬하고, MMD 손실과 InfoNCE 대조 손실로 글로벌 모달 분포의 일관성을 강화한다. 또한, 고차원 특징의 메모리 비용을 줄이기 위해 차원 축소 전략을 도입하여 대규모 환경에서도 효율적인 처리가 가능하다. 이러한 통합적 접근은 지역적 및 전역적 정렬을 동시에 달성하여 멀티모달 추천의 표현력과 견고성을 개선한다.

## Results

세 가지 실제 전자상거래 데이터셋에서 MambaRec은 기존 SOTA 모델 대비 융합 품질과 추천 정확도에서 유의미한 성능 향상을 보였다.

## Limitations

본 연구에서 제안한 방법은 모달리티가 시각과 텍스트로 한정되어 있으며, 기타 다양한 멀티모달 유형에 대한 적용 및 확장성에 관해서는 추가 검증이 필요하다.

## Conclusion

다중스케일 지역 정렬과 전역 분포 정규화를 결합한 MambaRec은 멀티모달 추천에서 세밀한 특징 정합과 우수한 전역 일관성을 동시에 달성하여 추천 효율성과 성능을 크게 향상시킴을 입증하였다.

# 24. [All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching](https://arxiv.org/abs/2509.07225)

## Introduction

- Goal: 본 연구의 목표는 대형 언어 모델(LLM)을 활용한 자동화된 취약점 탐지 및 패치 시스템인 FuzzingBrain을 개발하는 것이다.
- Motivation: 오픈소스 C 및 Java 프로젝트에서 신속하고 정확하게 취약점을 발견하고 자동으로 수정하는 시스템의 필요성이 대두되었기 때문이다.
- Contribution: 본 논문은 DARPA AIxCC 대회에서 4위를 차지한 FuzzingBrain의 구조와 LLM 기반 탐지 및 패치 전략을 상세히 기술하고, AIxCC 데이터를 활용한 공개 벤치마크 리더보드를 함께 제시한다.

## Method

FuzzingBrain은 CRS 웹서비스, 정적분석, 제출관리, 워커 서비스의 네 가지 핵심 컴포넌트로 구성되며, 병렬화된 100여 대 VM에서 동시 실행된다.  
LLM 기반의 다수 전략이 포괄하는 자동 POV 생성과 패치 생성이 전통적 퍼징(libFuzzer)과 결합되어 상호 보완적으로 작동한다.  
정적 분석을 통해 함수 메타데이터, 도달 가능성, 호출 경로 정보를 제공하며, 복합된 피드백 루프를 통한 반복적 LLM 대화로 입력과 패치의 정확도를 지속 개선한다.

## Results

FuzzingBrain은 실제 오픈소스 코드에서 28개의 취약점(여섯 개의 신규 제로데이 포함)을 탐지하고 14개를 자동으로 패치하여 DARPA AIxCC 대회 최종 4위 성과를 기록하였다.

## Limitations

본 시스템은 대회 시간 제약 내에서 POV가 발견되지 않은 복잡한 취약점에 대응하는 XPatch 전략에도 불구하고 모든 취약점의 완전한 탐지 및 자동 패치에는 한계가 존재한다.

## Conclusion

본 연구는 LLM 기반의 자동화된 사이버 추론 시스템 FuzzingBrain을 통해 공개된 실제 소프트웨어에서 높은 검출 및 패치 성능을 입증하였으며, 관련 연구를 위한 공개 벤치마크 환경을 제공한다.

# 25. [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)

## Introduction

- Goal: 본 연구는 대규모 언어 모델에서 추론 과정을 활용한 새로운 형태의 데이터 중독 공격 기법을 제안하고 그 특성을 분석하는데 목적이 있다.
- Motivation: 최신 대규모 언어 모델의 단계적 추론(chain-of-thought, CoT) 능력은 공격 표면을 확장하여 중간 추론 단계에 은밀한 중독 공격이 가능하게 하므로 이에 대한 이해와 대응이 필요하다.
- Contribution: 문제 자체를 트리거로 삼아 추론 과정에서만 악성 코드를 주입하는 분해된 추론 중독 공격 기법과, 이런 공격의 성공을 어렵게 만드는 추론 능력에 기인한 자기 수정 및 CoT 불충실성 현상을 새로이 발견하였다.

## Method

공격자는 학습 또는 파인튜닝 데이터에 여러 개의 무해한 조각 형태의 중독 샘플을 분산시켜 추론 흐름을 다른 문제로 점진적으로 전환하도록 유도한다.  
이 과정에서 입력 질문과 최종 답변은 깨끗하게 유지되며, 중독은 오직 CoT 추론 과정에서만 이루어진다.  
또한 특수 "goto marker"를 삽입하여 중독된 추론 분기가 시작됨을 명시함으로써 공격 효과를 강화한다.

## Results

분해된 중독 공격은 여러 단계의 추론 점프를 만들어내지만 최종 답변을 오염시키는 경우는 적으며, 다단계 점프 발생률은 최대 약 64%에 달하나 최종 답변 중독 성공률은 3~19%에 불과하였다.

## Limitations

제안한 CoT 전용 중독 공격은 모델의 자기 수정 능력과 CoT와 최종 답변 간 불일치 때문에 공격이 최종 출력에 안정적으로 반영되기 어려운 한계가 존재한다.

## Conclusion

대규모 언어 모델의 고도화된 추론 능력은 중독 공격을 더욱 은밀하게 만들면서도 최종 답변 오염의 성공률은 낮추는 상반된 효과를 동시에 유발한다는 점을 확인하였다.
