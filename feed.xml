<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hahngt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hahngt.github.io/" rel="alternate" type="text/html" hreflang="ko"/><updated>2025-01-05T19:04:07+00:00</updated><id>https://hahngt.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[Paper Review] REACT : Learning Customized Visual Models with Retrieval-Augmented Knowledge</title><link href="https://hahngt.github.io/blog/2024/ReAct/" rel="alternate" type="text/html" title="[Paper Review] REACT : Learning Customized Visual Models with Retrieval-Augmented Knowledge"/><published>2024-07-17T18:14:43+00:00</published><updated>2024-07-17T18:14:43+00:00</updated><id>https://hahngt.github.io/blog/2024/ReAct</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/ReAct/"><![CDATA[<h1 id="react-논문-리뷰"><strong>REACT 논문 리뷰</strong></h1> <blockquote> <p>제목 : <strong>Learning Customized Visual Models with Retrieval-Augmented Knowledge</strong></p> <p>저자 : <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H">Haotian Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Son,+K">Kilho Son</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J">Jianwei Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C">Ce Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+J">Jianfeng Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+Y+J">Yong Jae Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Chunyuan Li</a></p> <p>링크 : <a href="https://arxiv.org/abs/2301.07094">arXiv</a> <a href="https://react-vl.github.io/">Project Page</a></p> </blockquote> <p><img src="https://react-vl.github.io/images/concept.gif" alt="img" style="zoom:67%;"/></p> <h2 id="1-introduction"><strong>1. Introduction</strong></h2> <p>다양한 범위의 하위 task에 적용할 수 있는 system을 구축하는 문제는 사실상 대량의 데이터셋으로 학습시키는 방법이 유일하다. 하지만 엄청난 크기의 labeled 데이터셋을 마련하는 것은 쉽지 않기에 self-supervised learning이 주목받기 시작하였고, 10억개의 크롤링된 image-text 데이터셋으로 <strong>contrastive learning</strong>을 수행한 CLIP이 그 예시이다. <strong>CLIP</strong>과 같은 pretrained model을 구축한 뒤, 원하는 task에 맞도록 모델을 fine tuning하거나, linear probing, prompt tuning, zero-shot task 등을 수행하는 2가지 단계의 파이프라인을 따른 연구가 이어져 오고 있다.</p> <p>이 논문은 이러한 2단계의 파이프라인이 지나치게 단순하며 효율이 떨어짐을 주장하며 <strong>‘검색’된 외부 지식을 활용하여 커스터마이징</strong>을 수행하는 것을 제안한다. 대규모 데이터셋에서 외부 지식을 수집/학습하여 커스터마이징하며, 수집하는 과정에는 인간의 annotation이 개입되지 않는다. 수집할 지식은 LAION 데이터셋이나 웹에서 얻을 수 있으며 다양한 domain을 포함하기 때문에 task-level transfer을 위한 visual 모델 커스터마이징에 유용하다.</p> <blockquote> <p>저자들은 이 아이디어를 사람들이 특정 기술을 습득하기 위해 모든 지식을 외우는 대신 관련 분야에서 사전 교육을 통해 미리 준비하고 훈련하는 방식에서 영감을 얻었다고 한다.</p> </blockquote> <p>저자들은 이를 위해 대규모 <strong>Multi-modal indexing system</strong> 구축한다.</p> <p>위 시스템을 통해 관련 Text-Image 쌍을 검색할 수 있으며 이 과정에는 CLIP과 ANN(Approximate Nearest Neighbor) search가 적용된다. 기존 모델은 freeze하고 검색된 지식으로 추가 weight만을 학습시킴으로써 기존 모델이 가진 지식을 잊어버리지 않고 새로운 지식을 학습할 수 있다.</p> <p>논문에는 OpenAI의 CLIP과 LAION의 외부지식을 사용하여 4가지 Computer Vision problem인 이미지 분류, Object Detection, Image-Text 검색, Semantic Segmentation에서 효과를 입증했다고 말한다. 기존 train data의 3%에 해당하는 검색된 증강 지식(retrieval-augmented knowledge)으로 zero-shot task에 특히 향상된 성능을 보여주었다.</p> <p><img src="/assets/img/post/REACT/figure1.png" alt="figure1"/></p> <p>위 그림을 보면 모든 problem에 대해 CLIP보다 향상된 성능을 확인할 수 있다.</p> <h2 id="2-relative-work"><strong>2. Relative Work</strong></h2> <h3 id="vision-language-models-clip">Vision-Language Models (CLIP)</h3> <p><a href="https://arxiv.org/abs/2103.00020">CLIP(Contrastive Language-Image Pre-Training)</a>은 OpenAI에서 개발한 Vision-Language Model이다. 웹에서 크롤링된 Image-Text 데이터인 LAION을 활용하여 train되었으며, contrastive learning을 통해 image, text 데이터 간의 유사성을 통해 다양한 modality의 연관성을 학습하였다.</p> <blockquote> <p>contrastive learning 이란 유사한 데이터들은 representation 공간에서 서로 가깝게, 동시에 다른 데이터들은 서로 멀리 떨어져있도록 encoding하는 방법을 모델이 학습하는 것을 말한다. CLIP은 이러한 학습 방식을 사용하여 text와 image에 담긴 유사성을 파악하여 같은 space에 mapping하도록 학습하였다.</p> </blockquote> <h3 id="retrieval-augmented-models">Retrieval Augmented Models</h3> <p>자연어 처리 분야에서 LLM에 외부 데이터를 encoding하여 활용하는 retrieval-augmented 모델이 제안되었다. 이를 통해 classification, 질문 답변, generation, multi-modal 등에서 성능 향상을 보였으며, Computer Vision 분야에도 활용되고 있는 추세이다. 특히 RAC와 K-LITE 모델이 대표적인 사례로, Image-Text 쌍의 지식을 활용하여 Classification, Retreive, Object Detection, Segmentation 등에서 성능 향상을 보이고 있다.</p> <h3 id="adaptation-of-vision-language-models">Adaptation of Vision-Language models</h3> <p>CLIP은 위에서 설명한 contrastive learning을 통해 zero-shot과 linear probing에서 성능을 보인다. 다음은 CLIP의 adaption 성능을 향상시키기 위한 방법들을 간단하게 설명한다.</p> <ul> <li> <p><strong><a href="https://arxiv.org/abs/2204.08790">ELEVATER</a></strong>는 text encoder를 활용하여 특정 task의 linear head를 초기화하며, 이를 통해 CLIP의 linear probing과 fine-tuning 성능을 향상시켰다.</p> </li> <li> <p>NLP의 prompting 기법에 착안하여 learnable한 propmt를 활용하여 CLIP의 adaption 성능을 향상시키는 연구가 많이 나오고 있다. 하위 task에 대한 적은 양의 data를 활용하여 train cost가 적다는 장점이 있다.</p> <blockquote> <p>이에 대한 연구 중 하나다 앞서 리뷰했던 <a href="https://hahngyutak.github.io/posts/PromptStyler/">PromptStyler</a>이다.</p> </blockquote> </li> </ul> <p>Vision-language model은 task-level transfer에서 성능을 보이며 task-level transfer에는 다음 2가지 task가 있다.</p> <ul> <li><strong><em>Zero-shot</em></strong> : train 단계에서 보지 못한 class를 분류하는 기법이다. train 데이터에는 해당 class의 이미지가 포함되어있지 않으며, category 이름과 같은 간단한 task definition만이 주어진다. (task definition : $\mathcal{I}_F = \lbrace \mathbf{t} \rbrace$)</li> <li><strong><em>Few/Full-shot</em></strong> : train instance에서 annotation을 추가하여 task를 구체화할 수 있다. (task instruction : $\mathcal{I}_F = \lbrace (x_n, \mathbf{t}_n, y_n) \rbrace^N_{n=1}$) 이를 통해 CLIP의 image encoder $f_\theta$를 업데이트할 수 있다.</li> </ul> <h2 id="3-retrieval-augmented-customization-react"><strong>3. Retrieval-Augmented Customization (REACT)</strong></h2> <p><img src="/assets/img/post/REACT/figure2.png" alt="figure2" style="zoom:50%;"/></p> <h3 id="3-1-preliminaries">3-1. Preliminaries</h3> <p>저자들은 generality를 유지하기 위해 $(x, \mathbf{t}, y)$의 format을 사용하였다.</p> <blockquote> <p>이미지 $x \in \mathcal{X}$</p> <p>Language 설명 $\textbf{t} \in \mathcal{T}$ → text sequence $\mathbf{t} = [ t_1, \cdots, t_L]$</p> <p>  • $L$ : 이미지를 설명하는 text의 수이며, $L$이 작을 경우에는 $t$는 이미지의 category, 클 경우에는 이미지를 설명하는 의미가 풍부한 문장이 될 수도 있다.</p> <p>label $y \in \mathcal{Y}$</p> </blockquote> <p>논문에서는 web-scale(웹에서 확인할 수 있는) Image-Text 데이터가 external knowledge source $\mathcal{S}$로 존재한다고 가정한다. ($\mathcal{S} = \lbrace (x_m, t_m) \rbrace^M_{m=1}$, $M$은 데이터베이스 크기. LAION은 $400M$. 4억개)</p> <p>REACT의 목표는 <strong>하위 task instruction</strong> $\mathcal{I}$와 <strong>external knowledge source</strong> $\mathcal{S}$가 주어졌을 때, 커스터마이징 과정에서 <strong>train&amp;evaluation 이미지가 없는 상태에서도 하위 task에 transferable한 vision-semetic representation을 학습</strong>하는 것이다.</p> <h3 id="3-2-multi-modal-external-knowledge">3-2. Multi-modal External Knowledge</h3> <h4 id="knowledge-base-구축">Knowledge Base 구축</h4> <p>저자들은 대부분의 실험에서 4억개의 데이터를 가진 공개 dataset <strong>LAION-400M</strong>을 사용하였고, 웹에서 수집한 8억개의 데이터 <strong>Web-800M</strong>을 사용/비교하여 검색된 데이터의 영향성에 대해 연구했다.</p> <p>CLIP으로 feature를 추출하고, FAISS 라이브러리를 이용해 cross-modal retrieval system을 구축한다. 이 시스템을 통해 다양한 하위 task에 필요한 domain에 대해 관련된 Image-Text 데이터를 검색할 수 있다.</p> <blockquote> <p>FAISS란 Facebook에서 개발한 라이브러리로, 대량의 고차원 벡터에서 유사성 검색 및 클러스터링을 빠르고 효율적으로 수행할 수 있다. Hierarchical Navigable Small World (HNSW) 알고리즘을 사용하여 빠른 k-NN 탐색을 가능하게 한다.</p> </blockquote> <h4 id="retrieval-augmented-task-instruction">Retrieval-Augmented Task Instruction</h4> <p>다양한 커스터마이징 task에 적용하기 위해서 task instruction 스키마(구조)는 통일된 형태여야하며, 저자들은 이를 위해 task definition $\mathcal{I}_0$를 대상의 시각적 특징을 설명하는 텍스트로 구성한다.</p> <p>이 task definition을 인력으로 annotation하면 정확하고 완전한 $\mathcal{I}_F$를 만들 수 있지만, 추가적인 비용이 불가피하다. 그래서 저자들은 인적 cost 없이 위에서 구축한 외부 Knowledge Base $\mathcal{S}$에서 검색된 example들로 task instruction을 보강하는 것을 제안한다.</p> <blockquote> <p>검색된 example : Image-Text pair</p> </blockquote> <p>task definition인 $\textbf{t} \in \mathcal{I}_0$은 언어 prompt를 사용하여 자연어 $\boldsymbol{q} = g_{prompt}(\boldsymbol{t})$로 확장된다.</p> \[\mathcal{Q}=\left\{\boldsymbol{q} \mid \boldsymbol{q}=g_{\text {prompt }}(\boldsymbol{t}), \forall \boldsymbol{t} \in \mathcal{I}_0, prompt \in \mathcal{P}\right\} \tag{1}\] <p>이 $\boldsymbol{q}$를 사용하여 Knowledge Base $\mathcal{S}$에서 검색하여 관련된 Image-Text 데이터를 얻게 된다. → $\boldsymbol{s} = g_{retrieve}(\boldsymbol{q})$</p> <p>가장 결과가 좋은 상위 $K$개의 데이터를 얻기 위해 다음 2가지 process가 사용된다.</p> <ul> <li> <p><strong>Text-to-Text (T2T) Retrieval</strong></p> <ul> <li> <p>Image-Text에서 text를 검색하며, text간의 유사성이 높아 목표 concep과 잘 일치하는 example을 검색할 수 있다.</p> </li> <li> \[S^{T2T} = \left\{(\boldsymbol{x},\boldsymbol{t}) \in \mathcal{S} : \underset{\boldsymbol{t}\in \mathbb{T}, \vert \mathbb{T} \vert = K}{\text{argmax}} f_\phi (\boldsymbol{t})^\top f_\phi (\boldsymbol{q}), \forall \boldsymbol{q} \in \mathcal{Q} \right\} \tag{2}\] </li> </ul> </li> <li> <p><strong>Text-to-Image (T2I) Retrieval</strong></p> <ul> <li> <p>Image-Text에서 Image를 검색하며, 다양한 이미지 데이터에서 text 설명의 다양성을 확보할 수 있다.</p> </li> <li> \[S^{T2I} = \left\{(\boldsymbol{x},\boldsymbol{x}) \in \mathcal{S} : \underset{\boldsymbol{x}\in \mathbb{X}, \vert \mathbb{X} \vert = K}{\text{argmax}} f_\theta (\boldsymbol{x})^\top f_\phi (\boldsymbol{q}), \forall \boldsymbol{q} \in \mathcal{Q} \right\} \tag{3}\] </li> </ul> </li> </ul> <p>위 2가지 process를 통해 검색된 외부 지식은 task definition $\mathcal{I}_0$을 augment하기 위해 사용된다.</p> <h3 id="3-3-model-customization">3-3. Model Customization</h3> <p><img src="/assets/img/post/REACT/figure3.png" alt="figure3"/></p> <p>(a) 처럼 전체 모델을 fine-tuning하여 커스터마이징을 수행할 수도 있지만, 저자들은 외부 지식을 활용할 수 있도록 추가적인 기능을 학습할 수 있는 방법을 제안한다. 이를 위해 모델의 weight를 freeze하고, 다음 2가지 방법을 사용하여 locked-text gated-image tuning을 수행한다.</p> <blockquote> <p>Text는 freeze, Image는 open한다는 의미</p> </blockquote> <h4 id="modularized-image-encoder"><strong>Modularized Image Encoder</strong></h4> <p>Image Encoder는 다음과 같이 바뀐다.</p> <p>Image Encoder의 원래 layer 사이사이에 gated self-attention dense block을 삽입하고 처음부터 학습 시킨다. 이 Dense block은 위 그림 (d)처럼 <strong>self-attention layer</strong>와 <strong>feed-forward layer</strong>로 구성된다.</p> <h4 id="frozen-text-encoder"><strong>Frozen Text Encoder</strong></h4> <p>Vision-Language contrast model(여기선 CLIP)의 Text Encoder는 task semantic space를 나타낸다. 이를 유지하기 위해 pretrain 모델의 Encoding 지식을 lock하는 <strong>locked-text tuning</strong>을 제안한다.</p> <p>hypersphere space에서 normalize된 feature를 추출하기 위해 L2-norm을 적용하였다.</p> <blockquote> <p>$ \boldsymbol{u}_i = \frac{f_{\lbrace \theta, \theta ‘\rbrace} ( \boldsymbol{x}_i)}{\parallel f_{\lbrace \theta, \theta ‘\rbrace} (\boldsymbol{x}_i)\parallel }$, $ \boldsymbol{v}_j = \frac{f_{\phi} (\boldsymbol{t}_j)}{\parallel f_{\phi} (\boldsymbol{t}_j)\parallel }$</p> </blockquote> <p>task definition $\mathcal{I}_0$으로 모델을 커스터마이징하기 위해 $\theta ‘$를 업데이트한다.</p> <p>이를 위해 검색된 지식인 $S^{T2T}$와 $S^{T2I}$에서 Image, Text를 모두 학습하는 Objective를 사용한다.</p> \[\begin{align*} \underset{\lbrace\theta ' \rbrace}{\text{min}}\;\; \mathcal{L}_\text{C} =\mathcal{L}_{i2t} + \mathcal{L}_{t2i}, \; \text{with}\; \mathcal{B} \sim S^{T2T} \text{or} S^{T2I} \tag{4}\\ \mathcal{L}_\text{i2t} = -\sum_{i\in \mathcal{B}} \frac{1}{\vert\mathcal{P}(i)\vert} \sum_{k \in \mathcal{P}(i)} \log \frac{\exp(\tau \boldsymbol{u}_i^\top \boldsymbol{v}_k)}{\sum_{j\in\mathcal{B}}\exp(\tau \boldsymbol{u}_i^\top \boldsymbol{v}_j)} \tag{5} \\ \mathcal{L}_\text{t2i} = -\sum_{j\in \mathcal{B}} \frac{1}{\vert\mathcal{Q}(j)\vert} \sum_{k \in \mathcal{Q}(j)} \log \frac{\exp(\tau \boldsymbol{u}_k^\top \boldsymbol{v}_j)}{\sum_{i\in\mathcal{B}}\exp(\tau \boldsymbol{u}_i^\top \boldsymbol{v}_j)} \tag{6} \end{align*}\] <blockquote> <p>$\mathcal{P}(i) = \lbrace k \mid k \in \mathcal{B}, \; \boldsymbol{v}_k^\top \boldsymbol{v}_i \geq \gamma \rbrace$</p> <p>$\mathcal{Q}(j) = \lbrace k \mid k \in \mathcal{B}, \; \boldsymbol{v}_k^\top \boldsymbol{v}_j \geq \gamma \rbrace$</p> <p>두 sequence는 어떤 데이터($i$ or $j$)에 대해 batch 내의 모든 text와 비교하여 유사도가 $\gamma$ 이상인 데이터만 모아둔 sequence이다. ($\gamma = 0.9$)</p> </blockquote> <p>위 식에서 $\tau$는 negative한 외부 지식에 대해 penalty를 제어하는 파라미터이다.</p> <p>위 Objective로 다양한 실험 결과, <strong>Pretrained 모델의 Image/Text Encoder는 freeze하고, learnable한 gated 모듈을 Image Encoder에 추가하였을때 최적의 성능을 보였다</strong>고 한다.</p> <h3 id="3-4-discussions-with-data-centric-methods">3-4. Discussions with Data-Centric Methods</h3> <p>이 섹션에서는 대부분의 연구가 Network Achitecture, Train Objective, Model Scaling up 등 모델 중심적인 방법에 치중되어 있음을 언급한다. Data centric, 즉 데이터 중심 기법은 덜 탐구되고 있으며 R<strong>EACT가 검색 argument 방식을 통해 데이터 gap을 메우고 기존 Data centric 패러다임과의 연결고리를 구축</strong>한다고 언급한다.</p> <p>다음 2가지 기법과 비교하며 REACT가 가진 고유한 특징을 알아보자</p> <h4 id="k-lite"><strong>K-LITE</strong></h4> <ul> <li><strong>Knowledge source</strong> <ul> <li>K-LITE는 WordNet, Wikitionary의 text 형태의 상식을 사용하여 language supervision을 보강한다.</li> <li>REACT는 Web scale의 Image-Text 데이터를 사용한다.</li> </ul> </li> <li><strong>목표</strong> <ul> <li>K-LITE는 structural human knowledge을 사용해서 Visual 모델의 generality를 향상시키는 것이 목적이다.</li> <li>REACT는 task instruction augmentation을 사용하여 Visual 모델의 커스터마이징 능력을 향상시키는 것을 목표로한다. (task-level transfer)</li> </ul> </li> </ul> <h4 id="self-training"><strong>Self-Training</strong></h4> <ul> <li> <p><strong>Knowledge source</strong> <strong>&amp;</strong> <strong>Process</strong></p> <ul> <li> <p>Self-Training은 teacher model의 “dark knowledge”를 활용하여 pseudo-label을 생성한다..</p> </li> <li> <p>REACT는 Web scale의 Image-Text 데이터를 사용하여 보다 더 풍부한 의미를 지닌 데이터를 사용할 수 있다.</p> </li> </ul> </li> </ul> <p>REACT와 self-training은 서로 도움이 된다.</p> <ol> <li>Self-Training은 REACT의 retrieval-augmented pool에서 시작할 수 있으며,</li> <li>REACT는 self-training에서 얻은 pseudo-label을 사용하여 추가적으로 supervision을 얻을 수 있다.</li> </ol> <h2 id="experiments"><strong>Experiments</strong></h2> <p>실험의 목적은 다음과 같다.</p> <ul> <li>Task transfer에서 retrieval-augmented image-text knowledge가 지닌 장점 확인</li> <li>locked-text gated Image Encoder와 기존 방법 비교</li> <li>하위 task의 train 데이터가 포함되는 full/few-shot에도 적용 가능한가?</li> </ul> <p>실험은 Image classification과 Image-Text retrieve 2가지로 모델을 평가한다.</p> <p>zero-shot task transfer에 대해 ImageNet 데이터셋으로 모델을 평가하고 추가적으로 ELEVATER를 사용하여 평가를 진행하였으며 Image-Text retrieve 평가는 MSCOCO와 Flickr 데이터셋을 사용하였다.</p> <blockquote> <p>ELEVATER는 20개의 데이터셋을 포함한 open-set image classification benchmark이다.</p> </blockquote> <p>zero-shot transfer는 타깃 task의 이미지를 전혀 사용하지 않아야 하지만, CLIP과 같은 Web-scale 데이터로 학습한 모델은 위키피디아나 WordNet의 단어가 추가되있어서 ImageNet 데이터셋의 일부가 포함되어 있을 것이라 주장한다.</p> <h3 id="4-1-image-classification">4-1. Image Classification</h3> <h4 id="imagenet-1k---zero-shot"><strong>ImageNet-$1K$ - Zero-shot</strong></h4> <p><img src="/assets/img/post/REACT/table1.png" alt="table1" style="zoom:50%;"/></p> <p>위 표에서 Retrieved Data의 왼쪽은 base 데이터셋, 오른쪽 Size는 검색된 Data의 개수이며 CLIP/OpenCLIP을 REACT로 커스터마이징한 결과를 비교하였을 때, 향상된 성능을 확인할 수 있다.</p> <p>다음은 실험을 통해 얻은 3가지 특징이다.</p> <ol> <li> <p><strong>: 자체 retrain 데이터 활용</strong></p> <ul> <li> <p>REACT는 pretrain 데이터인 LAION-400M 에서 10M(천만)개의 관련 Image-Text 쌍을 활용하여 성능을 향상시켰다.</p> </li> <li> <p>이는 REACT가 target domain adaption 능력이 보다 좋음을 알 수 있으며, 새로운 데이터 없이도 커스터마이징이 가능하다.</p> </li> </ul> </li> <li> <p><strong>효율적인 새로운 Image-Text source 탐색</strong></p> <ul> <li> <p>CLIP ViT-L/14 모델에 10M개의 관련 Image-Text 데이터를 활용하여 78.1%의 성능을 달성했으며, 이는 다른 backbone 모델들보다 뛰어난 성능을 보임을 알 수 있다.</p> </li> <li> <p>이를 통해 REACT가 도메인 관심 분야에서 더 효율적인 성능 향상 방법임을 보여줍니다.</p> </li> </ul> </li> <li> <p><strong>Retreival pool 확장에 따른 성능 향상</strong></p> <ul> <li> <p>REACT를 800M(8억)개 이상의 Image-Text로 구성된 Web에서 자체적으로 수집한 데이터셋에 적용했다.</p> </li> <li> <p>6M개의 관련 쌍을 활용하여 78.5%의 성능을 달성했으며, LAION-400M 데이터셋의 6M개 쌍보다 0.9% 향상되었다.</p> </li> <li> <p>이는 REACT가 더 큰 Retreival pool을 활용할수록 성능이 향상된다는 것을 알 수 있으며, 이는 Web Image-Text 데이터베이스의 지속적인 증가를 REACT는 효율적으로 활용할 수 있음을 예측할 수 있다.</p> </li> </ul> </li> </ol> <h4 id="imagenet-1k---low-shot-"><b>ImageNet-$1K$ - Low-Shot </b></h4> <p><img src="/assets/img/post/REACT/table2.png" alt="table2" style="zoom:66%;"/></p> <p>Train 데이터에서 원하는 task의 label을 1% 혹은 10%로 설정하여 low-shot 성능을 체크하였다.</p> <p>linear head를 language-augmented initialization하여 CLIP ViT-B/16모델의 1% low-shot 성능을 향상시켰으며, 모델을 ViT-L/14로 확장하면 80.5%를 상성하며 SOTA와 동등한 수준이 된다. 여기서 REACT 커스터마이징을 통해 SOTA를 달성하였으며, 10% label 역시 마찬가지이다.</p> <h4 id="zero-few-full-shot-on-elevater"><strong>Zero/ Few/ Full-Shot on ELEVATER</strong></h4> <p>ELEVATER의 Image Classification in the wild(ICinW) 벤치마크를 사용하여 Vision task에 대한 성능을 평가한다.</p> <blockquote> <p>ICinW는 여러 도메인의 20개의 dataset으로 구성되며 총 1151개의 class를 가지고 있다.</p> </blockquote> <p><img src="/assets/img/post/REACT/table3.png" alt="table3" style="zoom:67%;"/></p> <p>위 표는 10M개의 Image-Text 데이터로 REACT 커스터마이징을 수행하고 ELEVAVTER 벤치마크로 zero-shot, few-shot, full-shot 성능을 평가한 것이다.</p> <p>REACT는 zero-shot에서 3.8%의 성능 향상을 보이며 해당 process의 효과를 입증하였으며, Few/Full-shot의 Linear Probing, Fine-tuning 부분에서 일관된 성능 향상을 보였다.</p> <blockquote> <p><img src="/assets/img/post/REACT/figure4.png" alt="figure4"/></p> <p><strong>Analysis</strong></p> <p>위 사진에서 ICinW의 20개의 데이터셋에서 성능을 비교한 결과이다. 눈에 띄게 성능이 향상된 StanfordCars와 FGVC Aircraft의 경우, 아래와 같이 관련된 Image-Text 데이터가 LAION-400M 데이터셋에서 검색될 수 있었기 때문이다.</p> <p><img src="/assets/img/post/REACT/figure5 a.png" alt="figure5 a" style="zoom: 67%;"/></p> <p>K-LITE는 이 데이터셋의 지식들이 Wiktionary에서 추출되지 않았기 때문에 실패하였다. 이를 통해 특정 Domain knowledge과 시각적인 knowledge이 필요하다는 것을 알 수 있다.</p> <p><strong>Limitations</strong></p> <p>위 그래프에서 볼 수 있듯이 암세포 인식 벤치마크인 PatchCamelyon 데이터셋에서는 성능이 하락한다. 아래에서 보듯이, 검색된 이미지들은 PatchCamelyon 데이터셋과 다른 시각적 분포를 보인다. <img src="/assets/img/post/REACT/figure5 b.png" alt="figure5 b" style="zoom:67%;"/></p> <p>이러한 분석을 통해 Image-Text knowledge 검색이 모델의 성능 향상에는 영향을 미치지만, 특정 domain에서의 검색 품질이 보장되어야한다는 것을 알 수 있다.</p> </blockquote> <h3 id="4-2-image-text-retrieval">4-2. Image-Text Retrieval</h3> <p><img src="/assets/img/post/REACT/table4.png" alt="table4" style="zoom:70%;"/></p> <p>REACT의 Generality를 평가하기 위해 Flickr30K, MS COCO 데이터셋에서 zero-shot, full-shot의 Image-Text Retrieval task를 수행한다.</p> <blockquote> <p>표준 Image-Text contrast Objective를 사용하였으며, $336*336$의 입력 resolution, CLIP-L/14를 사용한다.</p> </blockquote> <p>위 표에서 볼 수 있듯이, REACT는 Flickr30K, MSCOCO에 대한 zero-shot, full-shot Retrieval 모두에서 일반적인 CLIP에 비해 성능이 향상된다.</p> <ul> <li>Image-to-Text, Text-to-Image 검색 모두에서 성능이 향상되었으며, Fine-tuning 후 에도 성능이 향상되었다.</li> <li>8억 6400만개의 파라미터가 있는 Bletchley에서도 향상된 성능을 보이는 것으로 보아, REACT가 retrieval task에서 모델 size에 따라 확장됨을 알 수 있다.</li> </ul> <h3 id="4-3-dense-prediction-tasks">4-3. Dense Prediction Tasks</h3> <p>REACT는 Image Constrast Loss로 optimize되어있지만, Dense prediction task(obeject detection, semetic segmentation)에서도 효과적이다.</p> <h4 id="object-detection"><strong>Object Detection</strong></h4> <p>SOTA인 ResNet50와 ViT-B/16를 backbone으로 한 RegionCLIP으로 2가지 실험을 진행한다.</p> <ol> <li>Ground-Truth(GT), Region Proposal Networks(RPN)를 사용한 Zero-shot Inference</li> <li>MSCOCO 데이터셋으로 open-vocabulary Object Detection</li> </ol> <p><img src="/assets/img/post/REACT/table5.png" alt="table5" style="zoom:67%;"/></p> <p>위 표를 보면, REACT는 모든 setting에서 CLIP보다 일관되게 성능이 향상됨을 확인할 수 있다. 또한, REACT에서 사용된 데이터는 웹에서 검색되어 무료로 제공되므로, COCO Image-Text pair를 사용할 필요가 없다.</p> <p>따라서 REACT 커스터마이제이션은 Objcet Detection 모델의 성능을 향상시키고, 비용 효율적인 방법으로 unseen 클래스에 대한 일반화 능력을 높일 수 있음을 시사한다.</p> <h4 id="sementic-segmentation"><strong>Sementic Segmentation</strong></h4> <p>추후 업데이트 예정</p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Multi-modal"/><category term="AI"/><category term="multi-modal"/><category term="clip"/><category term="domain"/><category term="vision-language"/><summary type="html"><![CDATA[REACT 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] PromptStyler</title><link href="https://hahngt.github.io/blog/2024/PromptStyler/" rel="alternate" type="text/html" title="[Paper Review] PromptStyler"/><published>2024-05-04T18:14:43+00:00</published><updated>2024-05-04T18:14:43+00:00</updated><id>https://hahngt.github.io/blog/2024/PromptStyler</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/PromptStyler/"><![CDATA[<h1 id="promptstyler-논문-리뷰"><strong>PromptStyler 논문 리뷰</strong></h1> <blockquote> <p>제목 : <strong>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</strong></p> <p>저자 : <a href="https://jhcho99.github.io/">Junhyeong Cho</a>, <a href="https://scholar.google.com/citations?user=7PSBLtIAAAAJ&amp;hl=en">Gilhyun Nam</a>, <a href="https://cvlab.postech.ac.kr/~sungyeon/">Sungyeon Kim</a>, <a href="https://scholar.google.co.kr/citations?user=mDxJj2AAAAAJ&amp;hl=en">Hunmin Yang</a>, <a href="https://suhakwak.github.io/">Suha Kwak</a></p> <p>링크 : <a href="https://arxiv.org/abs/2307.15199">arXiv</a> <a href="https://promptstyler.github.io/">Project Page</a></p> </blockquote> <p>Vision-Language 공간(space)에서 두 모달리티 간의 전이성(transferability)이 존재한다는 것이 최근 연구를 통해 발견되었다. <span style=" background-color: #F7DDBE"><b>이 논문은 이미지를 사용하지 않고, 오직 prompt만 사용하여 Style 변환을 수행하는 PromptStyler를 제안한다.</b></span> 이는 pseudo-words(의사 단어)를 Styler word vector로 표현한 후, Space의 분포 변화를 시뮬레이션하여 다양한 스타일을 생성할 수 있다. 저자들은 이를 통해 이미지 학습 없이 다양한 데이터셋에서 Style 변환을 이루었다고 말한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Domain Adaptation은 Train, Test 데이터 간의 분포 변화가 클 때(<em>distribution shifts</em>), target domain에 모델을 적응시켜 성능을 높인다. 하지만 완전히 새로운 domain에 대해서는 성능이 감소하기 때문에, Domain Generalization이라는 다양한 domain에 대해 모델을 일반화하는 연구가 진행되고 있지만 여러 한계에 부딫힌다.</p> <p>논문에서는 “<strong>source domain 데이터 없이 모델의 latent space에서 다양한 분포 변화를 시뮬레이션하여 Domain Generalization를 효과적으로 수행할 수 있을지?</strong>“에 대한 질문을 던진다.</p> <p>이 논문에서는 Large-scale Vision Language 모델에서 text가 joint vision-language space에서 관련 image를 대표할 수 있다는 점을 활용한다. 특히, cross-modal 전이 가능성을 보여준다.</p> <blockquote> <p>cross-modal 전이</p> <p>이 논문에서는 text feature로 Classifier를 train하고 image feature를 사용해 Classifier에서 inference하는 것을 말함.</p> <p>이 train process를 통해 source-free Domain Generalization을 수행할 수 있으며, image 없이 prompt(text)만을 통하여 다양한 분포 변화를 시뮬레이션 할 수 있다.</p> </blockquote> <p><img src="/assets/img/PromptStyler/figure1.png" alt="figure1" style="zoom:67%;"/></p> <p>저자들은 <span style=" background-color: #F7DDBE"><b>learnable한 단어 vector를 통해 다양한 style을 합성할 수 있는 prompt 기반 Style 생성 기법인 PromptStyler</b></span>를 제안한다.</p> <p>OpenAI에서 개발한 Vision-Language 모델 CLIP을 사용하여 pseudoword $\mathit{S}_{\boldsymbol{\ast}}$에 대한 word vector(“a painting in the style of $\mathit{S}_{\boldsymbol{\ast}}$”)로 스타일을 포착할 수 있다고 한다.</p> <p><img src="/assets/img/PromptStyler/figure2.png" alt="figure2"/></p> <p>PropmtStyler는 다음과 같은 특성이 있다.</p> <ol> <li><strong>orthogonal style features</strong> : style feature들이 서로 영향을 주지 않으면서 독립적으로 구별될 수 있는 것 <ul> <li>style diversity(다양성)를 최대화</li> </ul> </li> <li><strong>content consistency</strong> : 학습된 style이 정보를 왜곡하는 것을 방지 <ul> <li>style-content feature는 <strong>content prompt</strong>(“[class]”)로부터 얻은 content feature가 다른 feature들 보다 더 가까이 위치하도록 강제</li> <li>style-contents feature는 style-content prompt(“a $\mathit{S}_{\boldsymbol{\ast}}$ style of a [class]”)에서 얻어짐</li> </ul> </li> </ol> <p><strong>학습된 style word vector는 style-content feature를 합성하여 Classifier를 Train</strong>시킨다. 이 feature은 joint space에서 “알고있는” content 이미지를 “알려지지 않은” style로 시뮬레이션할 수 있다.</p> <p>Linear Classifier은 [style-content feature, content(“[class]”)] 쌍 데이터로 학습된다.</p> <blockquote> <p>[style-content feature, content(“[class]”)] == [input, label]</p> </blockquote> <p>inference 절차는 다음과 같다.</p> <ol> <li>Image Encoder가 Input 이미지에서 feature 추출</li> <li>추출된 feature를 학습된 Classifier에 입력</li> </ol> <blockquote> <p>Pretrained Vision-Language 모델에서 사용한 Text/Image Encoder를 사용.</p> <p>Text Encoder는 Train, Image Encoder는 Inference에 사용</p> </blockquote> <p>논문에서 제안한 모델은 CLIP에 비해 경량화 되었지만, 더 빠른 Inference 속도를 보여줬다고 말한다.</p> <h2 id="related-work"><strong>Related Work</strong></h2> <h4 id="domain-generalization">Domain Generalization</h4> <p>source 및 target domain간의 분포 변화로 인한 신경망의 성능 저하를 방지하기 위함이다.</p> <p>이에는 2가지 방법이 있다.</p> <ol> <li>multi-source DG <ul> <li>다양한 source로 모델이 다양한 domain의 특징을 학습</li> <li>새로운 domain에 대해서도 잘 일반화</li> </ul> </li> <li>single-source DG <ul> <li>하나의 source만을 사용하지만, 증강 기법 등을 통해 다양한 도메인을 생성</li> <li>multi-source DG의 효과를 기대</li> </ul> </li> </ol> <h4 id="source-free-domain-generalization">Source-free Domain Generalization</h4> <p>source 및 target domain없이 새로운 domain을 합성하여 모델의 일반화 능력을 향상시키는 방법으로, 논문에서 제시하는 새로운 기법이다.</p> <h4 id="joint-vision-language-space">Joint vision-language space</h4> <p>논문에서는 image-text 쌍으로 학습된 Vision-language 모델에서, Joint vision-language space를 활용한다. 이 space에서 prompt(“a painting in the style of $\mathit{S}_{\boldsymbol{\ast}}$”)를 사용하여 시각적 특징을 조작하고 다양한 분포 변화를 시뮬레이션(다양한 style)할 수 있음을 말한다.</p> <h2 id="method"><strong>Method</strong></h2> <p><img src="/assets/img/PromptStyler/figure3.png" alt="figure3"/></p> <ul> <li>Large Vision-Language 모델인 CLIP 사용</li> <li>CLIP의 Image Encoder, Text Encoder를 사용하며 Framework에서 고정되어있음</li> </ul> <h3 id="1-prompt-driven-style-generation"><strong>1. Prompt-driven style generation</strong></h3> <p>Input prompt는 tokenizaiton process를 통해 여러 token으로 변환되며, 각 token은 word lookup process를 통해 word vector로 바뀐다.</p> <p>그 중, pseudo-word $\mathit{S}_{\boldsymbol{\ast}}$는 word lookup process에서 style word vector인 $s_i \in \mathbb{R}^D$로 변환된다.</p> <p>논문에서는 3가지 prompt가 사용된다고 한다.</p> <ul> <li> <p><strong>Style prompt $\mathcal{P}^{\text{style}}_i$</strong> : “a $\mathit{S}_i$ style of a”</p> </li> <li> <p><strong>Content prompt $\mathcal{P}^{\text{content}}_m$</strong> : “$[\text{class}]_m$”</p> </li> <li> <p><strong>Style-content prompt $\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m$</strong> : “a $\mathit{S}_i$ style fo a $[\text{class}]_m$”</p> </li> </ul> <blockquote> <p>$\mathit{S}_i$ : $i$번째 style word vector</p> <ul> <li>$K$개의 스타일을 학습하려면 $K$개의 style word vector인 $\lbrace s_i \rbrace_{i=1}^K$를 학습</li> </ul> <p>$[\text{class}]_m$ : $m$번째 class label</p> </blockquote> <p>저자들은 2가지 Loss 식을 제안한다.</p> <h4 id="style-diversity-loss"><strong>Style diversity Loss</strong></h4> <p>Joint vision-language space에서 Style diversity를 극대화하기 위해 저자들은 style word vector $\lbrace s_i \rbrace_{i=1}^K$를 순차적으로 학습한다.</p> <p>$i$번째 style vector $s_i$가 생성하는 feature를 $T(\mathcal{P}^{\text{style}}_i) \in \mathbb{R}^C$라 하고, 이전에 $1 \sim (i-1)$까지 $\lbrace s_j \rbrace_{j=1}^{i-1}$가 생성한 feature를 $\lbrace T(\mathcal{P}^{\text{style}}_j) \rbrace_{j=1}^{i-1}$라 하자.</p> <p>$T(\mathcal{P}^{\text{style}}_i)$는 이전에 생성되었던 feature들인 $\lbrace T(\mathcal{P}^{\text{style}}_j) \rbrace_{j=1}^{i-1}$에 직교하게 생성된다.</p> <p>즉 $i$번째 style vector를 학습하기 위한 <strong>Style diversity Loss</strong> $\mathcal{L}_{\text{style}}$는 다음과 같다.</p> \[\mathcal{L}_{\text{style}} = \frac{1}{i-1} \sum_{j=1}^{i-1}\left \vert\frac{T(\mathcal{P}^{\text{style}}_i)}{\Vert T(\mathcal{P}^{\text{style}}_i)\Vert_2} \bullet \frac{T(\mathcal{P}^{\text{style}}_j)}{\Vert T(\mathcal{P}^{\text{style}}_j)\Vert_2} \right \vert \tag{1}\] <p>위 식은 $i$번째 style feature와 기존 style feature의 cosine 유사성을 minimize하는 것을 목표로 한다.</p> <blockquote> <p>cosine 유사성이 0이 되면, 직교하게 생성되었다는 의미이다.</p> </blockquote> <h4 id="content-consistency-loss"><strong>Content consistency Loss</strong></h4> <p>위 style diversity loss만을 사용할 때 학습된 style인 $s_i$가 style-content feature $T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)$를 생성할 때, <u>content 정보를 많이 왜곡</u>한다고 한다.</p> <p>이는 style-content feature의 content 정보가 $i$번째 style vector $s_i$를 학습하는 동안 content feature $\mathcal{P}^{\text{content}}_m \in \mathbb{R}^C$와 일관성을 가지게 하여 해결할 수 있다.</p> <p>​ ⇨ $i$번째 style-content feature $T(\mathcal{P}^{\text{style}}_i)$는 해당 content feature $\mathcal{P}^{\text{content}}_m$와 높은 cosine 유사도를 가지게 한다.</p> <p>$i$번째 style vector $s_i$의 경우,</p> <p>($m$번째 class label을 가진 <strong>style-content feature</strong>) - ($n$번째 class label을 가진 <strong>content feature</strong>) 사이의 <b>cosine 유사도 점수인 $z_{imn}$</b>은 다음과 같다.</p> \[z_{imn} = \frac{T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)}{\Vert T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)\Vert_2} \bullet \frac{T(\mathcal{P}^{\text{content}}_n)}{\Vert T(\mathcal{P}^{\text{content}}_n)\Vert_2} \tag{2}\] <p>위 $z_{imn}$을 사용하여 $s_i$를 학습하기 위한 content consistency Loss $\mathcal{L}_{\text{content}}$는 다음과 같다. ($N$은 class 수)</p> \[\mathcal{L}_{\text{content}} = -\frac{1}{N} \sum_{m=1}^N \log\left ( \frac{e^{z_{imm}}}{\sum_{n=1}^N e^{z_{imn}}} \right) \tag{3}\] <p>이 $\mathcal{L}_{\text{content}}$는 style-content feature가 content feature에 가깝게 위치하도록 유도하여 $s_i$가 content 정보를 보존하도록 한다.</p> <h4 id="total-prompt-loss"><strong>Total prompt Loss</strong></h4> <p>PromptStyler는 최종적으로 <strong>Style diversity Loss</strong>와 <strong>Content consistency Loss</strong>를 모두 사용하여 K개의 style word vector $\lbrace s_i \rbrace_{i=1}^K$를 순차적으로 학습한다.</p> \[\mathcal{L}_{\text{prompt}} = \mathcal{L}_{\text{style}} + \mathcal{L}_{\text{content}} \tag{4}\] <p>위 loss를 활용한 학습 <strong>Algorithm 1</strong>은 다음과 같다.</p> <p><img src="/assets/img/PromptStyler/algorithm1.png" alt="algorithm1" style="zoom:50%;"/></p> <h3 id="2-training-a-linear-classifier-using-diverse-styles"><strong>2. Training a linear classifier using diverse styles</strong></h3> <p>앞에서 학습한 $s_i$를 이용하여 Linear Classifier를 학습하는 과정이다.</p> <ol> <li>$K$개의 style word vector $\lbrace s_i \rbrace_{i=1}^K$ 학습</li> <li>$KN$개의 style-content feature 생성 <ul> <li>학습된 $K$개의 style word vector와 $N$개의 class를 이용</li> <li><u>Text encoder $T(\cdot)$</u> 사용</li> </ul> </li> <li>Linear classifier 학습 <ul> <li>$KN$개의 style-content feature과 class label을 사용</li> </ul> </li> </ol> <p>여기서 저자들은, Joint vision-language space를 활용하기 위해 <strong>ArcFace loss</strong>를 사용한다고 한다.</p> <blockquote> <p><strong><a href="https://arxiv.org/abs/1801.07698">ArcFace loss</a></strong>란?</p> <p>ArcFace는 얼굴 인식 작업을 위해 고안된 각도 기반 softmax loss function이다.</p> <p>Classifier의 input feature와 가중치 간의 cosine 유사도를 계산하고 class 간 추가적인 각도 margin penalty를 적용한다.</p> \[L_{ArcFace} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j=1, j\neq y_i}^n e^{s\cos(\theta_j)}}\] <p>각도 기반이기 때문에 Class 간 경계를 더 잘 형성하고 유클리드 거리 기반 손실 함수의 한계를 극복하기 위해 제안된 loss이다.</p> </blockquote> <h3 id="3-inference-using-the-trained-classifier"><strong>3. Inference using the trained classifier</strong></h3> <p>학습된 Classifier를 <u>Image Encoder $I(\cdot)$</u>과 함께 Inference하는 과정이다.</p> <ol> <li>vision-language space에 mapping된 <b>Image feature $I(\text{x}) \in \mathbb{R}^C$</b>를 추출 <ul> <li>$\textbf{x}$는 Input image</li> <li>Encoder $I(\cdot)$에서 이미지를 $\mathcal{l} _2$ 정규화</li> </ul> </li> <li>Classifer로 Class score를 생성 <ul> <li>위 image feature $I(\text{x})$를 사용</li> </ul> </li> </ol> <h2 id="experiments"><strong>Experiments</strong></h2> <h3 id="evaluation-datasets"><strong>Evaluation datasets</strong></h3> <p>Generalization 성능을 평가하기 위해, 데이터셋이 아닌 4가지 Domain Generalization benchmark를 사용한다.</p> <ul> <li>PACS (4개의 도메인과 7개의 클래스)</li> <li>VLCS (4개의 도메인과 5개의 클래스)</li> <li>OfficeHome (4개의 도메인과 65개의 클래스)</li> <li>DomainNet (6개의 도메인과 345개의 클래스)</li> </ul> <h3 id="implementation-details"><strong>Implementation details</strong></h3> <p>저자들은 RTX 3090 GPU에서 30분간 학습시키는 조건을 모두 동일하게 사용하였다고 한다.</p> <h4 id="architecture"><strong>Architecture</strong></h4> <p>pretrained Vision-Language 모델로 CLIP을 사용하였으며, Image Encoder $I(\cdot)$는 ResNet-50을, Text encoder $T(\cdot)$는 Tramsformer를 사용하였다.</p> <blockquote> <p>Image Encoder는 추가적인 비교를 위해 Vision Trasformer인 ViT-L, Vit-B도 사용</p> </blockquote> <h4 id="learning-style-word-vectors-s_i"><strong>Learning style word vectors</strong> <b>$s_i$</b></h4> <p>저자들은 $s_i$를 학습할 때, 다음과 같은 2가지 prompt learning 기법을 사용하였다고 한다.</p> <ol> <li><a href="https://arxiv.org/abs/2109.01134">Learning to Prompt for Vision-Language Models</a> <ul> <li>거대 Vision-Language 모델인 CLIP을 전체 fine-tuning하는 것은 비효율적</li> <li>prompt를 도입하면 성능 향상에 효과적이지만 prompt engineering은 많은 시행착오가 필수적</li> <li><strong><em>Context Optimization (CoOp)</em></strong>를 도입하여 <strong>learnable vector 가 있는 prompt 의 context words 를 모델링</strong></li> </ul> </li> <li><a href="https://arxiv.org/abs/2203.05557">Conditional Prompt Learning for Vision-Language Models</a> <ul> <li>학습용으로 label이 지정된 이미지 몇 개만 사용하는 CoOp는 학습되지 않은 class로 일반화 불가능</li> <li>conditional prompt learning를 도입하여 Input image에 따라 condition이 지정된 prompt를 만듬</li> <li>각 instance에 adaption되므로 class shift에 덜 민감하며, CoOp보다 domain generalization 성능이 향상됨</li> </ul> </li> </ol> <p>style word vectors <b>$s_i$</b>학습 조건은 다음과 같다.</p> <ul> <li>$\sigma = 0.02$인 zero-mean Gaussian 분포를 사용하여 $\lbrace s_i \rbrace_{i=1}^K$를 랜덤으로 초기화</li> <li>SGD optimizer : learning rate = 0.002, momentum = 0.9</li> <li>Total iteration = 100</li> </ul> <h4 id="training-a-linear-classifier"><strong>Training a linear classifier</strong></h4> <ul> <li>50 epochs</li> <li>SGD Optimizer : learning rate = 0.005, momentum = 0.9, batch size = 128</li> <li>ArcFace : scaling factor = 0, 각도 margin = 5</li> </ul> <h4 id="inference"><strong>Inference</strong></h4> <p>Input 이미지는 $224 \times 224$로 resize하고, 정규화를 진행하였다.</p> <h3 id="evaluations"><strong>Evaluations</strong></h3> <h4 id="main-result"><strong>Main Result</strong></h4> <p><img src="/assets/img/PromptStyler/table2.png" alt="table2" style="zoom:50%;"/></p> <p>위 표를 보면 PromptStyler가 모든 benchmark에서 SOTA를 달성한 것을 알 수 있다.</p> <p>zero-shot CLIP을 제외한 기존 방법들은 source domain 데이터셋을 사용하였으며, zero-shot CLIP에서 사용한</p> <ol> <li>domain과 상관없는 prompt (“[class]”)</li> <li>domain별 prompt (“a photo of a [class]”)</li> </ol> <p>위 2가지 경우에도 PromptStyler가 더 좋은 성능을 보인다.</p> <p><strong>즉, latent space에서 prompt를 통해 다양한 분포 변화를 시뮬레이션함으로써 이미지를 사용하지 않고도 CLIP의 일반화 능력을 효과적으로 향상시킴을 알 수 있다.</strong></p> <h4 id="computational-evaluations-">**Computational evaluations. **</h4> <p>이 섹션에서는 parameter 수와 inference 속도를 비교한다. 비교 대상은 zero-shot CLIP과 PromptStyler이며, batch size를 1로 설정하고 단일 RTX 3090 GPU를 사용하였다.</p> <p><img src="/assets/img/PromptStyler/table3.png" alt="table3" style="zoom:50%;"/></p> <p>PromptStyler는 inference할 때 Text Encoder를 사용하지않아 zero-shot CLIP에 보다 <strong>가볍고 빠르다</strong>는 것을 알 수 있다.</p> <h4 id="t-sne-visualization-results"><strong>t-SNE visualization results</strong></h4> <p>prompt loss에 사용된 $\mathcal{L}_{\text{style}}$과 $\mathcal{L}_{\text{content}}$의 성능을 VLCS benchmark를 통해 평가하고, t-SNE로 시각화한다.</p> <blockquote> <p><strong><a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">Visualizing Data using t-SNE</a></strong></p> <p>t-SNE는 t-SNE는 2차원 또는 3차원 지도에 가지고 있는 데이터 포인트에 위치를 부여함으로서 이를 시각화할 수 있도록 해주는 방법론이다.</p> </blockquote> <p><img src="/assets/img/PromptStyler/figure4.png" alt="figure4" style="zoom:67%;"/></p> <ul> <li> <p>(a) - $\mathcal{L}_{\text{style}}$ : 다른 class label(다른 모양들)과 유사한 feature를 공유(같은 색끼리 모여있음)</p> </li> <li>(b) - $\mathcal{L}_{\text{content}}$ : style-content feature의 다양성 하락</li> <li>(c) - $\mathcal{L}_{\text{style}} + \mathcal{L}_{\text{content}}$ : content 정보를 왜곡시키지 않으면서 다양한 스타일을 생성</li> </ul> <h4 id="text-to-image-synthesis-results"><strong>Text-to-Image synthesis results</strong></h4> <p><img src="/assets/img/PromptStyler/figure5.png" alt="figure5"/></p> <p>위 사진은 “a <b>$\mathit{S}_{\boldsymbol{\ast}}$ </b>style of a <strong>cat</strong>“에서 추출된 style-content feature를 diffusers라는 라이브러리로 시각화한시킨 결과이다. 6개의 학습된 style word vector $s_i$를 사용하였다.</p> <h3 id="more-analyses"><strong>More analyses</strong></h3> <h4 id="loss">Loss</h4> <div class="image-container"> <img src="/assets/img/PromptStyler/table4.png" alt="table4" style="zoom:50%;"/> <img src="/assets/img/PromptStyler/table5.png" alt="table5" style="zoom:50%;"/> </div> <ul> <li>왼쪽 : Prompt Loss에 사용한 $\mathcal{L}_{\text{style}}$과 $\mathcal{L}_{\text{content}}$의 성능</li> <li>오른쪽 : Classifier에 사용한 ArcFace Loss의 성능</li> </ul> <h4 id="effect-of-the-number-of-styles"><strong>Effect of the number of styles</strong></h4> <p><img src="/assets/img/PromptStyler/figure6.png" alt="figure6"/></p> <p>style 개수가 5개이상만 되어도 상당한 성능 향상을 보여준다.</p> <h4 id="effect-of-the-number-of-iterations-">**Effect of the number of iterations **</h4> <p><img src="/assets/img/PromptStyler/figure7.png" alt="figure7"/></p> <p>iter가 20번이면 충분히 좋은 결과를 얻을 수 있음을 알 수 있다.</p> <h2 id="limitation"><strong>Limitation</strong></h2> <p>PromptStyler는 pretrained Vision-Language 모델의 joint vision-language space 성능에 의존한다. Terra Incognita 데이터셋의 경우, CLIP에서의 성능이 좋지 않기 때문에 PromptStyler에서도 성능이 감소하는 것을 확인하였다고 한다.</p> <h2 id="code"><strong>Code</strong></h2> <p><a href="https://colab.research.google.com/drive/1JDPvPufSxaj2f9T_VOY6tI4OhOtmWEFx?usp=sharing">Colab</a></p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Multi-modal"/><category term="AI"/><category term="generative"/><category term="Diffusion"/><category term="domain"/><category term="vision-language"/><summary type="html"><![CDATA[PromptStyler 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] DreamFusion</title><link href="https://hahngt.github.io/blog/2024/DreamFusion/" rel="alternate" type="text/html" title="[Paper Review] DreamFusion"/><published>2024-04-30T20:02:43+00:00</published><updated>2024-04-30T20:02:43+00:00</updated><id>https://hahngt.github.io/blog/2024/DreamFusion</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/DreamFusion/"><![CDATA[<h1 id="dreamfusion-논문-리뷰"><strong>DreamFusion 논문 리뷰</strong></h1> <p>DreamFusion: Text-to-3D Using 2D Diffusion: <a href="https://arxiv.org/abs/2209.14988">arxiv</a></p> <p>DreamFusion은 3D Generative model 관련 논문에서 많은 인용수를 자랑한다. 2D 이미지 여러장을 이용해 획기적인 방법으로 3D modeling을 수행하는 Nerf와 2D Diffusion 모델을 잘 섞어서 text-to-3D generation을 수행하는 DreamFusion은 이후 DreamBooth3D, SweetDreamer 등 3D generation 분야에서 SOTA를 달성한 많은 논문들에서 언급된다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>지금까지 생성모델은</p>]]></content><author><name></name></author><category term="Paper Review"/><category term="3D Vision"/><category term="Generative model"/><category term="AI"/><category term="generative"/><category term="Diffusion"/><category term="3D"/><summary type="html"><![CDATA[DreamFusion 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] DreamBooth</title><link href="https://hahngt.github.io/blog/2024/DreamBooth/" rel="alternate" type="text/html" title="[Paper Review] DreamBooth"/><published>2024-04-25T17:58:43+00:00</published><updated>2024-04-25T17:58:43+00:00</updated><id>https://hahngt.github.io/blog/2024/DreamBooth</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/DreamBooth/"><![CDATA[<h1 id="dreambooth-논문-리뷰"><strong>DreamBooth 논문 리뷰</strong></h1> <p>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation : <a href="https://arxiv.org/abs/2208.12242">arxiv</a></p> <p>구글과 보스턴 연구진이 2022년에 발표한 논문으로, Diffusion 논문에 대한 새로운 Fine-tuning 기법을 제시하였다. 특정 피사체에 대해 적은 데이터만으로도 대상의 사실적인 이미지를 생성할 수 있다. 대상의 핵심 특징을 보존하여 합성하고, 기존의 모델의 큰 오염 없이 새로운 피사체에 대해 이미지를 생성할 수 있다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>이 연구는 특정 대상을 새로운 관점에서 자연스럽게 재구성하는 것에 집중한다. 최근 많이 연구되는 text-to-image 모델은 대규모 image-caption 데이터셋으로 semetic하고 prior하게 학습하여 다양한 고품질 이미지를 생성하는 것이 장점이다.</p> <blockquote> <p>prior하다는 것은 “개”라는 단어를 다양한 상황과 포즈에서 나타날 수 있는 개의 여러 instance와 연결시키는 것</p> </blockquote> <p><img src="/assets/img/post/DreamBooth/figure1.png" alt="figure1"/></p> <p><span style=" background-color: #F7DDBE">이 연구에서는 text-to-image diffusion 모델의 “<b>개인화(personalization)</b>“에 집중한다.</span> 모델의 language-vision dictionary를 확장하여 사용자가 특정 subject를 자신이 원하는 새로운 단어와 결합하여 생성할 수 있도록 한다. 사용자가 생성하고자 하는 특정 대상에 새로운 단어를 연결하여 대상의 key feature를 high fidelity로 유지하면서 다양한 context 이미지를 합성할 수 있다.</p> <p>본 논문에서는 특정 subject의 3~5장 이미지를 사용하여 대상을 rare token identifier로 나타내고 이를 모델의 ouput domain에 삽입하고, subject와 연결된 unique identifier를 사용하여 이미지를 합성한다.</p> <blockquote> <p>이를 subject-driven generation이라 하며, 본 논문에서 이러한 새로운 문제를 정의하였다.</p> </blockquote> <p><img src="/assets/img/post/DreamBooth/figure2.png" alt="figure2" style="zoom:50%;"/></p> <p>위 이미지는 DALL-E2, Imagen과 함께 본 논문에서 제안한 기법을 비교한다.</p> <p>Input images에 있는 시계의 key feature(숫자 3이 특이한 글씨체로 쓰여진 시계)를 유지했는지 (fidelity), 다양한 context 이미지를 생성하였는지(New contexts)를 보여준다.</p> <p>이 논문에서는 위와 같이 pretrain된 diffusion 기반의 text-to-image 모델에 대해 새로운 fine-tuning 기법을 제안한다.</p> <h2 id="method"><strong>Method</strong></h2> <p>이 논문에서는 text 설명 없이s ubject에 대한 이미지 3~5장 만 주여지면, text prompt에 따라 detail fidelity가 높고 다양한 context로 이미지를 생성하는 것을 목표로 한다.</p> <h3 id="1-text-to-image-diffusion-models"><strong>1. Text-to-Image Diffusion Models</strong></h3> <p>text encoder $\Gamma$와 text prompt $\textbf{P}$로 만들어진 conditioning vector $c = \Gamma (\textbf{P})$가 있다. 이 $c$와 초기 $\epsilon \sim \mathcal{N}(0, \textit{I})$가 주어졌을 때, $\mathbf{x}_{\text{gen}} = \hat{ \mathbf{x}}_\theta (\epsilon, c)$를 생성한다.</p> <p>또한 이 $\hat{ \mathbf{x}}_\theta$는 다음과 같이 noise 이미지 $\mathbf{z}_t := \alpha_t\mathbf{x} + \sigma_t\mathbf{\epsilon}$의 noise를 제거하도록 학습한다.</p> \[\mathbb{E}_{\mathbf{x},c, \epsilon, t} \left[w_t || \hat{\mathbf{x}_\theta}(\alpha_t\mathbf{x} + \sigma_t\epsilon, c) - \mathbf{x}||^2_2\right] \tag{1}\] <h3 id="2-personalization-of-text-to-image-models"><strong>2. Personalization of Text-to-Image Models</strong></h3> <p>첫번째 task는 subject의 instance를 모델의 output domain에 삽입하는 것이다. 몇장 안되는 subject의 이미지로 생성 모델을 fine-tuning 할 때 가장 핵심적인 문제는 target distribution을 잘 학습하지 못하는 것이다. (그밖에도 overfitting, mode-collapse가 있음)</p> <p>이를 해결하기 위해 Eq. 1의 diffusion loss를 fine-tuning하여 새로운 정보를 domain에 삽입하는 데 효과가 좋다는 것을 발견하였다고 한다.</p> <h4 id="designing-prompts-for-few-shot-personalization"><strong>Designing Prompts for Few-Shot Personalization</strong></h4> <p>목표는 diffusion 모델의 <strong>dictionary(사전)에 새로운 (unique identifier, subject) 쌍을 이식</strong>하는 것이다. subject의 모든 input image에 “a [identifier] [class noun]”라는 label을 지정해준다.</p> <blockquote> <p>[identifier]는 subject에 연결된 고유값이며, [class noun]는 dog, cat과 같은 class를 말한다.</p> <p>[class noun]의 경우 classifier를 사용하거나, 사용자가 직접 붙여준다.</p> </blockquote> <p>즉, 논문에서는 모델의 특정 class의 prior를 subject의 unique identifier embedding과 연결하여 다양한 context에서 subject를 생성할 수 있도록 한다.</p> <h4 id="rare-token-identifiers"><strong>Rare-token Identifiers</strong></h4> <p>먼저 영어에서 각 문자를 tokenize하여 rare한 token을 찾는다. 이 token을 text space로 변환하여 identifier가 강력한 prior를 가질 확률을 최소화한다. 이를 <strong>rare-token lookup</strong>이라 하며, 이를 수행하여 rare-token identifier $f(\hat{\textbf{V}})$ sequence를 얻는다. 그런 다음, $f(\hat{\textbf{V}})$를 de-tokenize하여 text $\hat{\textbf{V}}$를 얻는다.</p> <blockquote> <p>$f$는 문자 sequences를 token에 mapping하는 tokenizer이고, $\hat{\textbf{V} }$는 $f(\hat{\textbf{V}})$에서 decoding된 text를 의미한다. 논문에서 token은 3자 이하의 unicode로 구성된 문자일 때 성능이 가장 좋다고 언급한다.</p> </blockquote> <h3 id="3-class-specific-prior-preservation-loss"><strong>3. Class-specific Prior Preservation Loss</strong></h3> <p>논문에서는 subject의 fidelity를 극대화 하기 위해서는 model의 모든 layer를 fine-tuning 해야함을 경험적으로 알 수 있었다고 한다. 여기에는 text-embedding을 condition으로 하는 fine-tuning layer가 포함되며, 이로 인하여 <strong>language drift</strong>문제가 발생한다고 한다.</p> <blockquote> <p><strong>language drift</strong></p> <p>language drift란 language model fine-tuning에서 관찰되는 문제로, 특정 task에 맞게 fine-tuning된 모델이 task에 대한 성능을 올리기 위해 점차 언어에 대한 syntactic and semantic knowledge을 잃어버리는 현상이다.</p> <p>논문에서는 위와 같이 언어 모델에서 나타나는 유사한 문제가 diffusion 모델에 영향을 미치는 것을 자기네들이 처음 발견하였다고 한다.</p> </blockquote> <p>또 다른 문제는 <strong>output의 다양성이 감소</strong>할 수 있다는 것이다. text-to-image diffusion 모델은 다양성이 높지만, 적은 image 데이터셋으로 fine-tuning 할 때 variability(가변성)이 줄어들 위험이 있다.</p> <p><strong>위 언급한 2가지 문제를 해결하기 위해 Class-specific Prior Preservation Loss를 제안한다.</strong> 자체적으로 생성된 sample로 모델을 가두어 여러번의 fine-tuning이 시작되어도 prior를 유지하도록 하는 것이다.</p> <p>randon initial noise $\mathbf{z}_{t_1} \sim \mathcal{N}(0, \textbf{I})$과 conditioning vector $\textbf{c}_{pr} := \Gamma(f(\text{“a [class noun]”}))$로 고정된 pretrained diffusion model에서 $\mathbf{x}_{pr} = \hat{\mathbf{x}}(\mathbf{z}_{t_1}, \mathbf{c}_{pr})$을 생성한다.</p> <p>해당 loss는 다음과 같다.</p> \[\mathbb{E}_{\mathbf{x},c, \epsilon, \epsilon', t} \left[w_t || \hat{\mathbf{x}_\theta}(\alpha_t\mathbf{x} + \sigma_t\epsilon, c) - \mathbf{x}||^2_2 + \lambda w_{t'}|| \hat{\mathbf{x}_\theta}(\alpha_{t'}\mathbf{x}_{\text{pr}} + \sigma_{t'}\epsilon', c_{\text{pr}}) - \mathbf{x}_\text{pr}||^2_2 \right] \tag{2}\] <p>여기서 추가된 2번째 term은 생성된 sample로 모델을 supervise하는 prior-preservation term이며, $\lambda$는 이 term의 가중치를 조절한다.</p> <p><img src="/assets/img/post/DreamBooth/figure3.png" alt="figure3" style="zoom:50%;"/></p> <p>위 사진은 class에서 생성된 sample과 Prior Preservation Loss로 모델을 fine-tuning하는 과정을 보여준다.</p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="AI"/><category term="generative"/><category term="Diffusion"/><category term="fine-tuning"/><summary type="html"><![CDATA[DreamBooth 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] TADA : Timestep-Awara Data Augmentation for Diffusion models</title><link href="https://hahngt.github.io/blog/2024/TADA/" rel="alternate" type="text/html" title="[Paper Review] TADA : Timestep-Awara Data Augmentation for Diffusion models"/><published>2024-04-06T20:27:11+00:00</published><updated>2024-04-06T20:27:11+00:00</updated><id>https://hahngt.github.io/blog/2024/TADA</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/TADA/"><![CDATA[<h1 id="tada--timestep-awara-data-augmentation-for-diffusion-models-논문-리뷰"><strong>TADA : Timestep-Awara Data Augmentation for Diffusion models 논문 리뷰</strong></h1> <p><a href="https://openreview.net/forum?id=U6Mb3CRuj8">논문 링크</a></p> <p>데이터 증강(Data Augmentation)은 주어진 원본 데이터를 확장하여 데이터셋의 다양성을 증가시키는 기법이다. 이 방법은 특히 학습 데이터가 부족한 경우, 모델의 일반화 능력을 향상시키기 위해 사용한다.</p> <p>이번 논문에서는 Diffusion model의 distribution 변화가 특정 timestep에서 발생한다는 것을 발견하고 timestep에 따라 유연하게 강도를 조정하는 Data augmentation 전략을 제안한다.</p> <p>이러한 전략이 다양한 diffusion model에 도움되기를 바란다고 저자들은 말한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Diffusion 모델의 reverse process에서 데이터 증강이 어떤 영향을 미치는지 살펴보았을 때, 성능 저하에 영향을 미치는 timestep을 파악하고, 특정 timestep이 sampling 과정의 변형에 기여하여 의도하지 않은 sample 생성이 이루어진다는 것을 파악하였다.</p> <p>위에서 파악한 것을 바탕으로 저자들은 timestep에 따라 증강 강도를 유연하게 조정하는 Timestep-Aware Data Augmentation (TADA) 전략을 제안한다.</p> <p>위 전략은 $T(x_t, w_t)$로 표시되는데, timestep $t$(noise level이 되기도 함)에 따라 증강 강도 $w_t$를 조절한다. 즉, input 데이터에 큰 noise가 포함된 경우 증강 강도가 강한 train sample($w_t$)로 model을 train한다. vulnerable한 timestep 동안에는 0에 가까운 $w_t$를 적용하다가 다시 약한 noise가 포함된 data가 input이 들어오면 다시 강도를 높인다.</p> <h2 id="method"><strong>Method</strong></h2> <h3 id="preliminaries"><strong>Preliminaries</strong></h3> <p>TADA를 살펴보기 전 필요한 여러 개념들을 살펴보자</p> <h4 id="diffusion-models"><strong>Diffusion models</strong></h4> <p>$n$개의 데이터 point ${x_0^1, \cdots, x_0^n}$가 분포 $q(x_0)$에서 sampling되었다고 가정하자. Diffusion model은 분포 $q(x_0)$에 가장 근접한 모델 $p_θ(x_0)$를 만드는 것이 목표이다.</p> <p>Diffusion model을 training하는 것에는 2가지 process가 있다.</p> <ul> <li>foward process : Gaussian noise $z \sim \mathcal{N}(0,\textit{I})$를 timestep $t$에 따라 추가하여 noise data $x_t$를 만드는 과정</li> <li>reverse process : foward process에서 추가된 noise를 제거(denoising)하여 원래의 데이터를 복구하는 과정</li> </ul> <p>Foward process에서는 $x_0$를 이용해 $x_t = \alpha_t x_0 + \sigma^2 z$를 계산한다. 모델 $\widehat{\epsilon}_\theta (x_t, t)$는 weighted MSE를 최소화하여 timestep $t$에서 추가된 noise $z$를 예측하도록 train된다.</p> <p>Reverse process에서는 $p_\theta(x_{t-1}|x_t)$를 통해 $x_t$로 $x_{t-1}$을 계산한다. 이 과정은 $x_T \sim \mathcal{N}(0, \textit{I})$, 즉 완전한 Gaussian noise인 상태에서 시작한다. $\widehat{x}_\theta (x_t, t) = \frac{x_t - \sigma_t^2 \widehat{\epsilon}<em>\theta (x_t, t)}{\alpha_t}$일 때, reverse process $x</em>{t-1}$는 다음과 같다.</p> \[\widehat{x}_{t-1} = \alpha_{t-1} \widehat{x}_\theta(x_t, t) + \sigma_{t-1}^2z \tag{1}\] <p>$\alpha_t$와 $\sigma_t^2$, Objective, sampline method 등에는 다양한 방법이 있지만, 이 논문에서는 DDPM의 향상된 버전인 Improved DDPM에서 제안한 방법을 채택하였다.</p> <h4 id="signal-to-nosie-ratiosnr"><strong>Signal-to-nosie ratio(SNR)</strong></h4> <p><a href="https://arxiv.org/abs/2107.00630">Variational Diffusion Models</a>에서는 signal-to-noise ratio(SNR)이라는 개념을 도입했으며, 이는 각 timestep $t$에서 noise level을 측정하는 개념이다. SNR은 $t$에 따라 점점 감소하는 함수이다. ($\text{SNR}(t) = \frac{\alpha_t^2}{\sigma_t^2}$)</p> <h4 id="data-augmentation"><strong>Data augmentation</strong></h4> <p>이 논문에서는 데이터 증강을 $T(x_t, w)$로 표시하며, $w \in [0,1]$은 증강의 강도를 제어하는 정규화된 hyper parameter이다.</p> <h3 id="analyzing-the-effect-of-data-augmentation-on-learned-reverse-process"><strong>Analyzing the effect of data augmentation on learned reverse process</strong></h3> <p>데이터 증강이 Diffusion model에 미치는 영향을 파악하고 취약한 $t$를 파악하기 위해 증강을 통해 학습된 model과 일반적인 model의 reverse process를 비교한다.</p> <p><img src="/assets/img/post/TADA for Diffusion/Figure 1.png" alt="Figure 1"/></p> <p>위 이미지에서 <strong>(a)</strong>는 horizontal-flip 으로만 train된 baseline 모델 ($\widehat{ε}_{\text{base}}$)과 증강 데이터로 train된 모델인 ($\widehat{ε}_{\text{aug}}$)의 두 Diffusion model의 reverse process를 비교한 실험의 결과이다. <strong>(a)</strong>그래프를 보면 rough(초기)와 fine(마지막) 시점에서는 두 모델의 차이(<span style="color: #F7DDBE">노란선</span>)가 거의 없으며, sensitive한 timestep에서는 차이가 벌어짐을 알 수 있다.</p> <p><strong>(b)</strong>는 <strong>(a)</strong>의 sensitive timestep에서 $\widehat{ε}_{\text{aug}}$와 $\widehat{ε}_{\text{base}}$ 사이의 reverse process를 $\widehat{ε}_{\text{base}} \rightarrow \widehat{ε}_{\text{aug}}$ 와 $\widehat{ε}_{\text{aug}} \rightarrow \widehat{ε}_{\text{base}}$로 바꿔서 두 가지 sampling process를 생성한 결과이다.</p> <ul> <li>처음 두 행은 sample이 처음에는 데이터 분포를 따르지만 결국 증강된 데이터 분포에 포함되는 $\widehat{ε}_{\text{base}} \rightarrow \widehat{ε}_{\text{aug}}$ 의 경우를 보여준다. 즉, sample이 $\widehat{ε}_{\text{base}}$에 의한 데이터 분포를 따르더라도 $\widehat{ε}_{\text{aug}}$에 의해 변경되어 증강된 것과 같은 결과물로 다시 나타난다.</li> <li>마지막 두 행은 처음에 $\widehat{ε}_{\text{aug}}$의 궤적을 따르던 샘플이 sensitive timestep 동안 $\widehat{ε}_{\text{base}}$에 의해 조정되어 궁극적으로 데이터 분포와 일치하게 된 경우를 보여준다.</li> </ul> <blockquote> <p>Diffusion model의 sensitive timestep에서 sampling trajectory을 변경하여 최종 sample이 원래 데이터 분포를 따르는지 아니면 증강된 분포를 따르는지를 결정할 수 있음을 알 수 있다.</p> </blockquote> <h3 id="timestep-aware-data-augmentation-for-diffusion-models"><strong>Timestep-Aware Data Augmentation for Diffusion models</strong></h3> <p>위 실험을 바탕으로 논문에서는 rough와 fine timestep에서는 강력한 증강(큰 $w$)을 적용하고, sensitive timestep에서는 강도를 낮추는 전략을 제안한다.</p> <ul> <li><strong>sensitive timestep($t \in [t_{rough}, t_{fine}]$)</strong>: 약한 강도의 증강이 train 데이터에 적용되어 생성된 sample $p(x)$에 가깝게 유지</li> <li><strong>rough($t \in (t_{rough},T]$) 및 fine($t \in [0,t_{fine})$) timestep</strong>: 강력한 증강을 통해 과적합을 방지하고 확산 모델의 일반화 기능을 개선</li> </ul> <p>증강 강도 $w_t$는 다음과 같이 구할 수 있다.</p> \[w_t = k(r_t - r_{rough})(r_t - r_{fine}) + \delta \tag{2}\] <p>여기서 $r_t = \log (\textbf{SNR}(t))$이며, $\delta$는 sensitive timestep에서 최대 증강 강도를 나타낸다. $w_t$가 너무 낮으면 안되기 때문에 $\delta=0.1$로 설정하였다.</p> <p><img src="/assets/img/post/TADA for Diffusion/Figure 2.png" alt="Figure 2"/></p> <blockquote> <p>위 그림은 <strong>Eq. 2</strong>에 대한 그래프이다.</p> </blockquote> <p><img src="/assets/img/post/TADA for Diffusion/Figure5.png" alt="Figure5" style="zoom:50%;"/></p> <p>$k$는 clipping이 발생하기 전에 $w_t$가 0이 되도록 한다. 이는 자동으로 계산되는데, 위 그래프는 $k$를 조절하였을 때 $ r_{rough}$와 $ r_{fine}$에서 증강 강도가 어떻게 변화하는지 나타내는 그래프이다.</p> <p>Diffusion 모델의 noise level이 해상도에 따라 달라지므로 SNR을 조정할 필요를 느끼고 다른 논문에서 제안한 식을 적용하였다. 이미지의 해상도를 $d$, timestep을 $t$라 할 때</p> \[\text{SNR}_{calibrated}(t) = \frac{\text{SNR}(t)}{(d/64)^2} \tag{3}\] <p>이 식을 <strong>Eq. 2</strong>의 $r_t$에 적용시키면 해상도에 따른 조정이 가능해진다고 한다.</p> <h2 id="experiment"><strong>Experiment</strong></h2> <p>TADA를 이전 연구에서 사용된 두 가지 증강 방법과 비교하는 실험을 한다.</p> <ol> <li><strong>50% 수평 뒤집기(h-flip)</strong>: 확산 모델에서 일반적으로 사용</li> <li><strong>증강 정규화 방법(AR)</strong>:&lt;Elucidating the Design Space of Diffusion-Based Generative Models&gt;(2023)에서 사용한 기법</li> </ol> <h3 id="benefit-of-tada"><strong>Benefit of TADA</strong></h3> <p>TADA는 sensitive timestep 동안 증강 강도를 조정하기 때문에 <strong>distribution-shifted sample을 생성하지 않는다</strong>는 것을 보여주며, <strong>overfitting 문제를 완화</strong>하여 작은 데이터 세트에서 Diffusion 모델의 성능을 향상시킨다는 것을 보여준다.</p> <p><img src="/assets/img/post/TADA for Diffusion/table1.png" alt="table1"/></p> <p>위 <strong>표 1</strong>를 보면, iter가 100,000회인 FID 및 KID 결과를 나타낸다. h-filp 기법보다 성능이 좋은 것을 알 수 있으며, TADA라는 비교적 간단한 기법으로 AR과 비슷한 결과를 낸다는 것을 알 수 있다.</p> <p><img src="/assets/img/post/TADA for Diffusion/Figure3.png" alt="Figure3" style="zoom:50%;"/></p> <p>위 사진은 TADA가 분포 내에서 sample을 생성하는지 시각화한 것으로, 모든 timestep에서 증강을 적용한 Naive augmentation에 비해 TADA가 훨씬 선명한 얼굴을 생성한다는 것을 알 수 있다.</p> <p><img src="/assets/img/post/TADA for Diffusion/Figure4.png" alt="Figure4" style="zoom:50%;"/></p> <p>위 그래프는 overfitting에 미치는 영향을 보여주는 그래프로, FID값을 나타내는 그래프이다.</p> <p>TADA(보라색)가 h-flip(파랑색)보다 overfitting을 완화하는 데 더 효과적임 알 수 있다.</p> <h3 id="generalization"><strong>Generalization</strong></h3> <p>TADA가 Diffusion 모델 학습에서 다양한 설계에 적용할 수 있음을 알아보기 위해 높은 해상도, Transfer learning, 다양한 Noise scheduling, 다양한 모델 크기로 실험을 진행하였다.</p> <h4 id="high-resolution"><strong>High resolution</strong></h4> <p><img src="/assets/img/post/TADA for Diffusion/table2.png" alt="table2" style="zoom:50%;"/></p> <p><strong>표 2</strong>역시 <strong>표 1</strong>과 동일</p> <p><img src="/assets/img/post/TADA for Diffusion/Figure6.png" alt="Figure6"/></p> <p>높은 해상도인 256*256 데이터에 TADA를 적용한 결과이다.</p> <h4 id="transfer-learning"><strong>Transfer Learning</strong></h4> <p><img src="/assets/img/post/TADA for Diffusion/table3.png" alt="table3" style="zoom:50%;"/></p> <p>위 <strong>표 3</strong>은 AHFQ-v2의 세 가지 class에서 TADA가 AR을 능가하고 있음을 보여준다.</p> <h4 id="noise-scheduling-sampling-step-model-size"><strong>Noise scheduling, Sampling step, Model size</strong></h4> <p><img src="/assets/img/post/TADA for Diffusion/table4.png" alt="table4" style="zoom:50%;"/></p> <p>위 <strong>표 4</strong>는 3가지 관점에서 TADA와 h-flip을 평가한 결과이다.</p> <ul> <li>TADA는 <strong>linear</strong>과 <strong>cosine</strong> <strong>noise schedule</strong> 모두에서 h-flip보다 우수한 성능을 보여준다.</li> <li><strong>{50, 100, 500, 1000} sampling</strong> 단계에서의 TADA와 h-flip의 성능 추이를 보면, TADA는 h-flip보다 지속적으로 우수한 성능을 보인다.</li> <li>TADA는 <strong>대규모 모델</strong>에서는 뚜렷한 이점을 보였지만 <strong>소규모 모델</strong>에서는 효과가 떨어진다. <ul> <li>이러한 결과는 데이터 증강이 대규모 모델의 성능을 향상시키는 데 중요한 역할을 하기 때문에 예상되는 결과라고 논문에서 말한다.</li> </ul> </li> </ul> <h3 id="ablation-study"><strong>Ablation Study</strong></h3> <p>이 section은 TADA에서 각 구성 요소의 영향을 분석하기 위해 세 가지 ablation studies를 수행한다.</p> <p><img src="/assets/img/post/TADA for Diffusion/table5.png" alt="table5"/></p> <h4 id="augmentation-range"><strong>Augmentation Range</strong></h4> <p><strong>표 5a</strong>에서는 특정 timestep에서만 $w_t$를 적용하고 다른 범위에서는 $w_t = 0$으로 설정하여, $rough$, $sensitive$, $fine$에 대한 FID score를 측정한다.</p> <p>baseline(h-flip)을 ‘none’으로 고려할 때, 각 timestep의 기여도는 sampling 단계에 따라 다르다.</p> <ul> <li>50 sampling step에서는 sensitive timestep이 가장 영향력이 크다.</li> <li>250 sampling step에서는 fine timestep이 가장 영향력이 크다.</li> </ul> <p>sampling step에 관계없이 TADA가 다양한 범위에서 baseline model에 비해 명확한 개선을 보이는 것을 알 수 있다.</p> <h4 id="m-variation"><strong>$M$ Variation</strong></h4> <p><strong>표 5b</strong>에서는 다양한 $M$ 값에 대해 TADA를 테스트하고, 250 sampling step에서의 FID score를 측정한다. $M = 0$ (h-flip)의 baseline model과 비교할 때,</p> <ul> <li>$M$이 적당히 작을 때(ex: $M=2$) 증강을 적용하는 것이 유익하다는 것을 알 수 있다.</li> <li>$M$이 너무 크면 train data 분포가 원본 train data로부터 너무 멀어져 학습을 방해할 수 있다.</li> </ul> <p>이를 바탕으로, 논문에서는 모든 setting 에 대해 $M = 2$로 설정한다.</p> <h4 id="κ-variation"><strong>$κ$ Variation</strong></h4> <p>TADA에서 $κ$의 영향을 평가하기 위해, <strong>linear</strong>과 <strong>cosine</strong> <strong>noise schedule</strong>에 대해 $κ$의 값을 조절하여 실험을 진행하였다.</p> <p><strong>표 5c</strong>는 $κ$를 변화시키며 얻은 FID score를 보여주는데, 직접 설정해준 $κ$가 default보다 성능이 우수한 것을 알 수 있다.</p> <blockquote> <p>default는 $κ$가 자동으로 계산되는 것임</p> </blockquote> <p>default가 합리적이지만, 더 나은 hyper-parameter tuning으로 TADA의 성능을 향상시킬 수 있음을 시사한다.</p> <h2 id="discussion--conclusion"><strong>Discussion &amp; Conclusion</strong></h2> <p>Generative model을 위한 다른 데이터 증강 방법과 마찬가지로, TADA는 데이터 부족 문제를 완전히 해결할 수 없으며, 더 많은 데이터를 수집하는 것보다 덜 효과적이다. 또한, 논문에서 비교 실험을 위해 채택한 증강기법(h-flip, AR)은 GAN에서 일반적으로 사용되는 기법이므로 Diffusion model에 최적이 아닐 수 있다.</p> <p>하지만, Diffusion model에서 데이터 증강과 분포 변화 사이의 관계를 처음으로 심층적으로 조사하였으며, 특정 timestep에서는 유의미한 영향을 미친다는 것을 알아내었다.</p> <p>이를 바탕으로 시간 간격에 따른 데이터 증강 강도를 조절하는 방법인 TADA를 제안하였으며 간단하게 구현하더라도 Dataset, model 구성, Sampling step에 걸쳐 Diffusion model을 일관되게 향상시킬 수 있음을 보여주었다.</p> <p>TADA는 데이터가 제한된 환경에서 Diffusion model의 성능을 효과적으로 개선하여 overfitting 문제를 해결하고 Transfer learning에서도 성능을 개선하였다.</p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="AI"/><category term="generative"/><category term="Diffusion"/><summary type="html"><![CDATA[TADA : Timestep-Awara Data Augmentation for Diffusion models 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] SAGAN - Self Attention GAN</title><link href="https://hahngt.github.io/blog/2024/SAGAN/" rel="alternate" type="text/html" title="[Paper Review] SAGAN - Self Attention GAN"/><published>2024-04-03T21:41:11+00:00</published><updated>2024-04-03T21:41:11+00:00</updated><id>https://hahngt.github.io/blog/2024/SAGAN</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/SAGAN/"><![CDATA[<h1 id="self-attention-gansagan-논문-리뷰"><strong>Self-Attention GAN(SAGAN) 논문 리뷰</strong></h1> <p><a href="https://arxiv.org/abs/1805.08318">논문 링크</a></p> <p>이번에는 Self-attention 기법을 GAN에 적용한 SAGAN에 대한 논문을 읽어볼까 한다.</p> <p>기존의 GAN은 convolution에서 low-resolution feature map의 정보만을 활용해 high-resolution인 detail 요소를 생성한다. 하지만, SAGAN에서는 모든 feature map의 정보를 활용하여 detail 요소를 생성하며, $D$는 멀리 있는 아주 작은 detail까지 서로 일치하는지 확인할 수 있다.</p> <p>Self-Attention을 적용한 GAN인 SAGAN은 이미지 생성분야에서 새로운 SOTA를 달성하였다.</p> <h2 id="️introduction">✏️<strong>Introduction</strong></h2> <p>이전 GAN들은 텍스쳐로 구분되는 class(하늘, 바다, 풍경 등)에는 좋은 성능을 보이지만, 복잡한 구조를 가진 class(강아지의 털은 잘 표현하지만 발은 잘 그리지 못함)에는 detail적인 부분에서 아쉬운 성능을 보였다. 이는 다른 이미지 영역의 dependency(종속성)를 modeling할 때 convolution에 의존하기 때문이다.</p> <blockquote> <p>Convolution 연산을 할 때, receptive field는 local에 국한되기 때문에(데이터를 감지하는 영역이 작기 때문에) long range dependencies를 처리하기 위해서는 convolution layer를 깊게 쌓아 각 layer를 통과할 때마다 receptive field를 점차 확장시켜 넓은 영역에 대한 정보를 감지해야한다.</p> <p><strong>즉, 가벼운 모델로는 처리가 불가능하며, Optimization 알고리즘이 dependency를 감지하는 parameter값을 찾는 게 힘들어지며, 생소한 데이터에 대해 실패할 확률이 크다.</strong></p> </blockquote> <p>이러한 문제를 논문에서는 <span style=" background-color: #F7DDBE"><strong>Self-Attention 기법을 활용하여 long range dependencies를 효과적으로 modeling하는 SAGAN을 제안</strong>한다. </span>어떤 위치에서의 반응을 모든 위치 feature의 가중치 합으로 계산하는데, weight나 attention vector의 계산은 매우 적은 비용으로 가능하다.</p> <p>또한, <strong><em>network conditioning</em>(조건)</strong>에 대한 insight를 적용시켰다. $D$에만 적용되었던 spectral normalization 기법을 $G$에도 적용시켜 성능을 향상시켰다.</p> <p><strong>Image-Net 데이터셋으로 실험한 결과, Inception score를 36.8에서 52.52로 높이고, Fréchet Inception Distance를 27.62에서 18.65로 줄여 새로운 SOTA를 달성하였다고 한다.</strong></p> <h2 id="️related-work--attention">✏️Related Work : Attention</h2> <p>원래라면 넘어갔을 Related work section이지만, <span style="color:red;">Attention Model</span>을 GAN에 적용시킨 첫 논문이므로, 짚고 넘어가자</p> <blockquote> <p><em>필자가 작년에 허술하게 <a href="https://hahngyutak.github.io/posts/Transformer/">Attention is all you need 논문 리뷰</a>도 있다..</em></p> </blockquote> <h3 id="attention"><strong>Attention</strong></h3> <p>Attention은 ‘문장’을 더 잘 이해하기 위해 고안되었다. 사람은 문장을 이해할 때, <strong>문장 중에서 중요한 단어들을 좀 더 강조하여 이해</strong>한다. 예를 들어,</p> <p>“3일 전에 바지와 셔츠를 샀는데, 바지가 작아”</p> <p>위 문장에서 문맥을 이해하기 위해 우리는 “바지”에 초점을 맞춘다. “셔츠”라는 단어가 들어가있지만, 문장에서 큰 의미를 차지하지는 않는다.</p> <p>이와 같은 방식으로 인공신경망이 input sequence data의 전체 또는 일부를 되짚어 살펴보면서 어떤 부분이 의사결정에 중요한지, 판단하고 중요한 부분에 “집중”(Attention) 하는 방식인 Attention 메커니즘을 도입하게 된다.</p> <p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*wa4zt-LcMWRIYLfiHfBKvA.png" alt="Attention" style="zoom:67%;"/></p> <p>Input data에 대해 Decoder가 결과를 내기 위해 각 timestep마다 다음을 반복한다.</p> <ol> <li>Encoder가 <strong>어느 부분에 집중할지를 판단하기 위해 각 단어의 attention score를 계산</strong>한다.</li> <li>이 attention score를 기반으로 가중치가 반영된 <strong>Attention output</strong>을 생성한다.</li> <li>2번의 결과와 Decoder의 hidden layer를 사용해 이번 step의 Decoder output을 도출한다.</li> <li>3번의 Decoder output은 다음 step의 Encoder의 Input이 되어 반복한다.</li> </ol> <p>Attention 메커니즘은 RNN처럼 순차 데이터(문장, 음성 등)를 처리하는 <strong>seq2seq 모델의 성능을 크게 향상시켰다.</strong></p> <h3 id="self-attention"><strong>Self-Attention</strong></h3> <p><span style=" background-color: #F7DDBE"><b>Self-Attention</b>은 문장 안의 단어들의 연관성을 알기 위해 스스로 Attention을 적용</span>하는 것이다. timestep이 필요한 Attention과 달리, Self-Attention은 단어 하나 당 모든 단어에 대해 Attention Score를 구하므로 timestep이 필요가 없다.</p> <p>RNN처럼 들어온 timestep 순서로 계산하거나, CNN의 Convolution처럼 input data의 local을 사용하는 것이 아닌 모든 input에 대해 한번에 계산하기 때문에 local dependency가 아닌 <strong>long range dependency modeling이 가능</strong>하다는 장점이 있다.</p> <blockquote> <p>즉, RNN처럼 방향이 정해진(timestep) Attention은 자신보다 나중에 들어갈 input 데이터들은 활용하지 못하지만, Self-Attention 기법을 사용하면 <strong>모든 input 데이터에 대해 Attention 연산이 이루어지기 때문에 모든 input 데이터 정보를 활용</strong>할 수 있다.</p> </blockquote> <p>Self-Attention은 input 데이터의 각 요소가 자기 자신을 포함한 다른 모든 요소와의 관계를 학습할 수 있게 한다. Self-Attention의 핵심 요소는 Query, Key, Value가 있다.</p> <ul> <li><strong>Query(질문)</strong>: 현재 처리하고 있는 문장 또는 단어의 표현이다. 질문을 던지는 주체로 생각할 수 있으며, 다른 단어들과의 관계를 알고 싶어하는 단어의 관점을 나타낸다.</li> <li><strong>Key(키)</strong>: 비교 대상이 되는 문장 또는 단어의 표현입니다. 각 Key는 정보의 저장소 역할을 하며, Query가 접근하려고 하는 대상이다. Query가 각 Key와 얼마나 관련이 있는지를 평가하여 해당 단어가 얼마나 주목해야 할 대상인지 결정한다.</li> <li><strong>Value(값)</strong>: Key에 대응하는 실제 값이다. Query와 Key의 관계가 얼마나 강한지에 따라, 각 Value는 Query에 의해 가중치가 부여된다. 이 과정을 통해 얻은 가중치가 높은 Value들이 결합되어, 최종적으로 Query에 대한 응답이 된다.</li> </ul> <p>이러한 모든 요소가 서로를 조사하고, 그 결과로 각 요소는 다른 모든 요소와의 관계에 따라 새로운 표현을 얻게 된다. 이 과정은 <strong>input 데이터 내의 숨겨진 패턴과 관계를 효과적으로 파악</strong>할 수 있다.</p> <p>이러한 Attention은 다음과 같이 구할 수 있다.</p> <p>[</p> <p>\text{Attention}(Q, K, V) = \text{softmax}\left (\frac{QK^T}{\sqrt{d_k}} \right)V</p> <p>]</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left (\frac{QK^T}{\sqrt{d_k}} \right)V\] <details> <summary>위 수식 자세히 파고들기</summary><br/> <ol> <li>Query와 Key를 내적하여 연관성을 나타내는 Attention Score를 계산</li> <li>$\sqrt{d_k}$로 나누는 scaling을 통해 모델 학습을 수월하게 함</li> <li>softmax 함수로 정규화</li> <li>위에서 계산된 score matrix와 Value matrix를 내적하여 Attention matrix 형성</li> </ol> 위 과정을 이미지 분야에 적용시켜보자.<br/> <img src="/assets/img/post/SAGAN/sa.png" alt="vision attention"/> <ul> <li>이미지에서 일부분을 나타내는 작은 패치(patch)들을 추출하여 다루기 쉬운 형태로 변환하기 위해 임베딩</li> <li>이미지의 각 패치를 Query, Key, Value로 사용하여 Self-Attention을 계산</li> <li>각 패치(Query)와 다른 모든 패치(Key)로 <b>Attention map</b>을 계산</li> <li>Attention map에 값(Value)를 가중 평균하여 이미지 내에서 각 패치가 서로 어떻게 관련되어 있는지를 모델링</li> </ul> <blockquote>이 Attention map을 시각화하면 아래과 같이 Query인 값과 Key 값들의 <b>연관된 정도에 따라 다른 밝기 수준</b>을 가지고 있어 어떤 픽셀과 관련이 있는지 알 수 있다.</blockquote> <img src="/assets/img/post/SAGAN/Figure1.png" alt="query시각화"/> </details> <p><br/></p> <p>SAGAN에서는 image generation task에 이러한 Self-Attention을 사용해 long range dependency 문제를 해결하고자 한다.</p> <h2 id="️self-attention-gans"><strong>✏️Self-Attention GANs</strong></h2> <p>대부분의 GAN은 주변 값들의 정보를 활용하는 Convolution 연산을 통해 구축되어있다. 이때문에, 이미지의 long-rande dependencies을 modeling할 때 비효율적이다.</p> <p>이번 논문에서는 Self-Attention을 적용하여 $G$와 $D$가 넓게 펼쳐진 영역 간의 관계를 효율적으로 modeling할 수 있도록 하는 SAGAN을 제안한다.</p> <p><img src="/assets/img/post/SAGAN/Figure2.png" alt="Figure2"/></p> <p>convolution layer를 통과한 feature map($x$)를 입력 $x \in \mathbb{R}^{C \times N}$의 image features는 Attention을 계산하기 위해 3개의 feature space인 <strong>Query $f(x)$, Key $g(x)$, Value $h(x)$</strong>로 1x1 convolution으로 계산하여 변환된다. $\rightarrow f(x) = \textit{W}_f x$, $g(x) = \textit{W}_g x$, $h(x) = \textit{W}_h x$</p> <p>아래 수식은 Query와 Key의 곱에 softmax 연산을 통해 Attention Map을 구하는 식이다.</p> \[\beta_{j,i} = \frac{e^{s_{ij}}}{\sum_{i=1}^N e^{s_{ij}}}, \;\text{where}\;\; s_{ij} = f(x_i)^Tg(x_j) \tag{1}\] <p>여기서 $\beta_{j,i}$는 $j^{\text{th}}$번째 영역을 처리할때 $i^{\text{th}}$영역이 얼마나 영향을 미치는지 결정하는 weight이다.</p> <p>channel 수를 $C$, 이전 hidden layer에서 넘어온 feature의 feature location 수를 $N$이라 할 때, Attentino layer의 output은 $o = (o_1, o_2, \cdots, o_j, \cdots, o_N)$이다. ($o$ : <strong>self-attention feature map</strong>)</p> \[{o_j} = v\left(\sum_{i=1}^N \beta_{j,i}h(x_i) \right), \; h(x_i) = W_hx_i,\; v(x_i) = W_vx_i \tag{2}\] <p>$W_g\in \mathbb{R}^{\overline{C} \times C}$, $W_f\in \mathbb{R}^{\overline{C} \times C}$, $W_h\in \mathbb{R}^{\overline{C} \times C}$, $W_v\in \mathbb{R}^{C \times \overline{C}}$ 모두 $1 \times 1$ convolution 연산을 거쳐 학습된 weight이다.</p> <p>ImageNet으로 몇번의 epoch를 train한 후, channel 수 $\overline{C}$를 $\frac{C}{k}$($k = 1, 2, 4, 8$)로 감소시켜봐도 성능 저하는 보이지 않아 메모리 효율성 때문에 $k = 8$로 지정하였다고 저자들을 말한다.</p> <p>게다가, Self-attention feature map에 scale parameter를 곱하고, input feature map을 더하여 최종 output을 만들었다.</p> \[y_i = \gamma o_i + {x_i} \tag{3}\] <p>여기서 $\gamma$는 처음에 0으로 초기화된 parameter이다. 논문에서는 이러한 $\gamma$를 곱해 처음에는 local 정보를 활용하다가, 점점 갈수록 local이 아닌 정보에 가중치를 부여하게끔 할 수 있음을 나타낸다.</p> <blockquote> <p>이 말은즉슨, 처음에는 쉬운 학습을 하고 나중에 복잡한 task를 학습하게 유도한 것이라고 저자는 말한다.</p> </blockquote> <p>제안된 self-attention 모듈은 $G$와 $D$ 모두 적용했으며, hinge loss를 GAN에 적용하였다.</p> <blockquote> <p><strong><a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a></strong>는 classification에서 사용하는 loss function으로, 주로 SVM에서 최대 margin 분류에 사용된다.</p> \[l(y) = \text{max}(0, 1 -t* y ),\; \text{where} \; t = \pm1\] <p>여기서 $t$는 정답 혹은 오답 class를 예측하는 중인지를 결정한다.</p> <p>$|y| &gt; 1$일 경우에서는 $y$가 margin 밖에 있는 상황이며 다음과 같은 경우가 생긴다.</p> <ul> <li>$t$와 $y$가 같은 부호이면 $l(y)=0$이 된다.</li> <li>즉, 정확하게 분류한다는 뜻이다.</li> <li>$t$와 $y$가 다른 부호이면 오답이며 $l(y)$는 linear해진다. (잘못 분류했다는 뜻)</li> </ul> <p>$|y| &lt; 1$이고 $t$와 $y$가 같은 부호이더라도 올바른 예측을 하고 있지만, 아직 확실하지 않다는 것을 의미한다.</p> <p><img src="https://www.researchgate.net/publication/332402217/figure/fig2/AS:882690739933184@1587461280015/Soft-margin-SVM-example-the-encircled-samples-are-correctly-classified-but-they-are-on.png" alt="svm" style="zoom:50%;"/></p> <p><strong>즉 분류가 잘 되는 margin 바깥 부분의 관측값이라면 loss가 0이 되도록 하고,분류가 잘 되지 않는 margin 내의 관측값이라면 loss가 커지게 유도하는 것이 hinge loss의 목적이다.</strong></p> </blockquote> <p>SAGAN에서 사용하는 $G$와 $D$의 loss 식은 다음과 같다.</p> \[L_D = -\mathbb{E}_{(x,y) \sim p_{\text{data}}}[\text{min}(0, -1 + D(x,y))] - \mathbb{E}_{z \sim p_z,y \sim p_{\text{data}}}[\text{min}(0, -1-D(G(z), y))]\] \[L_G = -\mathbb{E}_{z\sim p_z, y \sim p_{\text{data}}}D(G(z),y) \tag{4}\] <p>$D$는 최적의 hyperplane을 찾기 위해 학습한다.</p> <ol> <li><strong>$D(x, y)$는 실제 데이터</strong>에 대한 분류값이며, 이 값이 1(real에 대한 분류값)보다 작으면 $D(x, y)\geq1$이 되도록 유도한다.</li> <li><strong>$D(G(z), y)$는 $G$가 생성한 이미지</strong>에 대한 분류값이며, 이 값이 $-1$보다 큰 경우 $D(G(z), y) \leq -1$ 가 되도록 유도한다.</li> </ol> <p>$G$는 $D(G(z), y)$가 $D$의 hyperplane에 가까워지도록 학습하는 것이 목표이다.</p> <h2 id="️techniques-to-stabilize-the-training-of-gans">✏️<strong>Techniques to Stabilize the Training of GANs</strong></h2> <p>GAN 학습을 안정화(Stabilize)하기 위해 적용한 기법 2가지를 알아보자.</p> <ol> <li>$D$에 적용된 spectral normalization을 $G$에도 적용한다.</li> <li>$D$가 학습이 느리다는 문제를 해결하기 위해 two-timescale update rule을 적용한다.</li> </ol> <h3 id="1-spectral-normalization"><strong>1. Spectral normalization</strong></h3> <p>&lt;Spectral normalization for generative adversarial networks, 2018&gt;에서 spectral normalization을 $D$에 적용하였을 때, GAN의 학습이 안정화되며 계산 비용 역시 상대적으로 적다는 것이다.</p> <blockquote> <p>Spectral normalization은 $D$의 <strong>Lipschitz constant를 컨트롤</strong>하기위해서 사용된다. Lipschitz norm은 함수의 gradient를 제한하는 것인데, 이때 tuning해야하는 hyper parameter인 Lipschitz constant를 신경써서 조절할 필요가 없어진다.</p> <p><em>자세한 내용은 추후 SNGAN을 리뷰하면서 설명하겠다…</em></p> </blockquote> <p>저자들은 $G$도 spectral normalization의 이점을 활용할 수 있다고 말한다. parameter의 비대화를 방지하고 비정상적인 gradient를 피할 수 있다. 실험을 통해 $G$, $D$ 둘 다 computing cost를 획기적으로 줄이며 안정적인 train을 보여준다고 한다.</p> <h3 id="2-imbalanced-learning-rate-for-g-and-d-updates"><strong>2. Imbalanced learning rate for $G$ and $D$ updates</strong></h3> <p>일반적으로 GAN에서 $G$와 $D$의 train balance를 맞추기 위해 $G$ 1 업데이트 당 $D$ $k$ 업데이트 단계를 거친다.</p> <p>하지만 &lt;GANs trained by a two time-scale update rule converge to a local nash equilibrium&gt;에서 제안한 <strong>Two Time-Scale Update Rule(TTUR)은 $D$와 $G$에 다른 learning rate를 적용하는 방법</strong>이다. 이 방법을 사용하여 효율적으로 $G$와 $D$의 balance를 맞출 수 있었으며, 같은 train 시간으로 더 나은 결과를 얻었다고 말한다.</p> <h2 id="️experiments">✏️<strong>Experiments</strong></h2> <p>논문에서는 train할 때 사용한 detail은 다음과 같다.</p> <ul> <li>128*128 이미지 생성</li> <li>spectral normalization 적용</li> <li>$G$는 conditional batch normalization을, $D$에는 projection을 적용</li> <li>$\beta_1 = 0$, $\beta_2 = 0.9$인 Adam optimizer 사용</li> <li>TTUR : [$D$ - 0.0004, $G$ - 0.0001]</li> </ul> <h3 id="1-evaluating-the-proposed-stabilization-techniques"><strong>1. Evaluating the proposed stabilization techniques</strong></h3> <p>제안된 stabilization(안정화) 기법에 대한 평가를 진행하는 section이다.</p> <p>spectral normalization은 <strong>SN</strong>, learning rate의 balance를 맞추는 <strong>TTUR</strong>이라 하자.</p> <p><img src="/assets/img/post/SAGAN/Figure3.png" alt="Figure3"/></p> <p>위 사진은 SN과 TTUR을 사용한 모델의 학습 그래프이다.</p> <p>좌측 모델은 SN을 $D$에만 적용한 baseline 모델(<code class="language-plaintext highlighter-rouge">SN on D</code>)로, 학습이 매우 불안정함을 알 수 있다.</p> <p>중간 모델은 SN을 $G$와 $D$ 모두 적용한 모델(<code class="language-plaintext highlighter-rouge">SN on G/D</code>)로, 학습이 안정되었지만 26만 iter에서 이미지 품질이 떨어지기 시작하였다.</p> <blockquote> <p>아래 사진에서<code class="language-plaintext highlighter-rouge">SN on G/D</code>의 160k의 FID 값은 33.39였지만 <code class="language-plaintext highlighter-rouge">SN on G/D</code>의 260k의 FID값은 72.41로 오히려 결과 이미지의 품질이 하락</p> </blockquote> <p>우측 모델은 SN과 TTUR를 적용시켜 중간모델에서 $G$와 $D$의 학습률을 조정한 모델이며 논문에서 제안한 모델(<code class="language-plaintext highlighter-rouge">SN on G/D + TTUR</code>)이다. 이 모델의 경우 학습이 안정되고 이미지 품질 역시 FID 또는 Inception score의 하락은 보이지 않았다.</p> <p><img src="/assets/img/post/SAGAN/Figure4.png" alt="Figure4"/></p> <h3 id="2-self-attention-mechanism"><strong>2. Self-attention mechanism</strong></h3> <p>여러 feature map에 Self-attention을 적용시키면서 성능의 변화를 살펴보자.</p> <p><img src="/assets/img/post/SAGAN/table1.png" alt="table1"/></p> <p>위 표를 보면 high-level feature map($feat_{32}$, $feat_{64}$)에 Self-Attention을 적용시킨 모델이 low-level feature map($feat_{8}$, $feat_{16}$)에 적용시킨 모델보다 성능이 좋은 것을 알 수 있다.</p> <blockquote> <p>8*8 feature map과 같이 작은 크기에 적용시킨 Self-attention은 Convolution과 다를바 없지만, <strong>Self-attention을 적용시킨 feature map 크기가 커지면 활용할 수 있는 정보가 많아지며 long-range dependency modeling이 수월</strong>해진다.</p> </blockquote> <p>또한 Self-attention block 대신 Residual block을 사용할 때 차이가 큰 것으로 보아 SAGAN 사용으로 인한 성능 향상이 단순히 모델 깊이와 용량의 증가로 인한 것이 아님을 보여준다.</p> <p><img src="/assets/img/post/SAGAN/Figure1.png" alt="Figure1"/></p> <p>위 이미지는 Self-attention이 long range dependency modeling 했을 때 Query 위치에 대한 Attention map이다.</p> <p><img src="/assets/img/post/SAGAN/Figure5.png" alt="Figure5"/></p> <p>위 이미지는 마지막에 학습된 $G$의 Attention map을 시각화한 것이다. Query 위치를 보면 네트워크가 공간적인 인접성뿐만 아니라 <strong>색상, 질감 등의 유사성에 따라 Attention을 할당</strong>하는 것을 학습함을 알 수 있다.</p> <blockquote> <p>오른쪽 상단 강아지 이미지에서 파란색 Query point를 보면 관절 구조를 정확하게 파악하여 뚜렷한 다리를 그릴 수 있음을 볼 수 있다.</p> </blockquote> <h3 id="3-comparison-with-the-state-of-the-artsota"><strong>3. Comparison with the state-of-the-art(SOTA)</strong></h3> <p>기존에 ImageNet에서 conditional 이미지 생성 SOTA를 달성한 <a href="https://arxiv.org/abs/1802.05637">cGANs with Projection Discriminator</a>과 <a href="https://arxiv.org/abs/1610.09585">ACGAN</a>과 비교를 진행하였다.</p> <p><img src="/assets/img/post/SAGAN/table2.png" alt="table2"/></p> <p>위 표를 보면 Inception Score, Intra FID, FID라는 <strong>3가지 metric에서 SOTA를 달성</strong>함을 알 수 있다.</p> <p><img src="/assets/img/post/SAGAN/Figure6.png" alt="Figure6"/></p> <p>위 사진은 ImageNet의 몇가지 class에 대한 SAGAN의 생성 결과이며, 괄호() 안에 있는 숫자는 각 class에 대한 SAGAN과 CGANS with Projection Discriminator의 intra FID이다.</p> <p>금붕어, 강아지, 새 등과 같이 기하학&amp;구조적인 패턴을 가진 class에서는 SAGAN이 우수한 성능을 보이지만, 질감과 같은 특성으로 구별되는 돌담, 산호 등은 SAGAN의 성능이 더 낮은 것을 알 수 있다.</p> <p><span style=" background-color: #F7DDBE"><b>이는, Self-Attention이 기하학&amp;구조적인 패턴이 생기는 long range dependency을 modeling하는 점에서는 convolution 연산보다 좋은 성능을 보이지만 단순한 텍스쳐를 modeling할 때에는 local convolution과 유사한 역할을 함을 시사한다.</b></span></p> <h2 id="️conclusion-and-code"><strong>✏️Conclusion and Code</strong></h2> <p>Self-Attention GAN은 다음과 같은 변경 사항을 통해 SOTA를 달성하였다.</p> <ul> <li><strong>Self-Attention module</strong>을 통한 long range dependencies modeling이 수월</li> <li>$D$뿐만 아니라 $G$에도 적용된 <strong>spectral normalization</strong>을 통한 학습 안정화</li> <li><strong>Two Time-Scale Update Rule(TTUR)</strong>를 활용하여 $D$와 $G$의 학습 불균형 해결</li> </ul> <p><a href="https://colab.research.google.com/drive/1Oq803lWn8Qu4vGWtLUfzvPVe1tTad-7T?usp=sharing">SAGAN 구현 실습 - Fashion MNIST</a></p> <h2 id="️reference">✏️Reference</h2> <p><a href="https://solee328.github.io/gan/2023/09/27/sagan_paper.html">SAGAN - 논문 리뷰</a></p> <p><a href="https://codingopera.tistory.com/43">4-1. Transformer(Self Attention)</a></p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="GAN"/><category term="AI"/><category term="generative"/><category term="Attention"/><summary type="html"><![CDATA[Self-Attention GAN(SAGAN) 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] Score-based Generative model</title><link href="https://hahngt.github.io/blog/2024/Score_based_model/" rel="alternate" type="text/html" title="[Paper Review] Score-based Generative model"/><published>2024-03-30T10:17:11+00:00</published><updated>2024-03-30T10:17:11+00:00</updated><id>https://hahngt.github.io/blog/2024/Score_based_model</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/Score_based_model/"><![CDATA[<h1 id="score-based-model-논문-리뷰"><strong>Score-based model 논문 리뷰</strong></h1> <p><a href="https://arxiv.org/abs/1907.05600">arxiv : Generative Modeling by Estimating Gradients of the Data Distribution</a></p> <p><a href="https://github.com/ermongroup/ncsn">github</a></p> <p>데이터의 score function, 즉 데이터 분포의 기울기를 직접 모델링하는 방식을 도입한 첫번째 논문이다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Generative AI는 최근 두 가지 방식이 주를 이루고 있지만, 각각의 한계점이 명확하게 존재하였다.</p> <ul> <li>Likelihood-based Model <ul> <li>log-likelihood를 Object function으로 사용</li> <li>정규화된 확률 모델을 만들기 위해 여러 방법을 사용해야한다. <ol> <li>특수한 구조를 사용해 데이터의 확률 분포를 직접 modeling : Autogressive models, flow models</li> <li>Surrogate loss (대리 손실 함수)를 사용해 원래의 목표 함수를 대신해서 최적화 : VAE</li> <li>Contrastive divergence를 사용하여 모델의 에너지 함수를 학습 : Energy-based</li> </ol> </li> </ul> </li> <li>Generative Adversarial Networks <ul> <li>경쟁적인 train 방식을 사용하여 모델과 데이터 간의 분포 차이를 minimize <ul> <li>분포 차이를 $f$-divergences라 하며, 그 종류로는 KL, JS 등이 있다.</li> </ul> </li> <li>위 Likelihood-based 모델의 한계를 보완하지만, Train 과정이 불안정</li> <li>GAN의 objective는 다른 GAN 모델을 평가할 수 없다.</li> </ul> </li> </ul> <p>논문에서는 log data density인 <strong>Score</strong>(Stein)을 추정하고 sampling하는 Generative 모델의 새로운 method를 탐구한다. 데이터로부터 log density 함수의 gradient를 학습하기 위해 <strong>score mathing</strong> 모델을 사용한다. 그런 다음, <strong>Langevin dynamics(랑주뱅 역학)</strong>을 사용하여 sample을 생성하며, 이는 초기 sample을 Score에 따라 점차 density가 높은 역영역으로 Transition하는 방식으로 작동한다.</p> <blockquote> <p><strong>Score(Stein)</strong></p> <p>이전 생성모델들은 log likelihood의 gradient인 $\nabla_\theta \log p_\theta(x)$을 통해 $\theta$를 업데이트하는 방식이었다.</p> <p>하지만, Score-Based 모델은 학습 데이터 $x$ 자체의 gradient을 구하는 <strong>Stein Score Function</strong>을 바탕으로 학습을 진행한다.</p> \[s_\theta(x) = \nabla_x \log p_\theta(x)\] <p>즉, Score function이란 데이터 $x$의 분포를 기준으로 likelihood가 높은 방향으로 gradient를 유도하는 알고리즘이다.</p> <p><img src="/assets/img/post/Score-based Generative Model/Gaussian plot.png" alt="Gaussian plot" style="zoom: 25%;"/></p> <p>여기서 <strong>score</strong>는 각 지점에서 어떻게 움직여야 likelihood가 높은 지점으로 갈 수 있는지를 알려주는 역할을 한다. 따라서 위 분포에서는 <strong>화살표(score)</strong>를 따라가면 각각의 Gaussian의 mean vector에 도착하게 된다.</p> <p>Score-based model의 핵심아이디어는 확률분포함수를 사용하는 대신 이 score를 활용하는 것이다.</p> </blockquote> <blockquote> <p><strong>Langevin dynamics</strong></p> <p><strong>랑주뱅 동역학</strong>은 분자 시스템의 움직임의 수학적 모델링이다. 확률적 미분 방정식을 사용해 자유도를 생략하면서 단순화된 모델을 사용한다.</p> </blockquote> <h3 id="problem"><strong>Problem</strong></h3> <p>하지만, 이러한 접근 방식에는 다음과 같이 해결해야하는 과제 2가지가 있다.</p> <ol> <li>저차원 maniford에서의 데이터셋 분포는 score가 주변 space에서 정의되지 않으며 score matching이 일관된 추정치를 제공하지 않는다.</li> <li>low-density 영역의 데이터가 부족할 경우, score matching 정확도가 떨어지고 Langevin dynamics sampling의 mixing 속도가 감소한다.</li> </ol> <blockquote> <p>Langevin dynamics는 밀도가 낮은 영역에서 자주 초기화됨</p> </blockquote> <p>이 2가지 문제는 초기 sample을 high-density 영역으로 <strong>transition</strong>할 때 발생한다.</p> <p><img src="/assets/img/post/Score-based Generative Model/data scores.png" alt="data scores"/></p> <p>위 사진처럼 좌측 하단 distribution에서 우측 상단 distribution으로 transition하며 추가적인 특징을 가진 이미지를 생성하려고 한다. 중간 대각선 영역은 low density 영역이며 score가 존재하지 않는다. 때문에 <strong>이 영역을 지나야 하지만, score가 존재하지 않기</strong> 때문에 어느 방향으로 가야할 지를 모르는 문제가 생긴다. 이를 모델이 <strong>저차원 manifold에서 “collapse”된다</strong>고 표현한다.</p> <h3 id="solution">Solution</h3> <p>위 문제를 해결하기 위해 <strong>데이터에 random Gaussian noise를 추가하여 collapse를 방지</strong>한다. Noise level이 클수록 데이터 분포의 low density 영역에서 sample을 생성하여 score matching을 개선한다.</p> <p>논문에서는 <strong>Annealed version of Langevin dynamics</strong>을 제안하는데, 이는 처음에는 높은 level의 nosie를 추가하고, Noise level을 점차 낮추며 추가하는 방법이다.</p> <h2 id="score-based-generative-modeling"><strong>Score-based Generative modeling</strong></h2> <p>데이터셋을 $p_{data}(x)$에서 sampling된 i.i.d.한 $\lbrace x \in \mathbb{R}^D \rbrace^{N}_{i=1}$으로 가정하자.</p> <blockquote> <p>i.i.d.란 independent and identically distribution의 약어로 독립 항등분포, 즉 독립적이고 같은 확률 분포를 가지는 것을 말한다.</p> </blockquote> <p>PDF인 $p(x)$의 score를 $\nabla_x \log p(x)$로 정의되며 score network $s_\theta : \mathbb{R}^D \rightarrow \mathbb{R}^D$는 $\theta$로 parameterize된 Neural network이다. 이 신경망은 $p_{data}(x)$의 score를 근사화하도록 훈련된다.</p> <p>이 Score-based generative modeling framwork에는 <strong>score matching</strong>과 <strong>Langevin dynamics</strong>라는 두가지 요소가 있다.</p> <h3 id="score-matching-for-score-estimation"><strong>Score matching for score estimation</strong></h3> <p>score matching을 사용하면, $p_{data}(x)$를 추정하는 모델 없이 score network $s_\theta$를 train하여 $\nabla_x \log p_{data}(x)$를 추정할 수 있다.</p> <blockquote> <p>즉, $\theta$라는 파라미터를 지닌 모델을 활용하여 $\nabla_x \log p_{\theta}(x)$를 $\nabla_x \log p_{data}(x)$가 되도록 train하는 것이 Score-based의 아이디어이다.</p> </blockquote> <p>즉 $s_\theta (x) = \nabla_x \log p_{\theta}(x)$가 $\nabla_x \log p_{data}(x)$와 같아지도록 하기 위해 Euclidean distance로 정의한 Objective은 다음을 minimize한다.</p> \[\frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right]\] <p>위 식을 <strong>Fisher Divergence</strong>라고 한다.</p> <p>위 식은 부분적분을 통해 다음과 같이 유도할 수 있다. (우리가 모르는 $p_{\text{data}}(x)를 없애기 위해) (<strong>이과정이 score matching이다.</strong>)</p> \[\mathbb{E}_{p_{data}} \left [\text{tr}(\nabla_xs_\theta(x)) + \frac{1}{2}\left|\left| s_\theta(x) \right|\right|^2_2 \right] \tag{1}\] <details> <summary><b>Eq. 1</b>유도 과정</summary> $$ \begin{align} \frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right] &amp;= \frac{1}{2} \int p(x)(\nabla_x \log p(x) - s_\theta(x))^2 \text{d}x \nonumber\\ &amp;= \frac{1}{2} \int p(x)(\nabla_x \log p(x))^2\text{d}x + \frac{1}{2} \int p(x)s_\theta(x)^2 \text{d}x - \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x \nonumber \end{align} $$ 여기서 마지막 항 $- \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x$는 다음과 같이 유도할 수 있다. $$ \begin{align} - \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x &amp;= - \int p(x) \frac{\nabla_xp(x)}{p(x)}s_\theta(x)\text{d}x \nonumber\\ &amp;= -\int \nabla_x p(x)s_\theta(x) \text{d}x \nonumber\\ &amp;= -p(x)s_\theta(x) |^\infty_{x=-\infty} + \int p(x)\nabla_xs_\theta(x)\text{d}x \nonumber\\ &amp;= \int p(x)\nabla_x s_\theta(x)\text{d}x \nonumber \end{align} $$ 3번째 줄에서 boundary term은 $\pm\infty$에 의해 0이 된다. 즉 마지막 항을 $\int p(x)\nabla_x s_\theta(x)\text{d}x$으로 유도함으로써 $\frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right]$는 다음과 같이 유도할 수 있다. $$ = \underbrace{\frac{1}{2} \int p(x)(\nabla_x \log p(x))^2\text{d}x }_{1}+ \underbrace{\frac{1}{2} \int p(x)s_\theta(x)^2 \text{d}x}_2 + \underbrace{\int p(x)\nabla_x s\_\theta(x)\text{d}x}_3 $$ 여기서 1번 항을 $\theta$에 대해 independent하기 때문에 상수로 바꾸고 2, 3번 항을 각각 $\mathbb{E}$로 묶으면 다음과 같은 식이 된다. $$ C + \frac{1}{2}\mathbb{E}_{p(x)} [s_\theta(x)^2] + \mathbb{E}_{p(x)}[\nabla_xs_\theta(x)] $$ </details> <p><br/></p> <p>$\nabla_x s_\theta(x)$는 $s_\theta(x)$의 Jacobian matrix를 나타낸다.</p> <blockquote> <p>Jacobian matrix란, 다변수 함수의 도함수를 나타내는 행렬이다. 다변수 함수의 입력 변수가 여러 개일 때, 각 입력 변수에 대한 편미분 값을 행렬로 정리한 것을 말한다.</p> </blockquote> <p>이 Jacobian matrix는 Deep network와 고차원 데이터에서 계산이 어렵기 때문에 다음과 같은 Score matching method 2가지를 설명한다.</p> <h4 id="denoising-score-matching"><strong>Denoising score matching</strong></h4> <p>Denoising score matching은 $\text{tr}(\nabla_x s_\theta(x))$의 계산을 우회하는 변형된 score matching이다. 데이터 $x$에 nosie distribution $q_\sigma (\widetilde{x} | x)$를 추가하고, 이를 제거하는 방식으로 진행한다.</p> <p>noise가 추가된 데이터 distribution $q_\sigma (\widetilde{x}) \triangleq \int q_\sigma (\widetilde{x} | x)p_{data}dx$의 score를 추정하며, Objective는 다음과 같다.</p> \[\frac{1}{2} \mathbb{E}_{q_\sigma(\widetilde{x}|x)p_{data}(x)}\left [\left|\left| s_\theta(\widetilde{x}) - \nabla_{\widetilde{x}}\log q_\sigma(\widetilde{x}|x) \right|\right|^2_2 \right] \tag{2}\] <p><strong>최적의 score 모델</strong>은 위 식 <strong>Eq. (2)</strong>를 minimize하여 $s_{\theta^*}(x) = \nabla_x \log q_\sigma (x)$를 만족하는 $s_{\theta^*}(x)$이다.</p> <h4 id="sliced-score-matching"><strong>Sliced score matching</strong></h4> <p>Sliced score matching은 random projection을 사용하여 $\text{tr}(\nabla_xs_\theta(x))$의 근사치를 구한다.</p> <p>Data score와 모델 분포의 vector field가 scalar 필드가 되도록 score를 random한 방향으로 projection한 다음 scalar 필드를 비교하여 모델 분포가 데이터 분포에서 얼마나 떨어져 있는지 확인한다.</p> <p>radom projection 방향을 $\textbf{v}$로, $\textbf{v}$의 분포를 $p_{\textbf{v}}$로 할때, random projection에 대한 <strong>Fisher Divergence</strong>는 다음과 같다.</p> \[\frac{1}{2} \mathbb{E}_{p_{\text{data}}}[(\textbf{v}^\top\nabla_x \log p_{\text{data}}(x) - \textbf{v}_\top \nabla_x \log p_\theta(x))^2]\] <blockquote> <p>$s_\theta(x) = \nabla_x \log p_\theta(x)$이므로 아래 식과 동일</p> \[\frac{1}{2} \mathbb{E}_{p_{\text{data}}}[(\textbf{v}^\top\nabla_x \log p_{\text{data}}(x) - \textbf{v}_\top s_\theta(x))^2]\] </blockquote> <p>위 식 역시 <strong>Eq. 1</strong>을 유도한 trick을 사용하여 다음과 같이 나타낼 수 있다.</p> \[\mathbb{E}_{p_\textbf{v}}\mathbb{E}_{p_\text{data}} \left[\textbf{v}^\top \nabla_xs_\theta(x)\textbf{v} + \frac{1}{2}\left|\left| s_\theta(x) \right| \right| ^2_2 \right] \tag{3}\] <h3 id="sampling-with-langevin-dynamics"><strong>Sampling with Langevin dynamics</strong></h3> <p>Langevin dynamics는 score function $\nabla_x \log p(x)$를 사용하여 sampling한다.</p> <p>$\textbf{z}_t \sim \mathcal{N}(0, \mathit{I})$일때 다음과 같다.</p> \[\widetilde{x}_t = \widetilde{x}_{t-1} + \frac{\epsilon}{2} \nabla_x \log p(\widetilde{x}_{t-1}) + \sqrt{\epsilon}\;\textbf{z}_t \tag{4}\] <p>이 sampling 과정에서 $\nabla_x \log p(x)$가 필요하기 때문에, $s_\theta(x) \approx \nabla_x \log p(x)$가 되도록 train 한 다음, 이 $s_\theta$를 사용하여 sampling을 진행한다.</p> <h2 id="challenges-of-score-based-generative-modeling"><strong>Challenges of score-based generative modeling</strong></h2> <p>이 section에서는 Score-based 방식의 2가지 장애물을 다룬다.</p> <h3 id="1-the-manifold-hypothesis"><strong>1. The manifold hypothesis</strong></h3> <p>위에서 언급했듯이, 데이터가 저차원 manifold에 있는 경우, score가 well-define되지 않고, model이 collapse될 수 있다.</p> <p><img src="/assets/img/post/Score-based Generative Model/figure1.png" alt="figure1" style="zoom:50%;"/></p> <p><strong>Figure 1</strong>을 보자.</p> <p>왼쪽 그래프는 <strong>원본 CIFAR-10</strong>로 train한 Sliced score matching의 loss로 불규칙적인 변화를 보인다.</p> <p>이를 방지하기 위해 Gaussian noise를 추가하는데, 오른쪽 그래프를 보면 loss값들이 수렴하는 것을 볼 수 있다.</p> <h3 id="2-low-data-density-regions"><strong>2. Low data density regions</strong></h3> <p>low density 영역의 데이터 부족은 score matching을 통한 score estimate와 Langevin dynamics을 이용한 MCMC samping에 어려움을 야기한다.</p> <h4 id="1-inaccurate-score-estimation-with-score-matching"><strong>1. Inaccurate score estimation with score matching</strong></h4> <p>low density 영역에서는 <strong>Eq. 2</strong>를 minimize하는 score matching을 통해 score function이 잘 추정되지 않는다.</p> <p><img src="/assets/img/post/Score-based Generative Model/figure2.png" alt="figure2" style="zoom:67%;"/></p> <p>위 사진에서 실제 데이터 score (왼쪽)와 추정된 score (오른쪽)의 중앙을 보면 score가 제대로 구해지지 않았다는 것을 볼 수 있다. 해당 영역에서의 score estimation은 매우 부정확해지며, Score estimation이 부정확하면 결국 Langevin dynamics을 통한 sampling 과정도 부정확해지게 될 것이다.</p> <h4 id="2-slow-mixing-of-langevin-dynamics"><strong>2. Slow mixing of Langevin dynamics</strong></h4> <p>다른 두 데이터 분포를 mix할 때 그 비율을 알 수 없다는 문제도 발생한다.</p> <p>$p_{\text{data}}$가 다음과 같이 $p_1$과 $p_2$ 분포의 mixture로 구성된 데이터 분포라고 가정하자.</p> \[p_{\text{data}}(x) = \pi p_1(x) + (1-\pi)p_2(x)\] <p>$\nabla_x \log p_{\text{data}}(x)$를 구하기 위해서 각 항에 $\log$를 씌우면, 각각 $\log \pi$와 $\log (1-\pi)$는 상수가 되어버리므로, 결국 다음과 같이 된다.</p> \[\nabla_x \log p_{\text{data}}(x) = \nabla_x \log p_1(x) + \nabla_x \log p_2(x)\] <p>즉, mixture 비율(즉 가중치)인 $\pi$에 상관없이 score가 추정된다.</p> <p><img src="/assets/img/post/Score-based Generative Model/figure3.png" alt="figure3" style="zoom:67%;"/></p> <p>위 그림에서 (a)는 원래 데이터 분포이며, 두 분포가 다른 비율로 mixture된 것을 알 수 있다.</p> <p>(b)는 Langevin dynamics으로 sampling된 결과이며, 두 분포가 균일하게 samping된 것을 볼 수 있다.</p> <p>(c)는 annealed된 Langevin dynamics으로 sampling된 결과이며, mixture 비율을 반영한 것이다.</p> <h2 id="noise-conditional-score-networks--learning-and-inference"><strong>Noise Conditional Score Networks : learning and inference</strong></h2> <p>논문에서는 Gaussian Noise를 추가한 경우, Score-based 모델링의 성능이 높아진다는 것을 발견하였다. 그 이유는 크게 다음과 같은 2가지가 있다.</p> <ol> <li>Gaussian noise의 support가 하나의 whole space이기 때문에 저차원 maniford에 제한받지 않게 되어 score matching이 수월하게 define된다.</li> <li>Gaussian nosie를 추가함으로써 low density 영역을 채워주기 때문에 score estimation을 개선할 수 있다.</li> </ol> <h3 id="1-noise-conditional-score-networks"><strong>1. Noise Conditional Score Networks</strong></h3> <p>$\lbrace\sigma_i\rbrace^L_{i=1}$를 $\frac{\sigma_1}{\sigma_2} = \cdots = \frac{\sigma_{L-1}}{\sigma_L} &gt; 1$를 만족하는 수열이라고 하자. 이 각각의 $\sigma$가 바로 level에 따른 nosie가 된다.</p> <blockquote> <p>$\sigma_1$의 경우 매우 큰 nosie이며 $\sigma_L$에 가까워질 수록 데이터에 영향을 거의 미치지 않는 작은 수준의 nosie이다.</p> </blockquote> \[q_\sigma(x) \triangleq \int p_{data}(\textbf{t})\mathcal{N}(x | t, \sigma^2\textit{I})d\textbf{t}\] <p>$q_\sigma(x)$를 위와 같이 <strong>timestep $\textbf{t}$에 따라 nosie가 부여된 데이터의 분포</strong>라고 정의하면, 이 데이터 분포를 이용하여 score를 추정하는 network는 다음과 같다.</p> \[\forall \sigma \in \lbrace\sigma_i\rbrace^L_{i=1} : s_\theta (x, \sigma) \approx \nabla_x \log q_\sigma(x)\] <p>이때 $x \in \mathbb{R}^D$, $s_\theta (x, \sigma) \in \mathbb{R}^D$이다.</p> <p>위와 같은 $s_\theta (x, \sigma)$를 <strong><em>Noise Conditional Score Network(NCSN)</em></strong>이라 부른다.</p> <p>NCSN에서 사용한 아키텍쳐의 특징으로는 <strong>U-Net</strong>과 <strong>Instance Normalization</strong>이 있다.</p> <h3 id="2-learning-ncsns-via-score-matching"><strong>2. Learning NCSNs via score matching</strong></h3> <p>위 언급한 두 score matching 기법 모드 NCSN를 train할 수 있지만, noise있는 데이터를 다루고 속도가 약간 더 빠른 <strong>Denoising score matching을 사용</strong>하였다고 한다.</p> <p>Noise 분포 $q_\sigma(\widetilde{x} \;| \;x) = \mathcal{N}(\widetilde{x}\; | x, \sigma^2 \textit{I})$ 를 사용하면 score function은 다음과 같다.</p> \[\nabla_{\widetilde{x}} \log q_\sigma(\widetilde{x} \;| \;x) = -\frac{\widetilde{x}-x}{\sigma^2}\] <details> <summary> Why?? </summary> <blockquote> Gaussian 분포 $p(x)$는 $p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$로 정의되며, 이에 대한 score function은 위 $p(x)$에 $\log$를 씌우고 $x$에 대해 편미분을 진행한 $\nabla_x \log p(x)$이며 다음과 같다. $$ \begin{align} \nabla_x \log p(x) &amp;= \nabla_x \left (\log\frac{1}{\sqrt{2\pi\sigma^2}} + -\frac{(x-\mu)^2}{2\sigma^2} \log e \right) \nonumber \\ &amp;= -\frac{x-\mu}{\sigma^2} \nonumber \end{align} $$ 여기에 우리는 Noise가 추가된 조건부 Gaussian 분포 $p(\widetilde{x} | x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\widetilde{x}-x)^2}{2\sigma^2}}$를 정의한다. 이렇게 정의된 조건부 Gaussian 분포의 평균은 $x$가 되는데, 그 이유는 분포가 주어진 조건부 $x$를 중심으로 sampling이 이루어지기 때문이다. 즉 $p(\widetilde{x} | x) = \mathcal{N}(\widetilde{x} | x, \sigma^2 \textit{I})$로 정의할 수 있으며, 위와 같이 처리하면 $ -\frac{\widetilde{x}-x}{\sigma^2}$를 얻을 수 있다. </blockquote> </details> <p><br/></p> <p>noise인 $\sigma$가 주어졌을 때, denoising score matching의 Objective(<a href="#denoising-score-matching">Eq. 2</a>)는 다음과 같다.</p> \[\mathscr{l}(\theta; \sigma) \triangleq \frac{1}{2} \mathbb{E}_{p_{\text{data}}(x)} \mathbb{E}_{\widetilde{x} \sim \mathcal{N}(x, \sigma^2, \textit{I})} \left[\left|\left|s_\theta(\widetilde{x}, \sigma) + \frac{\widetilde{x} - x}{\sigma^2} \right|\right|^2_2 \right] \tag{5}\] <p>여기에 $\sigma \in \lbrace\sigma_i\rbrace^L_{i=1}$을 결합하여 timestep에 대해 정의하면 <strong>Unified objective</strong>를 얻을 수 있다.</p> \[\mathcal{L}(\theta ; \lbrace\sigma_i\rbrace^L_{i=1}) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \mathscr{l}(\theta, \sigma_i) \tag{6}\] <p>여기서 $\lambda(\sigma)$는 $\sigma$에 의한 계수 함수(coefficient function)이다.</p> <p>이상적으로 모든 $i$에 대해 $\lambda(\sigma_i) \mathscr{l}(\theta, \sigma_i)$가 동일하기 위해 경험적으로 $\lambda(\sigma) = \sigma^2$을 얻어낼 수 있었다고 한다.</p> <h3 id="3-ncsn-inference-via-annealed-langevin-dynamics"><strong>3. NCSN inference via annealed Langevin dynamics</strong></h3> <p>NCSN $s_\theta(x, \sigma)$를 train 한 후, 이번 section에서는 sampling 과정인 <strong>Annealed Langevin dynamics</strong>인 <strong>Algorithm 1</strong>을 제안한다.</p> <p><img src="/assets/img/post/Score-based Generative Model/Algorithm1.png" alt="Algorithm1" style="zoom:67%;"/></p> <p>고정된 분포 $x_0$에서 시작하여 $q_{\sigma_{i}}(x)$와 step size $\alpha_i$를 통해 $q_{\sigma_{i+1}}(x)$를 만든다. step size $a_i$는 위 3번째 줄 식을 통해 점차 감소시키며, 마지막에 $q_{\sigma_{L}}(x)$로부터 만들어진 sample은 $p_{\text{data}}(x)$와 비슷해진다.</p> <blockquote> <p>이때 noise level은 거의 없는 수준이다. $\rightarrow \sigma_L \approx 0$</p> </blockquote> <p><strong>Gaussian Noise가 첨가된 분포인 $\lbrace q_{\sigma_i} \rbrace ^L_{i=1}$은</strong> whole space에 걸쳐 있기 때문에 <strong>density가 높다</strong>고 할 수 있다. <strong>score estimation과 Langevin dynamics는 high-density 영역에서 매우 잘 수행</strong>되므로 <span style=" background-color: #F7DDBE">처음에 언급한 문제 2가지를 해결</span>할 수 있다고 논문에서 말한다.</p> <p>$a_i$를 조정하는 다양한 방법이 있지만, 논문에서는 $\alpha_i \propto \sigma_i^2$를 채택하였다고 한다. 그 이유는 <strong>Langevin dynamics에서 signal-to-noise 비율을 고정</strong>하는 것에서 비롯되었다. ($\frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z}$)</p> \[\mathbb{E} \left[ \left|\left| \frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z} \right|\right|^2_2 \right] \approx \mathbb{E} \left[ \frac{\alpha_i \left|\left|s_\theta(x, \sigma_i) \right|\right|^2_2}{4} \right] \propto \frac{1}{4} \mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right]\] <p>위 관계를 기억하자. 앞에서 우리는 $\left|\left| s_\theta(x, \sigma) \right|\right|_2 \propto \frac{1}{\sigma}$일때 Score network 학습이 optimal 해진다는 것을 찾았다.</p> <p>(그렇지 않은 경우에는 $\mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right] \propto 1$)</p> <p>그러므로 위 관계는 다음과 같이 되어 $\sigma_i$와 무관해진다.</p> \[\left|\left| \frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z} \right|\right|^2_2 \propto \frac{1}{4} \mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right] \propto \frac{1}{4}\] <p>위 <strong>Algorithm 1</strong>의 효율성을 입증하기 위해 실험을 진행하였으며, 그 결과를 section <a href="#2-slow-mixing-of-langevin-dynamics">3.2</a>에서 설명하였듯이 Annealed Langevin dynamics는 두 mode간의 가중치를 반영하여 복구한 것을 볼 수 있다.</p> <blockquote> <p>Sampling은 Annealed Langevin dynamics로 진행한다면, <strong>score estimator 역시 noise가 추가된 sample로 학습을 해야 정확한 score가 추정</strong>된다.</p> <p>때문에, sample에 다양한 크기의 nosie($\mathcal{N}(0, \sigma_i^2\textit{I})$)를 추가하여 학습시키는데, 이때 추가하는 noise의 크기는 갈수록 증가한다. ($\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L$, <strong>sampling의 정반대</strong>)</p> <p>sample은 데이터 $x \sim p(x)$에 noise $z \sim \mathcal{N}(0, \textit{I})$를 더한 $x + \sigma_i z$로 만들며 NCSN 모델 $s_\theta (x, i)$를 학습시킨다.</p> \[p_{\sigma_i}(x) = \int p(y)\mathcal{N}(x;y, \sigma_i^2,\textit{I})\text{d}y\] <p>여느 Conditional 모델처럼 $\sigma$(더해지는 nosie level)가 입력으로 들어가기 때문에 $s_\theta(x, i) : \mathbb{R}^{D+1} \rightarrow \mathbb{R}^D$가 된다.</p> <p>$s_\theta(x, i)$의 Objective는 다음과 같다. (Denoising score matching 기반이며 noise인 $\sigma$가 추가됨)</p> \[\sum_{i=1}^L \lambda(i) \mathbb{E}_{p_{\sigma_i}(x)}\left[|| \nabla_x \log p_{\sigma_i}(x) - s_\theta(x, i)||^2_2\right]\] <p>이러한 train과정을 거치면 Annealed Langevin dynamics를 통해 sampling을 진행하면 된다.</p> </blockquote> <h2 id="experiments"><strong>Experiments</strong></h2> <h4 id="set-up">Set up</h4> <ul> <li>사용 데이터셋 : MNIST, CelebA, CIFAR-10</li> <li>[0, 1] 범위로 scaling</li> <li>Noise $\sigma \in \lbrace\sigma_i\rbrace^L_{i=1}$가 $1 \sim 0.01$이 되도록 $L = 10$으로 설정 ($\sigma_L = 0.01$은 육안으로 거의 구분 불가)</li> <li>$T = 100$, $\epsilon = 2 \times 10^{-5}$</li> <li>Architecture 특징 : U-net, Conditional Instance normalization, Dilated convolutions</li> </ul> <p>Yang song은 Score-based model을 학습할 때 다음과 같은 사항을 추천했다.</p> <ul> <li>$\sigma_1 &lt; \cdots &lt; \sigma_L$은 기하수열로 구성</li> <li>$\sigma_1$은 충분히 작은 값에서 시작하여야 하며 $\sigma_L$은 데이터의 maximum pairwise distance 수준으로 결정</li> <li>$L$은 주로 수백 ~ 수천의 크기로 설정</li> </ul> <figure> <img src="https://yang-song.net/assets/img/post/score/celeba_large.gif" alt="adf"/> <img src="https://yang-song.net/assets/img/post/score/cifar10_large.gif" alt="dfd"/> <figcaption>NCSN 모델이 CelebA와 CIFAR-10 데이터셋으로 학습시켜 sampling하는 결과를 보여준다.</figcaption> </figure> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>이번 논문에서는 Score-based Generative model의 기본 원리에 대해 탐구해보다. 이 모델은 기존의 GAN, VAE와 같은 방식이 아닌 새로운 생성 모델 프레임워크로서, score라는 개념을 중심으로 새로운 패러다임을 제시한다. CIFAR-10에서 inception score SOTA를 달성하였으며, SNGAN에 필적하는 FID score를 얻었다고 한다.</p> <h2 id="code"><strong>Code</strong></h2>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="AI"/><category term="generative"/><category term="Score-based"/><summary type="html"><![CDATA[Score-based model 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] WassersteinGAN</title><link href="https://hahngt.github.io/blog/2024/WassersteinGAN/" rel="alternate" type="text/html" title="[Paper Review] WassersteinGAN"/><published>2024-03-01T15:12:11+00:00</published><updated>2024-03-01T15:12:11+00:00</updated><id>https://hahngt.github.io/blog/2024/WassersteinGAN</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/WassersteinGAN/"><![CDATA[<h1 id="wassersteinganwgan-논문-리뷰"><strong>WassersteinGAN(wGAN) 논문 리뷰</strong></h1> <p><a href="https://arxiv.org/abs/1701.07875">[논문 링크]</a></p> <p>수학적인, 특히 확률론에 관련된 내용과 글이 많아서 가독성이 떨어질 수 있다.</p> <p>필자가 예전에 wGAN에서 필요한 수학적 개념들을 정리한 글이 있으니 <a href="https://hahngyutak.github.io/posts/wGAN/">참고</a></p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>probability distribution, 즉 확률 분포를 학습한다는 것은 확률 밀도(density)를 학습한다는 말이다. 이는 $(P_\theta)_{\theta \in \mathbb{R}_d}$ 의 parametric family를 정의하고, 데이터에서 likehood를 maximize한 밀도를 찾는 방식으로 이루어진다.</p> <blockquote> <p>parametric family란, 일련의 확률 밀도 함수 또는 확률 분포를 의미한다. 이러한 분포는 parameter set에 의해 결정되며, 이러한 파라미터를 변경함으로써 다양한 분포가 생성될 수 있다.</p> <p>예를 들어, 정규분포는 <strong>평균</strong>과 <strong>표준편차</strong>라는 2개의 파라미터를 가지는 분포이다. 이를 변경함으로써 다양한 형태의 정규분포를 만들 수 있으며, 가능한 모든 정규분포의 집합을 ‘정규분포의 parametric family’라 할 수 있다.</p> </blockquote> <p>데이터 예시인 $\lbrace x_{i} \rbrace ^m_{i=1}$가 있을 때, 다음과 같은 문제를 해결할 수 있다.</p> \[\underset{\theta \in \mathbb{R}^d}{\text{max}} \frac{1}{m} \sum_{i=1}^m\log P_\theta(x^{(i)})\] <p>데이터 분포 $\mathbb{P}_r$이 밀도를 가지고, $\mathbb{P}_\theta$가 parametrize된 밀도 $P_\theta$의 분포인 경우, 이는 KL divergence $KL(\mathbb{P}_r || \mathbb{P}_\theta)$를 minimize하는 것과 같다.</p> <p>위를 설명하려면, 모델의 density(밀도) $P_\theta$가 있어야하지만, 저차원 <strong>maniford</strong>에서 <strong>support</strong>하는 분포를 다루는 상황에서는 density가 존재하지 않을 수도 있다.</p> <details> <summary><b>Maniford란</b></summary> <blockquote> maniford란 고차원 공간에서 데이터를 설명하는 데 사용되는 저차원 구조이며, 고차원 데이터에서 복잡성을 줄이고, 데이터의 핵심 특징을 추출하는 데 사용된다. 예를 들어, 지구 표면의 작은 부분은 2차원 평면처럼 보이기 때문에 지구 표면은 3차원 공간의 2차원 maniford라고 할 수 있다. 이처럼, maniford는 종종 데이터의 '구조' 또는 '패턴'을 찾는 데 사용되며, maniford 학습은 고차원 데이터에서 maniford를 찾아내는 과정이다. </blockquote> </details> <p><br/></p> <details> <summary><b>support</b></summary> <blockquote> 확률론에서 support란, 어떠한 확률 분포나 확률 밀도에서 양수인 값을 가지는 집합을 의미하며, <b>확률 변수의 가능한 결과들의 집합</b>을 의미한다. </blockquote> </details> <p><br/></p> <p>이 경우, 모델 maniford와 실제 데이터 분포의 support가 교집합을 거의 가지지 않으며, 이는 <strong>KL distance가 정의되지 않거나 infinite</strong>하다는 것을 의미한다.</p> <p>이 문제를 해결하기 위해 일반적으로 모델 분포에 noise를 추가한다. (실제로 대부분의 Generate 모델에는 Gaussian noise가 포함되어있음.) 하지만, 이러한 잡음이 sample의 품질을 저하시킨다.</p> <p>확률 밀도 $\mathbb{P}_r$을 직접 추정하는 대신, fixed 분포 $p(z)$를 가지는 random variable $Z$를 정의하고, 이를 매개변수 함수 $g_\theta : \mathcal{Z} \rightarrow \mathcal{X}$를 통해 변환하여 특정 분포 $\mathbb{P}_\theta$를 따르는 sample을 직접 생성할 수 있다.</p> <p>이 방법은 VAE와 GAN이 사용하며, GAN은 $x$를 결정하는 latent variable $z$를 input으로 하고, $G$와 $D$의 관계를 학습하며 $G$의 분포를 $P(x)$에 가까워지도록 유도한다. 하지만, <strong>Mode collapsing</strong>이 빈번하게 일어난다는 문제가 있다.</p> <p>이 논문에서는 <strong>모델 분포와 실제 분포간의 거리를 측정</strong>하는 다양한 방법, 즉 두 분포 사이의 <strong>거리 또는 발산</strong>($\rho (\mathbb{P}_\theta \mathbb{P}_r)$)을 정의하는 다양한 방법에 집중한다.</p> <p>매개변수 $\theta$를 최적화하기 위해서는 모델 분포 $\mathbb{P}_\theta$를 $\theta \rightarrow \mathbb{P}_\theta$라는 mapping이 <strong>continuous</strong>하도록 해야한다.</p> <blockquote> <p><strong>continuous</strong>한다는 것, 즉 Continuity이란 parameter $\theta_t$가 $\theta$로 converge(수렴)할 때 분포 $P_{\theta_t}$ 역시 $P_\theta$로 수렴한다는 것을 의미</p> </blockquote> <p>분포 $P_{\theta_t}$의 converge은 거리를 계산하는 방식에 따라 달라진다. 거리가 약할 수록 분포는 converge하기 쉽고 $\theta$space에서 $\mathbb{P}_\theta$space의 continuous mapping을 정의하기 쉬워진다.</p> <blockquote> <p>왜 연속적이어야 하는가에 대해서는 optimizing algorithm을 생각하면 된다. gradient를 활용하는 경사 하강법은 함수가 연속적이지 않으면 gradient는 증발하거나 무한대가 되기 때문이다.</p> </blockquote> <h2 id="different-distances"><strong>Different Distances</strong></h2> <p>여기서는 Introduction에서 언급한 다양한 distance에 대해 소개한다.</p> <p>논문에서 사용하는 표기는 다음과 같다.</p> <ul> <li>$\mathcal{X}$ : <a href="https://ko.wikipedia.org/wiki/%EC%BD%A4%ED%8C%A9%ED%8A%B8_%EA%B3%B5%EA%B0%84">compact metric set</a> (ex- 이미지 공간 $[0,1]^d$)</li> <li> <p>$\Sigma$ : $\mathcal{X}$의 모든 <a href="https://ko.wikipedia.org/wiki/%EB%B3%B4%EB%A0%90_%EC%A7%91%ED%95%A9">Borel subset</a></p> </li> <li>$\text{Prob}(\mathcal{X})$ : $\mathcal{X}$에 정의된 확률 측정값의 공간</li> </ul> <blockquote> <p>참 어렵다,,</p> </blockquote> <p>이제 두 분포 $\mathbb{P}_r, \mathbb{P}_g \in \text{Prob}(\mathcal{X})$사이의 다양한 distance와 divergence를 정의할 수 있다.</p> <h4 id="total-variation-tv-distance">Total Variation (TV) distance</h4> \[\delta(\mathbb{P}_r, \mathbb{P}_g) = \underset{A \in \Sigma}{\text{sup}} \left |\mathbb{P}_r(A) - \mathbb{P}_g(A) \right|\] <p>두 확률 분포의 측정값의 차의 상한, 즉 벌어질 수 있는 가장 큰 값을 뜻한다.</p> <h4 id="kullback-leibler-kl-divergence">Kullback-Leibler (KL) divergence</h4> \[KL(\mathbb{P}_r \parallel \mathbb{P}_g) = \int \log \left(\frac{P_r(x)}{P_g(x)}P_r(x)d\mu(x) \right)\] <p>여기서 $\mathbb{P}_r$와 $\mathbb{P}_g$는 $\mathcal{X}$에 정의된 측정값 $\mu$에 대해 <strong>연속적</strong>이며, 즉 밀도를 가진다고 가정할 수 있다.</p> <p>KL divergence는 $\mathbb{P}_g(x) = 0$이고 $\mathbb{P}_r &gt; 0$일때 무한할 수 있으며 비대칭적이다. ($KL(\mathbb{P}_r \parallel \mathbb{P}_g) \neq KL(\mathbb{P}_g \parallel \mathbb{P}_r)$)</p> <h4 id="jensen-shannon-js-divergence">Jensen-Shannon (JS) divergence</h4> \[JS(\mathbb{P}_r, \mathbb{P}_g) = KL(\mathbb{P}_r \parallel \mathbb{P}_m) + KL(\mathbb{P}_g \parallel \mathbb{P}_m)\] <p>여기서 $\mathbb{P}_m = \frac{\mathbb{P}_r + \mathbb{P}_g}{2}$이다.</p> <p>JS divergence는 두 분포에 대한 평균에 대한 KL divergence로 정의되기 때문에 대칭성을 가진다.</p> <h4 id="earth-mover-em-distance-or-wasserstein-distance">Earth-Mover <strong>(EM) distance</strong> or <strong>Wasserstein distance</strong></h4> \[W(\mathbb{P}_r, \mathbb{P}_g) = \underset{\gamma \in \prod(\mathbb{P}_r, \mathbb{P}_g)}{\text{inf}} \mathbb{E}_{(x,y)\sim\gamma}\left[\parallel x-y \parallel \right] \tag{1}\] <p>$\prod(\mathbb{P}_r, \mathbb{P}_g)$는 최댓값이 각각 $\mathbb{P}_r$과 $\mathbb{P}_g$인 모든 joint distribution $\gamma(x,y)$의 집합을 나타낸다.</p> <blockquote> <p>직관적으로 $\gamma(x,y)$는 분포 $\mathbb{P}_r$을 분포 $\mathbb{P}_g$로 변환하기 위해 $x$에서 $y$로 얼마나 많은 “질량”을 운반해야 하는지를 나타낸다. 그러면 EM distance는 최적의 운반 계획의 “비용”이라고 할 수 있다.</p> <p>이에 관한 설명은 해당 <a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">링크</a>에 매우 잘 설명되어 있다. (wGAN을 이해하는데 큰 도움이 될것이다.)</p> </blockquote> <p>논문에서는 한가지 예시를 통해 EM distance의 타당성을 제시하고 있다. 아래 참고</p> <details> <summary><b>Example 1</b></summary> <br/>임의의 확률분포 $\mathbb{P}_0$와 $\mathbb{P}_\theta$를 정의하고 그 사이의 distance와 divergence를 구한다. <br/> <br/><blockquote> $Z \sim U[0,1]$은 $Z$가 0과 1사이의 균일한 분포를 따른다는 것이다. $\mathbb{P}_0$를 $(0, Z) \in \mathbb{R}^2$라고 하자. 이는 $x$는 항상 0이고, $y$축 위 임의의 점 $Z$를 의미한다. $g_\theta (z) = (\theta, z)$는 입력 $z$이며 실수 파라미터인 $\theta$에 의해 결정되는 $(\theta, z)$를 출력한다. <br/> 이때, 각 distance는 다음과 같다. </blockquote> <ul> <li>EM distance : $W(\mathbb{P}_0, \mathbb{P}_\theta) = |\theta|$</li> <li>JS divergence : $JS(\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} \log 2 &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> <li>KL divergence : $KL(\mathbb{P}_0 \parallel \mathbb{P}_\theta) = \begin{cases} +\infty &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> <li>TV distance : $\delta (\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} 1 &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> </ul> $\theta_t \rightarrow 0$일때, EM distance를 제외한 다른 distance, divergence는 $\theta$가 0일때 불연속적인것을 확인할 수 있다. 때문에 다른 distance와 divergence를 사용한 loss function은 사용할 수 없다. <br/> <br/> 아래 사진은 위에 대해 각각 EM distance(좌)와 JS divergence(우)의 경우를 보여준다. <img src="/assets/img/post/wGAN/figure1.png" alt="figure1"/> </details> <p><br/></p> <p>위 <strong>Example 1</strong>을 통해 EM distance가 JS divergence보다 week하다는 것을 알 수 있다. 그러므로 간단한 가정 하에서 $W(P_r,P_\theta)$가 $\theta$에 대해 연속인 loss function인지 의문을 제기할수 있으며, 아래 <strong>Theorem 1</strong>을 통해 말하듯이 연속이다.</p> <details> <summary><b>Theorem 1</b></summary> $P_r$을 $\mathcal{X}$에 대한 고정 분포이며, $Z$을 다른 space $\mathcal{Z}$에 대한 radom 변수라고 가정하자. <br/> $g : \mathcal{Z} \times \mathbb{R}^d$라는 함수라 할때, $g_\theta(z) = (z, \theta)$라 하자. $\mathbb{P}_\theta$는 $g_\theta(Z) $의 분포이다. <ol> <li>$g$가 $\theta$에 대해 연속이라면, $W(\mathbb{P}_r, \mathbb{P}_\theta$)도 연속이다.</li> <li>$g$가 local Lipschitz 조건과 regularity assumption 1을 만족한다면, $W(\mathbb{P}_r, \mathbb{P}_\theta)$는 항상 연속이며 미분가능하다.</li> <li>위의 명제 1, 2는 $JS(\mathbb{P}_r, \mathbb{P}_\theta)$과 모든 KL divergence에 대해서는 성립하지 않는다.</li> </ol> <br/><blockquote> <b>local Lipschitz</b>란 함수의 변화율이 특정한 범위 내에서 제한되어 있음을 말한다. $K$-Lipschitz는 Lipschitz 상수 $K$를 가지며, 다음과 같은 제한이 있다. $$ |f(x_1) - f(x_2)| \leq K|x_1-x_2| $$ 즉, f(x₁)와 f(x₂)를 연결하면 그 gradient는 항상 $K$보다 작은 절대값을 가지며, 이를 함수의 Lipschitz 상수라고 한다. 사인 함수의 경우 도함수의 절대값은 항상 1로 제한되므로 Lipschitz 상수는 1이다. 직관적으로 Lipschitz 연속성은 gradient를 제한하며 딥러닝에서 gradient explosions을 완화하는데 사용된다. </blockquote> <blockquote> <b>regularity assumption 1</b><br/> 유한한 차원 vetor space 사이의 locally Lipschitz를 $g:\mathcal{Z} \times \mathbb{R}_d \rightarrow \mathcal{X}$라 한다. 아래와 같은 local Lipschitz 상수 $L(\theta,z)$가 있는 경우 $\mathcal{Z}$에 대한 특정 확률 분포 $p$에 대해 $g$가 가정 1을 만족한다고 한다. $$ \mathbb{E}_{z \sim p} \left [L(\theta, z) \right] &lt; + \infty $$ </blockquote> </details> <p><br/></p> <p>Neural Network에서 EM distance를 minimize하여 train하는 것이 이론적으로 합리적임을 <strong>다음</strong>을 통해 말한다.</p> <details> <summary><b>Corollary 1</b></summary> $g_\theta$는 $θ$로 parameterize된 feedfoward Neural Network이고, $p(z)$는 $\mathbb{E}\_{z \sim p(z)} [\parallel z \parallel] &lt; \infty$ (Gaussian, uniform 등)이 되도록 하는 $z$에 대한 사전분포라고 가정하자. 그러면 위 assumption 1이 충족되므로 $W(\mathbb{P}_r,\mathbb{P}_θ)$는 모든 곳에서 연속적이고 거의 모든 곳에서 미분 가능하다. </details> <p><br/></p> <p>이 모든 것은 EM distance가 적어도 JS divergence보다 훨씬 더 합리적인 cost function이라는 것을 보여주며 위상의 상대적 강도는 다음과 같다.</p> \[\underset{strong \;\;\;\rightarrow \;\;\;week}{\text{KL} &gt; \text{(JS, TV)} &gt; \text{EM}}\] <h2 id="wasserstein-gan"><strong>Wasserstein GAN</strong></h2> <p>EM distance인 <strong>Eq. 1</strong>의 극한은 매우 intractable하다. <strong>Eq. 1</strong>에서의 $\mathbb{P}_r$과 $\mathbb{P}_g$의 joint distribution을 구해야 하지만, $\mathbb{P}_r$는 우리가 알고자 하는 대상이기 때문이다.</p> <p>그래서 Kantorovich-Rubinstein duality를 이용해서 다음과 같이 바꿀 수 있다.</p> \[W(\mathbb{P}_r, \mathbb{P}_\theta) = \underset{\parallel f\parallel _L \leq1}{\text{sup}}\; \mathbb{E}_{x \sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)] \tag{2}\] <p>$\parallel f\parallel _L \leq1$에서 1 대신 $K$를 대입하면, $K \cdot W(\mathbb{P}_r, \mathbb{P}_\theta)$가 된다. 이는 임의의 상수 $K$에 대해 $K$-Lipschitz를 고려하는 것이다.</p> <blockquote> <p><strong>supremum</strong>, 즉 $\text{sup}$은 주어진 집합에서 가장 큰 <strong>상한</strong>을 의미한다. 이는 주어진 집합의 모든 원소보다 크거나 같은 가장 작은 숫자를 찾는 것을 의미하며 즉, 어떤 집합의 모든 원소를 넘지 않으면서 그 집합의 상한인 수를 의미한다.</p> <p>이는 ‘최대값(maximum)’과는 다른 개념이다. 최대값은 집합의 원소 중에서 가장 큰 값을 의미하지만, supremum은 집합의 원소가 아니어도 된다.</p> <p>예를 들어, 0과 1 사이의 모든 실수를 포함하는 집합의 최대값은 정의되지 않지만, supremum은 1이다.</p> </blockquote> <p>그러므로, paramerize된 함수의 가족 $\lbrace f_w \rbrace_{w \in \mathcal{W}} $이 $K$-Lipschitz 인 경우 다음과 같은 문제를 해결할 수 있다.</p> <p>즉, parameterize된 함수 $f_w$로 바꾸고, $\mathbb{P}_\theta$를 $g_\theta$에 관한 식으로 바꾸면 다음과 같은 식을 구할 수 있다.</p> \[\underset{w\in \mathcal{W}}{\text{max}}\; \mathbb{E}_{x \sim \mathbb{P}_r} [f_w(x)] - \mathbb{E}_{z\sim p(z)}[f_w(g_\theta(z))] \tag{3}\] <p><strong>Eq. 3</strong>은 GAN의 loss식과 유사해 보이며, 이 식에서 존재하는 $\mathbb{P}_r$에 대한 의문점이 생길 수 있다. 실제로는 학습된 $D$가 그 역할을 해주며, gradient를 update할 때 $\theta$에 대해 미분하기 때문에 앞 항은 사라진다. (wGAN에서는 $D$를 <strong>critic</strong>으로 대체한다. 추후 설명)</p> <p>논문에 있는 <strong>Theorem 3</strong>은 위 식 <strong>Eq. 3</strong>을 미분한다.</p> \[\bigtriangledown _\theta W(\mathbb{P}_r, \mathbb{P}_\theta) = -\mathbb{E}_{z\sim p(z)}[\bigtriangledown _\theta f(g_\theta(z))]\] <p>우리는 <strong>Eq. 2</strong>의 maximize 문제를 해결하기 위한 함수 $f$를 찾아야한다. 이를 위해 가중치 $w$로 parameterize한 신경망을 사용하며 GAN에서의 <strong>Discriminator</strong> 역할을 하게 된다. WassersteinGAN에서는 이를 <strong>critic</strong>이라 명명한다.</p> <blockquote> <p>Eq. 3은 critic의 loss 식이자, EM distance를 의미하며 $w$는 그 critic($f$)의 파라미터이다.</p> </blockquote> <p>$f_w$가 compact space $\mathcal{W}$으로 제한하기 위해서 gradient update 후에 $w$를 $[-0.01, 0.01]$ 사이로 조정하며, 이를 <strong>Weight clipping</strong>이라고 한다.</p> <blockquote> <p><strong>Weight clipping</strong>은 Lipschitz 조건을 적용하기 위해 넣었지만, 그렇게 좋지 못한 방법이라고 논문에서 말한다. 이 논문에서는 단순하고 이미 성능이 좋다고 알려진 정도의 clipping을 적용하였으며, Neural Network에서 Lipschitz 조건을 적용하는 문제는 추후 연구를 위해 남겨 놓았다고 말한다.</p> </blockquote> <p>WassersteinGAN의 <strong>Generator</strong> loss fuction은 <strong>Theorem 3</strong>에 나와있듯이, Eq. 3을 $\theta$에 대해 미분하여 앞의 항을 사라지게 하여 얻을 수 있다.</p> <p><img src="/assets/img/post/wGAN/wGANloss.webp" alt="wGANloss" style="zoom:80%;"/></p> <p>위 그림은 wGAN의 아키텍처를 표현한 그림(<a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">출처</a>)이며 <strong>Algorithm 1</strong>에 설명되어있다.</p> <p><img src="/assets/img/post/wGAN/Algorithm1.png" alt="Algorithm1" style="zoom:67%;"/></p> <p>위 알고리즘을 천천히 뜯어보자.</p> <ol> <li>$n_{\text{critic}}$ 횟수만큼 critic을 update한다. <ul> <li>$\mathbb{P}_r$과 $p(z)$를 각각 mini batch 만큼 sampling한다.</li> <li>critic loss function을 이용해서 파라미터 $w$를 update한다. (Adam 대신 RMSProp)</li> <li>clip($w, -c, c$)부분은 위에서 언급한 Weight clipping을 수행한 것이다.</li> </ul> </li> <li>critic update가 끝나면 $G$를 loss function에 맞게 update한다.</li> </ol> <p>GAN의 $D$는 이미지가 real인지 fake인지 sigmoid 확률값을 사용해 판단하며 gradient 정보는 사용되지 않는다.</p> <p>하지만 wGAN의 <strong>critic</strong>은 EM distance의 식을 그대로 사용하기 때문에, gradient를 제공하는 linear function으로 수렴한다. 즉, output이 scalar값으로 이미지의 진위 여부를 점수처럼 표시한다.</p> <p><img src="/assets/img/post/wGAN/figure2.png" alt="figure2"/></p> <p>위 사진을 보면 <span style="color: #EE4444">GAN</span>의 $D$는 Gradient vanishing 문제가 생겼으며, <span style="color: #7CddD8">WGAN</span>은 안정적인 linear한 형태를 띄고 있음을 알 수 있다.</p> <p><span style=" background-color: #F7DDBE"><b>EM distance가 연속적이고 미분 가능하다는 것은 critic을 optimal 할때까지 계속 train 할 수 있다는 의미이며, 기존 GAN이 가지고 있던 mode collapse문제와 $G$와 $D$간의 balance 붕괴 문제가 해결되는 것이다.</b></span></p> <h2 id="empirical-results"><strong>Empirical Results</strong></h2> <p>저자들은 WassersteinGAN 알고리즘을 사용하여 이미지 생성 실험을 진행하였을 때, 2가지 이점을 얘기한다.</p> <ul> <li>$G$의 수렴과 sample의 품질에 영향을 미치는 loss metric</li> <li>Optimization process의 안정화</li> </ul> <h3 id="experimental-procedure"><strong>Experimental Procedure</strong></h3> <p>Train에 사용한 데이터는 LSUN-Bedroom dataset이며 WGAN과 비교 대상은 DCGAN의 아키텍쳐를 따른 GAN이다.</p> <h3 id="meaningful-loss-metric"><strong>Meaningful loss metric</strong></h3> <p>첫 번째 실험은 이 추정치가 생성된 샘플의 품질과 얼마나 잘 상관관계를 가지는지를 보여준다.</p> <p>DCGAN 아키텍처 외에도, $G$ 또는 $G$와 critic 모두를 512개의 hidden unit을 가진 4-layer ReLU-MLP(Multi-Layer Perceptron)로 교체하는 실험도 진행하였다.</p> <p><img src="/assets/img/post/wGAN/figure3.png" alt="figure3"/></p> <ul> <li>좌측 상단 : $G$만 4-layer ReLU-MLP로 교체한 결과 <ul> <li>학습이 진행될수록 sample 품질은 향상되고 loss는 감소</li> </ul> </li> <li>우측 상단 : 표준 DCGAN <ul> <li>loss는 빠르게 감소하고 sample 품질도 그에따라 증가</li> </ul> </li> </ul> <p>상단 2개의 plot은 모두 critic에 sigmoid가 없는 DCGAN이므로 비교할수 없다.</p> <ul> <li>아래쪽 : $G$와 $D$모두 4-layer ReLU-MLP로 교체 <ul> <li>학습률이 상당히 높은 MLP라서 학습 <strong>실패</strong></li> <li>loss와 sample이 모두 일정하다.</li> </ul> </li> </ul> <p><img src="/assets/img/post/wGAN/figure4.png" alt="figure4"/></p> <p>위 plot은 같은 모델로 실험을 진행하되, 각 iter 마다 JS Divergence를 계산한 그래프이다. sample 품질이 좋아져도 JS Divergence는 증가하거나 상수 값을 유지하는 것을 알 수 있다.</p> <p>실제로 JS Divergence의 max인 $\log 2 ≈ 0.69$에 매우 근접하는 경우가 많다. 즉, JS Divergence가 포화되고 $D$의 loss가 0가 되며 생성된 sample이 어떤 경우에는 meaningful한(오른쪽 상단 플롯) 반면 어떤 경우에는 무의미한 이미지가 되기도 한다.</p> <p>또한, 기존에는 Optimizer로 <strong>Adam</strong>을 사용했는데, 이와 같은 momentum기반 optimizer나 높은 train 속도를 사용하는 경우 WGAN의 train이 불안정해진다고 한다. <strong>critic</strong>의 loss는 fix되어있지 않기 때문에, momentum 기반 대신, non-fixed 문제에도 잘 작동하는 <strong>RMSPorp</strong>를 사용하였다고 한다.</p> <h4 id="improved-stability"><strong>Improved stability</strong></h4> <p>WassersteinGAN은 최적의 <strong>critic</strong>으로 train할 수 있다는 장점이 있다. 기존의 GAN은 $D$와 $G$의 balance를 맞추며 학습해야했다. 하지만 wGAN은 <strong>critic</strong>이 완전히 train되면, $G$에 loss를 제공하기만 하면 되며 $G$의 gradient 품질은 <strong>critic</strong>의 성능에 비례한다.</p> <p>DCGAN의 $G$를 사용하되, 여러 변화를 주어서 실험하여 $D$와 <strong>critic</strong>을 비교하였다. (즉, WGAN과 GAN을 비교함 셈)</p> <p><span style=" background-color: #F7DDBE"><b>논문에서는 실험할 때 WGAN에서는 mode collapse가 발견된적이 없다고 한다.</b></span></p> <blockquote> <p>즉, WGAN은 GAN의 학습 과정을 안정화시키는 데 큰 기여를 했다. EM distance를 사용하여 optimal한 지점에 도달할 수 있게 되었다. 이러한 변형은 mode collapse 를 해결함으로써 다양한 sample을 생성할 수 있게 하여 기존의 GAN이 가지고 있던 문제들을 해결하였다고 한다.</p> </blockquote> <h2 id="code-review"><strong>Code Review</strong></h2> <p><a href="https://colab.research.google.com/drive/1Uotwtr0CELvtnovj5NP-yGod6VyG9DRR?usp=sharing">wGAN 구현 Colab 이동하기</a></p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="GAN"/><category term="AI"/><category term="generative"/><summary type="html"><![CDATA[WassersteinGAN(wGAN) 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] Diffusion Models Beat GANs on Image Synthesis</title><link href="https://hahngt.github.io/blog/2024/Diffusion-Models-Beat-GANs/" rel="alternate" type="text/html" title="[Paper Review] Diffusion Models Beat GANs on Image Synthesis"/><published>2024-02-19T12:40:11+00:00</published><updated>2024-02-19T12:40:11+00:00</updated><id>https://hahngt.github.io/blog/2024/Diffusion%20Models%20Beat%20GANs</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/Diffusion-Models-Beat-GANs/"><![CDATA[<h1 id="diffusion-models-beat-gans-on-image-synthesis-논문-리뷰"><strong>Diffusion Models Beat GANs on Image Synthesis 논문 리뷰</strong></h1> <p>논문 링크 <a href="https://arxiv.org/abs/2105.05233">arxiv</a></p> <h2 id="introduction"><strong>Introduction</strong></h2> <p><img src="/assets/img/post/DMBG/figure1.png" alt="figure1"/></p> <p>Image generation 분야에서 SOTA(state of the art)를 달성한 모델은 GAN이다. 하지만 GAN은 fidelity와 diversity의 trade-off에서 <strong>fidelity</strong>를 선택하여 다양성이 부족하다는 것과 적절한 hyperparameter없이는 훈련이 어렵고 Mode Collapse 현상이 발생한다.</p> <blockquote> <p><strong>Diversity</strong>(다양성)과 <strong>Fidelity</strong>(충실도)의 trade-off는 생성 모델에서 중요한 개념이다.</p> <ul> <li> <p><strong>Diversity</strong>은 모델이 실제 데이터 분포의 다양성을 잘 포착하고, 신규 sample 생성 시 다양한 결과를 내놓는 것을 의미한다.</p> </li> <li> <p><strong>Fidelity</strong>는 생성된 sample이 실제 데이터와 얼마나 유사한지를 의미한다. 높은 충실도를 가진 모델은 실제 데이터와 구별하기 어려운 sample을 생성한다.</p> </li> </ul> <p>GAN은 Fidelity를 선택하여 SOTA를 달성하였다.</p> </blockquote> <p>Diffusion 모델은 최근 높은 품질의 이미지를 생성하며 다양한 이점을 보이는 모델이다. 하지만, LSUN, ImageNet과 같은 어려운 generative 데이터셋에서는 GAN에 뒤처진다.</p> <p>논문에서는 Diffusion 모델을 개선, fidelity과 diversity을 절충하여 GAN을 여러 측정 기준과 데이터셋에서 능가하는 모델을 제안한다.</p> <blockquote> <p>이 논문이 발표되면서 Image generation 분야에 SOTA는 이 논문이 되었다.</p> </blockquote> <h2 id="background"><strong>Background</strong></h2> <p>이 섹션에서는 Diffusion모델과 DDPM, DDIM에 대해 간단히 요약한 후 넘어가자.</p> <blockquote> <p><strong>자세한 내용을 알고싶으면 다음 링크를 참고</strong></p> <p><a href="https://hahngyutak.github.io/posts/DDPM/">DDPM 논문리뷰</a> <a href="https://hahngyutak.github.io/posts/DDIM/">DDIM 논문리뷰</a></p> </blockquote> <p>Diffusion 모델은 noise를 추가하는 foward process를 뒤집어 sampling을 진행한다. sampling은 noise인 $x_T$에서 시작하고 $x_{T-1}$, $x_{T-2}$, … 와 같이 점진적으로 noise를 제거하여 최종 sample인 $x_0$을 생성한다. 대부분의 Diffusion 모델에서는 noise $\epsilon$을 Gaussian nosie로 가정한다.</p> <h4 id="ddpm"><strong>DDPM</strong></h4> <p>Diffusion 모델은 $x_t$에서 약간 denoising된 $x_{t-1}$을 생성하는 것을 학습하는데, <strong>DDPM</strong>의 경우 $x_t$의 noise 정도를 예측하는 함수인 $\epsilon_\theta (x_t, t)$로 parameterize한다. $x_t$는 $x_0$와 $t$, noise $\epsilon$를 이용해 다음과 같이 정의할 수 있다.</p> \[\begin{align} q(x_{t}|x_{0}) &amp;= N(x_{t}; \sqrt{\overline{\alpha}_{t}}x_{0}, (1-\overline{\alpha}_{t})\text{I}) \nonumber \\ &amp;= \sqrt{\overline{\alpha}_t}x_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon, \;\epsilon \sim N(0,\text{I}) \nonumber \end{align}\] <p>Train Objective는 다음과 같이 간단화된 Loss식이 더 성능이 좋게 나오는것을 확인한다.</p> \[L_{simple}(\theta) = \mathbb{E}_{t, x_0, \epsilon}\left[\left|\left| \epsilon - \epsilon_\theta(\sqrt{\overline{\alpha}_t}x_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon, t)\right|\right|^2 \right]\] <p>DDPM은 $x_t$가 주어졌을 때 $x_{t-1}$의 분포인 $p_\theta(x_{t-1} | x_t)$를 Gaussian인 $N(x_{t-1}; \mu_\theta(x_t, t), \sum_\theta(x_t, t))$로 모델링 할 수 있으며 평균인 $\mu_\theta(x_t, t)$은 함수 $\epsilon_\theta(x_t, t)$로 계산할 수 있음을 보인다.</p> <p>또한 실제 variational lower bound $L_{vlb}$보다 $L_{simple}$이 더 좋은 결과를 나타낸다.</p> <h4 id="improved-denoising-diffusion-probabilistic-models"><strong>Improved denoising diffusion probabilistic models</strong></h4> <p>이 논문에서는 DDPM처럼 $\Sigma_\theta(x_t,t)$를 상수로 fix하는 것이 적은 step으로 sampling하는 것이 효과적이지 않으며 $\Sigma_\theta(x_t,t)$를 다음과 같이 output $v$가 inpolation되는 Neural Network로 Parameterization하는 것을 제안한다.</p> \[\Sigma_\theta(x_t,t) = \text{exp}(v \log \beta_t + (1-v)\log \widetilde{\beta}_t)\] <p>Training Objective 역시$L_{simple} + \lambda L_{vlb}$를 사용하여 $\epsilon_\theta(x_t, t)$와 $\sum_\theta(x_t, t))$을 동시에 Training하는 하이브리드 방식을 제안하여 적은 step으로 sampling해도 품질이 떨어지지 않는다.</p> <p>이번 논문에서는 이러한 Objective와 Parameterization을 채택하여 Experiment에 사용하였다고 한다.</p> <h4 id="ddim">DDIM</h4> <p>DDIM은 DDPM과 동일한 foward process를 가지지만 reverse process에서 non-Markovian으로 대체하여 deterministic한 mapping을 학습한다. 이를 통해 매우 적은 step으로 sampling할 수 있는 방법을 제안한다.</p> <h2 id="architecture-improvements"><strong>Architecture Improvements</strong></h2> <p>이 섹션에서는 Diffusion 모델에 적합한 Architecture를 찾는 과정을 수행한다.</p> <p>DDPM 등 여러 연구를 통해 U-net 구조가 sample의 품질을 향상시킨다는 것을 발견했다.</p> <details> <summary>U-Net</summary> ​ <img src="https://joungheekim.github.io/img/in-post/2020/2020-09-28/model_structure.gif" alt="unet_archi"/> <a href="https://velog.io/@lighthouse97/UNet%EC%9D%98-%EC%9D%B4%ED%95%B4">U-net의 이해 </a> </details> <p>다른 연구들을 통해 U-net 구조에 대한 약간의 변형이 CIFAR-10과 CelebA-64, ImageNet 128*128에서 높은 품질의 sample을 생성할 수 있음을 발견하였다.</p> <p>논문에서는 다음과 같은 변형을 사용하였다.</p> <ul> <li>모델 크기를 상대적으로 유지하며 깊이와 폭 size를 늘린다.</li> <li>attention head의 수를 늘린다.</li> <li>다양한 해상도(32*32, 16*16, 8*8)에서 attention기법을 사용한다.</li> <li>BigGAN의 Residual block을 사용하여 Up/Down sampling진행</li> <li>Residual connection을 $\frac{1}{\sqrt{2}}$로 rescaling한다.</li> </ul> <p>이 섹션의 모든 비교를 위해 batch size 256의 ImageNet 128×128 데이터셋에서 모델을 train하고 sampling step을 250으로 하여 진행한다.</p> <p><img src="/assets/img/post/DMBG/table1.png" alt="table1" style="zoom:50%;"/></p> <p>위 표는 700,000와 1,200,000 iter로 평가한 다양한 변형 조건 제거에 따른 결과이다. 모든 조건을 사용하였을 때 성능이 증가함을 보였다.</p> <p><img src="/assets/img/post/DMBG/figure2.png" alt="figure2"/></p> <p>그림 2에서 볼수있듯이 깊이가 증가하면 성능은 증가하지만 train 시간이 늘어나고 더 넓은 모델과 같은 성능을 내기 위해 train 시간이 더 걸리기 때문에 추가 실험에서 이 변경 사항을 사용하지 않는다고 한다.</p> <p><img src="/assets/img/post/DMBG/table2.png" alt="table2"/></p> <p>그림 2와 표 2를 보면 head당 64개의 channel이 wall-clock time에 가장 적합하다는 것을 알 수 있으므로, default값으로 64개의 channel을 사용하였다. 논문에서는 이 method가 최신 Transformer 구조와 더 잘 일치하다고 한다.</p> <blockquote> <p><a href="https://hahngyutak.github.io/posts/Transformer/">Transformer 구조 설명</a></p> </blockquote> <h4 id="adaptive-group-normalization"><strong>Adaptive Group Normalization</strong></h4> <p>논문에서는 AdaIN(adaptive instance norm), FiLM과 유사한 AdaGN(adaptive group normalization) layer를 사용한다. AdaGN은 Group Normalization을 진행한 후, 각 Residual block에 timestep과 class embedding의 정보를 전달한다.</p> \[\text{AdaGN}(h, y) = y_s \text{GroupNorm}(h) + y_b\] <blockquote> <p>여기서 $h$는 첫 Convolution layer에 따른 Residual block의 중간 activation이며 $y=[y_s, y_b]$는 timestep과 class embedding의 linear projection으로부터 얻는다.</p> </blockquote> <p><img src="/assets/img/post/DMBG/table3.png" alt="table3" style="zoom:50%;"/></p> <p>위 표를 보면 AdaGN을 사용함으로써 FID값이 개선된 것을 확인할 수 있다.</p> <p>최종적으로 실험에 사용한 setting은 다음과 같다.</p> <ul> <li>Variable width with 2 residual blocks per resolution</li> <li>Multiple heads with 64 channels per head</li> <li>Attention at 32, 16 and 8 resolutions</li> <li>BigGAN residual blocks for up and downsampling</li> <li>adaptive group normalization(AdaGN)</li> </ul> <h2 id="classifier-guidance"><strong>Classifier Guidance</strong></h2> <p>GAN의 조건부 생성에는 class label이 사용된다. GAN에서는 Discriminator를 Classifier $p(y|x)$처럼 사용하거나 class-conditional normalization 방법을 사용하여 conditional 생성을 수행한다.</p> <p>논문에서는 이러한 방식을 채용하여 Classifier $p(y|x)$를 활용하여 Diffusion 모델의 개선을 시도한다. 앞에서 AdaGN을 통해 class embedding 정보를 전달하고 있다. 앞선 연구(<a href="https://arxiv.org/abs/1503.03585">1</a>, <a href="https://arxiv.org/abs/2011.13456">2</a>)에서는 Classifier의 gradient를 이용해 pre-train된 Diffusion모델을 조절한다. noise 이미지 $x_t$로 Classifier $p_\phi (y|x_t, t)$를 학습한 후, 기울기 $\triangledown_{x_t} \log p_\phi (y|x_t, t)$를 사용해 어떤 class label $y$을 생성하도록 diffusion sampling process를 유도할 수 있다.</p> <p>지금부터 Classifier를 사용하여 conditional sampling process를 도출하는 2가지 방법에 대해 살펴본 후, 이러한 Classifier를 사용하여 sample의 품질을 향상시키는 방법을 알아보자.</p> <blockquote> <p>논문에서는 간결성을 위해 $p_\phi (y|x_t, t)$는 $p_\phi (y|x_t)$로, $\epsilon_\theta(x_t, t)$는 $\epsilon_\theta(x_t)$로 표기한다.</p> <p>위 표기는 각 timestep $t$에 대한 함수를 나타내며, train할때 모델이 input $t$에 대한 조건부임을 유의해야한다.</p> </blockquote> <h4 id="conditional-reverse-noising-process"><strong>Conditional Reverse Noising Process</strong></h4> <p>unconditional reverse process인 $p_\theta(x_t | x_{t+1})$에 label $y$의 condition을 부여하기 위해서는 다음과 같이 각 transition을 sampling 해야한다. ($Z$는 정규화 상수)</p> \[p_{\theta,\phi}(x_t|x_{t+1}, y) = Zp_\theta(x_t|x_{t+1})p_\phi(y|x_t) \tag{2}\] <p>이러한 분포에서 sampling하는 것은 어렵지만, <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al.</a>은 이 분포가 Gaussian 분포에 근사화된다는 것을 보여준다.</p> <p>Diffusion 모델이 $x_{t+1}$에서 $x_t$를 예측할 때 Gaussian 분포를 사용한다는 것을 상기하면서 다음 식을 보자.</p> \[\begin{align} p_\theta(x_t|x_{t+1}) &amp;= \textit{N}(\mu, \Sigma) \tag{3}\\ \log p_\theta(x_t|x_{t+1}) &amp;= -\frac{1}{2}(x_t - \mu)^T \Sigma^{-1}(x_t, - \mu) + C \tag{4} \end{align}\] <p>우리는 $\log p_\phi(y_t|x_{t})$이 $\Sigma_{-1}$보다 낮은 곡률을 가지고 있다고 가정할 수 있으며, 이는 무한한 diffusion step을 가질때 $||\Sigma|| \rightarrow 0$이 되어 reasonable하게 된다. 이러한 경우에 $\log p_\phi(y_t|x_{t})$는 $x_t = \mu$ 근처에서 Taylor 전개를 사용하여 다음과 같이 근사화될 수 있다.</p> <blockquote> <p>Taylor expansion(테일러 전개)와 Taylor series(테일러 급수)는 같은 뜻이다.</p> </blockquote> \[\begin{align} \log p_\phi(y_t|x_{t}) &amp;\approx \log p_\phi(y|x_t)\mid_{x_t=\mu} + (x_t - \mu) \bigtriangledown_{x_t} \log p_\phi(y|x_t)\mid_{x_t=\mu} \tag{5} \\ &amp;= (x_t - \mu)g + C_1 \tag{6} \\ \end{align}\] <p>여기서 $g = \bigtriangledown_{x_t} \log p_\phi(y|x_t)\mid_{x_t=\mu}$이며 $C_1$은 상수이다. 이를 통해 다음과 같이 전개할 수 있다.</p> \[\begin{align} \log (p_\theta(x_t|x_{t+1})p_\phi(y|x_t)) &amp;\approx -\frac{1}{2}(x_t-\mu)^T\Sigma^{-1}(x_t-\mu) + (x_t-\mu)g + C_2 \tag{7}\\ &amp;= -\frac{1}{2}(x_t-\mu-\Sigma g)^T\Sigma^{-1}(x_t-\mu-\Sigma g) + \frac{1}{2}g^T\Sigma g + C_2 \tag{8}\\ &amp;= -\frac{1}{2}(x_t-\mu-\Sigma g)^T\Sigma^{-1}(x_t-\mu-\Sigma g) + C_3 \tag{9} \\ &amp;= \log p(z) + C_4, \;z\sim \textit{N}(\mu + \Sigma g, \Sigma) \tag{10} \end{align}\] <p>마지막 줄에서 $C_4$는 Eq .2의 $Z$처럼 정규화 계수이므로 무시할 수 있다. 따라서 conditional transition operator는 unconditional과 유사하며, 평균이 $\Sigma g$만큼 이동한 Gaussian으로 근사화할 수 있음을 알 수 있다.</p> <p>이러한 sampling 알고리즘을 요약하여 <strong>Algorithm 1</strong>로 표현할 수 있다.</p> <p><img src="/assets/img/post/DMBG/algorithm1.png" alt="algorithm1" style="zoom:80%;"/></p> <p>여기서 $s$는 gradient에 대한 optional scale 계수이며 이는 추후 <strong>Scaling Classifier Gradients</strong>에서 자세히 다룬다.</p> <h4 id="conditional-sampling-for-ddim"><strong>Conditional Sampling for DDIM</strong></h4> <p>위 섹션에서의 유도는 DDPM과 같은 stochastic한 diffusion process에만 유효하며, DDIM과 같이 deterministic한 sampling 방법에는 적용할 수 없다.</p> <p>이를 위해 논문에서는 Score-based conditioning trick을 사용한다. sample에 추가된 noise를 예측하는 모델인 $\epsilon_\theta(x_t)$가 있는 경우, 이를 사용하여 Score function을 유도할 수 있다.</p> <blockquote> <p>Score-based conditioning trick는 Score-based generative models에서 사용하는 방법으로, conditional generate 문제에 사용된다.</p> <p>Score-based generative models은 각 데이터 sample의 “score”를 계산하는 function, 즉 score function을 학습한다. 여기서 “score”는 sample이 얼마나 모델이 학습한 데이터 분포에 부합하는지를 나타낸다.</p> <p>Score-based conditioning trick은 학습한 score function을 특정 조건을 만족하는 sample을 생성하도록 유도하여 특정 condition을 만족하는 sample을 생성하도록 유도하는 trick이다.</p> </blockquote> \[\bigtriangledown_{x_t} \log p_\theta(x_t) = -\frac{1}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t) \tag{11}\] <p>위 식을 Score function에 대입하여 $p(x_t)p(y|x)$를 구할수 있다.</p> \[\begin{align} \bigtriangledown_{x_t}\log (p_\theta(x_t)p_\phi(y|x_t)) &amp;= \bigtriangledown_{x_t} \log p_\theta(x_t) + \bigtriangledown_{x_t}\log p_\phi(y|x_t) \tag{12}\\ &amp;= -\frac{1}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t) + \bigtriangledown_{x_t}\log p_\phi(y|x_t) \tag{13} \end{align}\] <p>마지막으로, joint distribution의 score에 해당하는 새로운 epsilon prediction $\hat{\epsilon}(x_t)$을 정의할 수 있다.</p> \[\begin{align} \hat{\epsilon}(x_t) := \epsilon_\theta(x_t) - \sqrt{1-\overline{\alpha}_t} \bigtriangledown_{x_t} \log p_\phi(y|x_t) \tag{14} \end{align}\] <p>DDIM에 사용되는 sampling process를 사용하되, $\epsilon_\theta(x_t)$대신 $\hat{\epsilon}_\theta(x_t)$를 사용하면된다.</p> <p>이러한 sampling 과정은 <strong>Algorithm 2</strong>에 요약되어있다.</p> <p><img src="/assets/img/post/DMBG/algorithm2.png" alt="algorithm2" style="zoom:80%;"/></p> <h4 id="scaling-classifier-gradients"><strong>Scaling Classifier Gradients</strong></h4> <p>대규모 generative task에 Classifier guidance를 적용하기 위해 ImageNet을 Classifier에 train한다. Classifier 구조는 8×8 layer에 attention pool이 있는 <strong>U-net 모델의 downsampling 하는 부분을 사용</strong>하였다. Diffusion 모델과 동일한 noise distribution으로 train하고, overfitting을 방지하기 위해 무작위 crop을 추가하였다. 훈련 후에서는 <strong>Algorithm 1</strong>에 <strong>Eq. 10</strong>을 사용하여 Classifier를 sampling process에 적용시켰다.</p> <p><img src="/assets/img/post/DMBG/figure3.png" alt="figure3"/></p> <p>unconditional ImageNet 모델을 사용할 때, classifier의 gradient를 1보다 큰 계수로 설정해야한다는 것을 발견했다. 1로 하였을 때 classifier는 최종 sample의 class에 대해 50%의 적절한 확률을 할당했지만, 실제 이미지를 확인한 결과 의도한 class와 맞지 않는다는 것을 발견하였다고 한다. 이를 해결하기 위해 gradient를 1보다 크게 하였고, 확률이 거의 100%까지 증가했다고 한다.</p> <p>위 사진은 Pembroke Welsh corgi라는 class로 생성한 결과이며 왼쪽은 classifier scale가 1, 오른쪽은 10으로 조정한 sample이다.</p> <p>Classifier gradient의 scaling을 이해하려면 아래 식을 참고해야한다. ($Z$는 상수)</p> \[s \cdot \bigtriangledown_x \log p(y|x) = \bigtriangledown_x \log \frac{1}{Z}p(y|x)^s\] <p>conditioning process는 $p(y|x)^s$에 비례하는 nomalized된 classifier distribution에 기반을 두고 있다. $s &gt; 1$일때, 이 분포는 $p(y|x)$보다 선명해진다.</p> <blockquote> <p>Gradient scale이 커질수록 classifier에 기능이 집중되며 diverse가 낮지만 fidelity가 높은 sample을 생성할 수 있다.</p> </blockquote> <p><img src="/assets/img/post/DMBG/table4.png" alt="table4" style="zoom:50%;"/></p> <p>위 표는 classifier guidance를 통해 conditional/unconditional 모델의 sample 품질이 크게 향상됨을 보여주며, conditional model의 sample 품질이 더 좋다는 것을 알 수 있다.</p> <blockquote> <p>unconditional 모델은 AdaGN으로 class 정보를 전달하지 않은 모델이다.</p> </blockquote> <h2 id="result"><strong>Result</strong></h2> <p>다음 2가지에 대한 Evaluate를 진행한다.</p> <ol> <li> <p>unconditional 모델 아키텍처를 평가하기 위해 침실, 말, 고양이의 세 가지 LSUN class에 대해 별도의 Diffusion 모델을 train</p> </li> <li> <p>Classifier Guidance에 대해 평가하기 위해 128×128, 256×256, 512×512 해상도의 ImageNet 데이터셋에서 conditional diffusion 모델을 train</p> </li> </ol> <p><img src="/assets/img/post/DMBG/table5.png" alt="table5"/></p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="AI"/><category term="generative"/><category term="Diffusion"/><summary type="html"><![CDATA[Diffusion Models Beat GANs on Image Synthesis 논문 리뷰]]></summary></entry><entry><title type="html">[Paper Review] Pix2pix</title><link href="https://hahngt.github.io/blog/2024/Pix2pix/" rel="alternate" type="text/html" title="[Paper Review] Pix2pix"/><published>2024-02-07T15:12:11+00:00</published><updated>2024-02-07T15:12:11+00:00</updated><id>https://hahngt.github.io/blog/2024/Pix2pix</id><content type="html" xml:base="https://hahngt.github.io/blog/2024/Pix2pix/"><![CDATA[<h1 id="pix2pix-논문-리뷰"><strong>Pix2Pix 논문 리뷰</strong></h1> <p>Pix2pix는 Image-to-Image Translation을 수행하는 모델에 관한 연구로서, <strong>Conditional Generative Adversarial Network(CGAN)</strong>을 사용하여 하나의 structure을 이용한 이미지 간의 domain 변환을 수행하는 base model을 제공했다는 점에서 많은 인용 수를 자랑한다.</p> <p>이번 논문에서 제안하는 network는 <strong>input 이미지 → ouput 이미지로의 mapping</strong>을 학습할 뿐만 아니라 <strong>mapping train을 위한 loss function</strong>도 학습한다. 따라서 기존에는 서로 다른 loss식이 필요했던 문제에도 동일한 방식을 적용할 수 있다. 이번 논문은 더 이상 mapping function을 직접 설계하지 않으며, loss function 역시 직접 설계하지 않고도 합리적인 결과를 얻을 수 있음을 시사한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p><img src="/assets/img/post/pix2pix/figure1.png" alt="figure1"/></p> <p>어떤 개념을 다양한 언어로 표현할 수 있는 것처럼, 이미지 역시 RGB, gradient field, edge map, sementic label map 등으로 표현할 수 있다. 위 figure 1처럼 이미지 간 translation은 충분한 train 데이터가 주어졌을 때 이미지에 내포된 domain 중 하나를 다른 domain으로 변환하는 작업으로 정의할 수 있다.</p> <p>Image-to-Image Translation 역시 loss function을 minimize하는 것을 목표로 하는 CNN을 사용하여 많은 연구가 이루어졌다. predict된 sample과 실제 데이터 사이의 <strong>Euclidean distance를 minimize하면 흐릿(blur)한 이미지를 얻는다.</strong> Euclidean distance는 그럴듯한(?) output을 모두 평균내어 계산하기 때문이다.</p> <p>이러한 문제를 해결하기 위해 이번 논문에서는 생성된 이미지가 fake인지 real인지 구별하는 <strong>GAN</strong>의 특성을 이용한다. blur 이미지는 fake로 분류될 것이고, GAN은 데이터에 맞는 loss function을 학습하기 때문에 다양한 loss가 필요한 task에 적용할 수 있기 때문이다.</p> <blockquote> <p>특히 특정 조건을 가진 이미지를 학습하기 때문에 Conditional GAN, 즉 cGAN을 사용한다. 원래의 GAN은 생성되는 sample이 어떤 label을 가지는지 조절할수 없었다. 이에 대한 가이드라인을 $D$와 $G$의 input으로 넣어줌으로써 원하는 label에 대한 output을 도출하게 만든 모델이 cGAN이다.</p> </blockquote> <h2 id="method"><strong>Method</strong></h2> <h3 id="31-objective"><strong>3.1 Objective</strong></h3> <p>cGAN은 observed image인 $x$와 noise vector $z$를 output인 $y$로의 mappping을 학습한다. (<strong>$G : ( x,z ) \to y$</strong>)</p> <p>cGAN의 Loss 식은 다음과 같다.</p> \[\underset{G}{\text{min}}\; \underset{D}{\text{max}}\;\mathrm{L}_{cGAN}(G, D) = \mathbb{E}_{x, y} \left[\log D(x, y) \right] + \mathbb{E}_{x, z} \left[1 - \log D(x, G(x, z)) \right] \tag {1}\] <p>또한 원래 GAN의 objective와 L2 distance를 결합하는 것이 좋다는 것을 발견하였고, blurring이 덜한 L1 distance를 사용한다.</p> \[\mathrm{L}_{L1}(G) = \mathbb{E}_{x,y,z} \left[\| y - G(x,y)\|_1 \right] \tag{3} \\\] <p>최종 Objective는 다음과 같다.</p> \[G^* = \text{arg}\;\underset{G}{\text{min}}\;\underset{D}{\text{max}}\; \mathrm{L}_{cGAN}(G, D)+ \lambda \mathrm{L}_{L1}(G) \tag{4}\] <blockquote> <p>pix2pix는 noise $z$를 사용하지 않는다. $G$가 이 noise를 무시하도록 학습하며, 이때문에 mapping할 때 stochastic하지 않고 deterministic해진다.</p> </blockquote> <h3 id="32-network-architecture"><strong>3.2 Network Architecture</strong></h3> <p>논문에서 제안하는 모델에서의 $G$와 $D$는 DCGAN(Deep Convolution GAN)을 베이스로 하였다. (Convolution, BatchNorm, ReLU 형식)</p> <p><a href="https://hahngyutak.github.io/posts/DCGAN/">DCGAN 논문 리뷰</a></p> <h4 id="321-generator-with-skip"><strong>3.2.1 Generator with skip</strong></h4> <p>image-to-image translation의 특징은 고해상도 input grid를 고해상도 output grid에 mapping한다는 것이다. 이러한 특징을 고려하여 이전 연구들에서는 <strong>Encoder-Decoder 네트워크 구조</strong>를 보통 사용하였다. 이 구조에서는 input이 여러 layer를 통과하며 downsampling된 후, upsampling된다. 이 과정에서 input-output간의 공유되는 정보에는 low-level 정보가 포함되어있지 않고 핵심 정보만이 남아있다. 즉 <strong>이미지의 detail에 대한 정보는 남아있지 않는 것</strong>이다.</p> <p><img src="/assets/img/post/pix2pix/figure3.png" alt="figure3"/></p> <p>이러한 병목현상을 방지하기 위해 <strong>초반 layer에 존재하는 low-level 정보를 network를 통해 직접 전달</strong>해야하는데, 바로 <strong>U-Net</strong>이다. U-Net은 $i$번째 layer와 $n-i$번째 layer사이에 skip connection을 추가한다.</p> <details> <summary>U-Net</summary> ​ <img src="https://joungheekim.github.io/img/in-post/2020/2020-09-28/model_structure.gif" alt="unet_archi"/> <a href="https://velog.io/@lighthouse97/UNet%EC%9D%98-%EC%9D%B4%ED%95%B4">U-net의 이해 </a> </details> <h4 id="322-markovian-discriminator-patchgan"><strong>3.2.2 Markovian discriminator (PatchGAN)</strong></h4> <p>뒤에 나올 Figure 4를 보면, <strong>L1 loss</strong>를 사용한 모델 sample에 많은 blur를 확인할 수 있다. 이는 high-frequency, 즉 이미지의 edge부분의 선명도 향상에는 도움이 되지 않지만 <strong>low-frequency, 즉 배경이나 텍스쳐 부분의 정확성을 강화</strong>한다.</p> <blockquote> <p>이미지에서의 High-frequency(고주파), Low-frequency(저주파)는 픽셀 변화 정도에 따라 나누어진다. High-frequency(고주파)는 픽셀값의 변화가 큰 부분, 즉 사물의 edge나 corner 등에 해당한다. Low-frequency(저주파)는 픽셀값의 변화가 작은 일반적인 배경이나 텍스쳐 등에 해당한다.</p> </blockquote> <p>논문에서는 <strong>$D$가 high-frequency 구조만을 modeling하도록 제한</strong>하기 위해 판별에 사용할 정보를 local image patches로 제한하며 이를 PatchGAN으로 부른다.</p> <blockquote> <p>기존 GAN은 이미지의 전체를 보고 $D$가 판단한다. 즉 전체적인 이미지만 진짜처럼 만들고 detail한 부분을 신경쓰지 않아도 된다는 것이다.</p> <p>앞에서 L1 loss가 low-frequency에 대한 정확성을 강화한다고하였듯이, low-frequency 부분에 대한 판단은 L1에게 맡기고 cGAN이 high-frequency에 대한 부분을 맡기 위함이다.</p> <p>이미지의 detail한 부분을 파악하기 위해서는 low-frequency 부분을 필요없기 때문에 $D$가 high-frequency에 대한 높은 정확성을 가지도록 PatchGAN을 사용한다.</p> </blockquote> <p><img src="/assets/img/post/pix2pix/patchGAN.png" alt="patchGAN"/></p> <p><strong>PatchGAN의 $D$는 이미지를 $N \times N$개의 patch로 나눈 뒤, 각 patch를 real or fake로 분류</strong>한다. 모든 patch에 대한 Output의 평균을 구하여 최종 분류결과를 도출한다. PatchGAN은 파라미터의 크기가 적고, 빠르며 어떤 큰 이미지에도 적용할 수 있다는 장점을 가진다.</p> <h3 id="33-optimization-and-inference"><strong>3.3 Optimization and Inference</strong></h3> <p>2014년에 발표된 GAN 논문에서는 $D$를 $k$ step 업데이트 후, $G$를 1 step 업데이트한다. 또한 $\log \;(1-D(x, G(x,z)))$를 minimize한 대신, $\log \;(D(x,G(x, z)))$를 maximize하는 방향으로 $G$를 train한다.</p> <p>Pix2pix는 위 GAN의 optimizing 정책을 사용한다. <strong>mini-batch stochastic gradient descent(mini-batch SGD)</strong>와 <strong>Adam</strong>을 사용하며 <strong>learning rate는 0.0002</strong>, momentum parameter는 $\mathbf{\beta_1 = 0.5}$, $\mathbf{\beta_2 = 0.999}$이다.</p> <h2 id="experiments"><strong>Experiments</strong></h2> <h3 id="analysis-of-the-objective-function"><strong>Analysis of the objective function</strong></h3> <p>Eq. 4에서 어느 요소가 중요한지 알아보기 위해 논문에서는 각 항(L1, cGAN에 대한 loss)의 제거 실험을 진행하였다.</p> <p><img src="/assets/img/post/pix2pix/figure4.png" alt="figure4"/></p> <p>그림 4에서 볼 수 있듯이, L1만 사용하면 흐릿한 결과를 얻으며 $\lambda = 0$으로 하여 cGAN만 사용할 때에는 훨씬 선명한 결과를 얻지만 특정 상황에서 시각적인 왜곡이 발생한다. $\lambda = 100$일때 두 항을 모두 사용하면 이러한 왜곡이 줄어든다.</p> <p><img src="/assets/img/post/pix2pix/table1.png" alt="table1" style="zoom:67%;"/></p> <p>표 1을 봐도 L1 + cGAN의 성능이 가장 놓은 것을 알 수 있다.</p> <h3 id="analysis-of-the-generator-architecture"><strong>Analysis of the generator architecture</strong></h3> <p><img src="/assets/img/post/pix2pix/figure5.png" alt="figure5"/></p> <p>U-Net 구조와 L1+cGAN를 함께 사용한 모델이 생성한 sample의 품질이 가장 높다는 것을 알 수 있다.</p> <h3 id="from-pixelgans-to-patchgans-to-imagegans"><strong>From PixelGANs to PatchGANs to ImageGANs</strong></h3> <p><img src="/assets/img/post/pix2pix/figure6.png" alt="figure6"/></p> <p>Figure 6을 통해 PatchGAN의 Patch size에 따른 성능을 알 수 있다. 이미지를 많이 분할할 수록 high-frequency에 해당하는 부분, 즉 edge와 같은 detail적인 부분이 향상된다.</p> <p>여기서 $1 \times 1$에 해당하는 모델은 PixelGAN이라 하며 full image인 $286 \times 286$에 해당하는 모델은 ImageGAN이라 한다.</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <ul> <li>pix2pix는 다양한 image-to-image translation에 적용할 수 있는 일반적인 프레임워크를 제공한다.</li> <li>pix2pix는 다양한 image-to-image translation에서 높은 성능을 보여준다. 이는 GANs의 효과적인 적용을 통해 가능하게 되었으며, 이는 GANs의 강력함을 입증하는 사례로 작용되었다.</li> </ul> <p><img src="/assets/img/post/pix2pix/figure14.png" alt="figure14"/></p> <p><img src="/assets/img/post/pix2pix/figure15.png" alt="figure15"/></p> <p><img src="/assets/img/post/pix2pix/figure16.png" alt="figure16"/></p> <blockquote> <p>사람이 그린 detail한 스케치로도 사실적인 sample을 생성할 수 있다.</p> </blockquote> <h2 id="code-review"><strong>Code Review</strong></h2> <p><a href="https://colab.research.google.com/drive/1pVfVmviZ3y8hAFA4ozvD8db8CRWnGD_0?usp=sharing">Pix2pix - Google Colab</a></p>]]></content><author><name></name></author><category term="Paper Review"/><category term="Computer Vision"/><category term="Generative model"/><category term="GAN"/><category term="AI"/><category term="generative"/><summary type="html"><![CDATA[Pix2Pix 논문 리뷰]]></summary></entry></feed>