<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [Paper Review] StarGAN | Gyutak Hahn </title> <meta name="author" content="Gyutak Hahn"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="AI, ms"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hahngt.github.io/blog/2023/StarGAN/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "[Paper Review] StarGAN",
            "description": "",
            "published": "April 30, 2023",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gyutak</span> Hahn </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="https://hahngyutak.github.io/index.html" rel="external nofollow noopener" target="_blank">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>[Paper Review] StarGAN</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#"></a> </div> </nav> </d-contents> <p><a href="https://github.com/HahnGyuTak/Thesis-Review/blob/main/StarGAN/Code.md" rel="external nofollow noopener" target="_blank">About Code</a></p> <p><a href="https://github.com/HahnGyuTak/Thesis-Review/blob/main/StarGAN/Review_EN.md" rel="external nofollow noopener" target="_blank">English Ver</a></p> <h2 id="abstract">Abstract</h2> <ul> <li>최근 연구들은 2개의 도메인에서 이미지 변환에 효과적인 성능을 보였지만, 3개 이상의 도메인을 다루는 접근에서는 한계를 보였다.</li> <li>이를 해결하기 위해하나의 모델을 사용하며 다중 도메인에 대한 이미지 변환을 수행하는 StarGAN을 제안한다.</li> <li>하나의 네트워크에 있는 여러 도메인에, 여러 데이터셋(RaFD, CelebA)을 학습시킴</li> <li>이로써 입력 이미지를 원하는 도메인으로의 유연한 변환을 수행</li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li>다른 두 도메인의 학습데이터로 a 에서 b로 변환하는 학습을 수행</li> <li>이미지에서 여러 도메인 추출(머리색, 성별, 나이 등) <ul> <li>CelebA : 머리색, 성별, 나이 등을 추출</li> <li>RaFD : 행복, 분노, 슬픔 등과 같은 감정 도메인 추출</li> </ul> </li> <li>RaFD로 학습하여 CelebA 이미지에 표정을 바꾸기 위해 두 데이터셋을 함께 학습</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219863839-0c3d675b-a9dd-43a1-ae0c-8789f3025ac9.jpeg" alt="IMG_E790BB5F1968-1" width="700"></p> <ul> <li>기존의 모델은 여러 도메인 간의 변환이 비효과적이며 비효율적이었다. <ul> <li>k개의 도메인이 존재할 경우 k(k-1)개의 generator를 학습시켜야 했기 때문</li> <li>각 generator는 전체 데이터셋을 모두 활용하지 못함</li> </ul> </li> <li>StarGAN은 하나의 generatordp 여러 도메인 간의 매핑 즉, 연결 자체를 학습 <ul> <li>2개의 이미지와 도메인 정보를 입력하면 대응하는 도메인간의 변환을 학습</li> <li>필요없는 레이블은 무시, 데이터셋에서 추출한 특정 레이블만 학습</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>GAN (Generative Adversarial Networks)</li> <li>Conditional GANs</li> <li>Img-to-Img Translation (CycleGAN) <ul> <li>하나의 모델에 두 도메인 간의 관계만 학습</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="1-muti-domain-image-to-image-translation">1. Muti-Domain Image-to-Image Translation</h3> <ul> <li>다중 도메인 간 매핑을 하나의 G(generator)에 학습시키는 것이 목표</li> <li>입력 이미지 x, 출력 이미지 y, 목표 도메인 레이블 c : [G(x, c) -&gt; y]</li> <li>c 를 랜덤으로 생성해서 G가 유연하게 이미지를 변환할 수 있음</li> <li>하나의 D(discriminator)가 여러 도메인을 control할 수 있도록 보조적인 분류기가 존재</li> <li>D : x → {Dsrc(x), Dcls(x)}.</li> </ul> <h4 id="adversarial-loss">Adversarial Loss</h4> <p><img src="https://user-images.githubusercontent.com/50629765/219863865-89884597-4d1c-4371-988f-08c97afbf0e9.jpeg" alt="IMG_57FC6DAEAC7A-1" width="300"></p> <ul> <li>G는 loss 를 최소화하려하고, D는 이를 최대화하려고 한다.</li> </ul> <h4 id="domain-classification-loss">Domain Classification Loss</h4> <ul> <li>G(x,c)로 생성된 이미지가 c로 분류되도록 하기 위해, D 맨 위에 classifier 추가</li> <li>real Img의 도메인 분류 손실을 사용하여 D를 최적화</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219863884-b591a08f-989c-460a-b5c0-6a2e04da097b.jpeg" alt="IMG_DD09A5BA8DC8-1" width="300"></p> <ul> <li>real Img의 도메인 분류 손실을 사용하여 D를 최적화</li> <li>D cls(c′|x)는 D가 계산한 도메인 레이블의 확률 분포</li> <li>D 는 이 loss 를 최소화함으로서 (real Img인 x, 레이블 c’)을 분류하는것을 학습</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219863889-f1be11ce-18cd-4b66-a968-66573beb0be1.jpeg" alt="IMG_F4ECA0F27C1F-1" width="300"></p> <ul> <li>G는 생성한 이미지가 target 도메인인 c로 분류되게끔 하기 위해 이 loss를 최소화한다.</li> </ul> <h4 id="reconstruction-loss">Reconstruction Loss</h4> <ul> <li>위 loss를 최소화 한다 해도, 입력된 이미지에서 변환된 도메인을 제외하고 보존하는 것에 어려움이 있음</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219863895-5c768829-1131-4f4d-9a04-8c595500e1f9.jpeg" alt="IMG_72F4C97DFD50-1" width="300"></p> <ul> <li>cycle consistency(순환 일관성) loss 적용</li> <li>G에 변환된 이미지 G(x, c)와 원본 도메인 c’을 입력 → 원본이미지 재구성</li> <li>총 G를 2번 사용 (원본 → 변환된 이미지 → 원본 재구성)</li> </ul> <h4 id="full-objective">Full Objective</h4> <ul> <li>optimize Fuctions of G and D</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219863898-35a4e46b-de00-4192-b902-60bce54c374a.jpeg" alt="IMG_4F814845022B-1" width="300"></p> <ul> <li>λcls 와 λrec 는 분류, 재구성 손실에 영향을 주는 하이퍼 파라미터</li> <li>λcls 는 1, λrec 는 10 사용</li> </ul> <h3 id="2-training-with-multiple-datasets">2. Training with Multiple Datasets</h3> <ul> <li>CelebA에는 머리색과 같은 속성이, RaFD에는 표정 속성이 있음</li> <li>이와 같은 다중 데이터셋에서는 G(x, c)에서 재구성할 때 필요한 c’레이블이 필요하기 때문에 문제가 생김</li> </ul> <h4 id="mask-vector">Mask Vector</h4> <ul> <li>make vector를 도입, 불특정한 레이블을 무시하고 명시된 레이블에 집중</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/219866136-663ceab6-d40f-4080-a6fe-cc7df648216e.jpeg" alt="mask_vecotr" width="300"></p> <ul> <li>c는 i번째 dataset의 레이블 벡터를 의미</li> </ul> <h4 id="training-strategy">Training Strategy</h4> <ul> <li>다중 dataset을 학습할 때, domain 레이블은 mask vector로 정의하여 G에 입력</li> <li>G의 구조는 하나의 dataset으로 학습하는 것과 다를바 없음</li> <li>모든 dataset의 확률분포를 만들기 위해 D의 보조 분류기를 확장</li> <li>D가 특정 레이블에 대한 classification error를 최소하하는 과정을 통해 학습 <ul> <li>CelebA 이미지로 학습을 진행할 때, D는 CelebA에 존재하는 label의 classification loss를 최소화</li> </ul> </li> <li>RaFD와 CelebA를 교대로 학습 진행 -&gt; 모든 레이블 학습</li> </ul> <h2 id="implementation">Implementation</h2> <h3 id="imporve-gan-training">Imporve GAN Training</h3> <p><img src="https://user-images.githubusercontent.com/50629765/224294892-b999b941-b5e9-4999-9484-a957a639e620.png" alt="image" width="300"></p> <ul> <li>위 수식으로 정의된 gradient penalty를 사용하는 Wasserstein GAN 사용</li> </ul> <h3 id="network-acchitecture">Network Acchitecture</h3> <ul> <li>CycleGAN에서 채택된 StarGAN의 Generator 네트워크 구성 <ul> <li>convolutional layer (stride size : 2) -&gt; downsampling</li> <li>Residual Block of 6</li> <li>transposed convolutional layer (stride size : 2) -&gt; upsampling</li> </ul> </li> <li>G에만 표준화 적용</li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="1-baseline-models">1. Baseline Models</h3> <ul> <li>DIAT - image-to-image transform (2 domain) <ul> <li>두 domain Img인 X, Y의 매핑을 학습</li> <li>|x - F(G(x))| 로 매핑한 정규화를 통해 원본 Img의 특징 보존</li> </ul> </li> <li>CycleGAN - image-to-image transform (2 domain) <ul> <li>G(x)를 다시 G에 입력하여, 원본 이미지와의 손실 측정</li> </ul> </li> <li>IcGAN - cGAN에 기인 <ul> <li>매핑 G : {z, c} → x 에서 역매핑 Ez : x → z 및 Ec : x → c 역시 학습</li> <li>잠재 벡터를 보존하면서 조건 벡터만 변경하여 이미지를 합성</li> </ul> </li> </ul> <h3 id="2-datasets">2. Datasets</h3> <ul> <li>CelebA <ul> <li>유명인사의 얼굴 데이터의 7가지 domain</li> <li>머리색 (black, blond, brown), 성별, 나이</li> </ul> </li> <li>RaFD <ul> <li>참여자들의 8가지 표정 데이터 수집</li> </ul> </li> </ul> <h3 id="3-training">3. Training</h3> <ul> <li>Optimizer : Adam</li> <li>batch size : 16</li> <li>CelebA <ul> <li>learning rate : (epoch) (0 ~ 10) 0.0001, (11 ~ ) 0까지 점차 감소</li> </ul> </li> <li>RaFD <ul> <li>learning rate : (epoch) (0 ~ 100) 0.0001, (11 ~ ) 0까지 점차 감소</li> </ul> </li> <li>GPU : NVIDIA Tesla M40</li> </ul> <h3 id="4-experimental-results-on-celeba">4. Experimental Results on CelebA</h3> <ul> <li>DIAT, CycleGAN과 같은 교차 도메인 모델을 label의 모든 쌍을 학습시킴</li> <li>평가 <ul> <li>고정된 변환을 학습 하는 대신, StarGAN은 target 도메인에 대한 label에 따라 유연하게 변환할 수 있도록 학습</li> <li>다른 r교차 도메인들 보다 성능 Good</li> <li>IcGAN보다 얼굴의 정체성을 유지 <ul> <li>합성곱 layer에서 활성화된 맵을 latent representation 로 저장하기 때문</li> <li>latent repesentation : 이미지에서 발견된 특징들을 나타내는 숫자 배열</li> </ul> </li> </ul> </li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/224294980-cb4ed0b4-e434-4439-a4da-daeacbf76501.jpeg" alt="IMG_78CC5B591D7C-1" width="600"> <img src="https://user-images.githubusercontent.com/50629765/224295011-bb1cfb6f-a118-4d86-8450-319729932c8e.jpeg" alt="IMG_ADAA2C15EAE7-1" width="600"></p> <h3 id="5-experimental-results-on-rafd">5. Experimental Results on RaFD</h3> <ul> <li>무표정을 입력으로 RaFD의 여러 표정들을 학습</li> <li>각 도메인 당 약 500장의 데이터</li> <li>평가 <ul> <li>StarGAN 모델이 가장 자연스러운 표정 생성</li> <li>DIAT와 CycleGAN은 입력 이미지의 정체성은 유지, 하지만 선명도가 저하</li> <li>IcGAN은 정체성 유지조차 실패</li> <li>Why? <ul> <li>타 모델은 2 domain학습시 1000개의 데이터 학습</li> <li>StarGAN은 모든 domain의 데이터 약 4000개 사용</li> </ul> </li> </ul> </li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/224295058-2a43d1ef-60eb-4af9-a95a-e8f3dbadc859.jpeg" alt="IMG_828603DA420E-1" width="600"></p> <ul> <li>RaFD로 분류기 학습 후, 생성된 이미지 분류 결과</li> </ul> <p><img src="https://user-images.githubusercontent.com/50629765/224295099-5e4b4a1d-7fa9-47b2-8089-9f2934bfc68f.jpeg" alt="RaFD cls" width="600"></p> <ul> <li>StarGAN이 classification error 가 가장 작음</li> <li>학습에 필요한 parameter가 타 모델에 비해 현저히 작음 <ul> <li>-&gt; StarGAN은 단 한쌍의 (G, D)를 사용하기 때문</li> </ul> </li> </ul> <h3 id="6-experimental-results-on-celeba--rafd">6. Experimental Results on CelebA + RaFD</h3> <p><img src="https://user-images.githubusercontent.com/50629765/224295184-4d2ad2c0-bd7f-4449-8965-c2edcc81b9f4.jpeg" alt="IMG_0D6FA5D07ACF-1" width="600"></p> <ul> <li>CelebA 와 RaFD 데이터셋 둘 다 사용 (Multiple datasets) (with mask vector)</li> <li>JNT(jointly train), SNG(single train)</li> <li>joint training의 효과 <ul> <li>JNT가 더 선명한 품질의 표정을 생성</li> <li>SNG는 CelebA의 image translation을 학습 X 이기 때문</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gyutak Hahn. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="https://hahngyutak.github.io/index.html"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-paper-review-react-learning-customized-visual-models-with-retrieval-augmented-knowledge",title:"[Paper Review] REACT : Learning Customized Visual Models with Retrieval-Augmented Knowledge",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ReAct/"}},{id:"post-paper-review-promptstyler",title:"[Paper Review] PromptStyler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/PromptStyler/"}},{id:"post-paper-review-dreamfusion",title:"[Paper Review] DreamFusion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DreamFusion/"}},{id:"post-paper-review-dreambooth",title:"[Paper Review] DreamBooth",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DreamBooth/"}},{id:"post-paper-review-tada-timestep-awara-data-augmentation-for-diffusion-models",title:"[Paper Review] TADA : Timestep-Awara Data Augmentation for Diffusion models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/TADA/"}},{id:"post-paper-review-sagan-self-attention-gan",title:"[Paper Review] SAGAN - Self Attention GAN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/SAGAN/"}},{id:"post-paper-review-score-based-generative-model",title:"[Paper Review] Score-based Generative model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Score_based_model/"}},{id:"post-paper-review-wassersteingan",title:"[Paper Review] WassersteinGAN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/WassersteinGAN/"}},{id:"post-paper-review-diffusion-models-beat-gans-on-image-synthesis",title:"[Paper Review] Diffusion Models Beat GANs on Image Synthesis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Diffusion-Models-Beat-GANs/"}},{id:"post-paper-review-pix2pix",title:"[Paper Review] Pix2pix",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Pix2pix/"}},{id:"post-paper-review-ddim",title:"[Paper Review] DDIM",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DDIM/"}},{id:"post-paper-review-dcgan",title:"[Paper Review] DCGAN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DCGAN/"}},{id:"post-paper-review-ddpm",title:"[Paper Review] DDPM",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DDPM/"}},{id:"post-paper-review-gan",title:"[Paper Review] GAN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/GAN/"}},{id:"post-paper-review-transfomer-attention-is-all-you-need",title:"[Paper Review] Transfomer - Attention Is All You Need",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/Transformer/"}},{id:"post-\uac1c\ub150-\uc815\ub9ac-vae",title:"[\uac1c\ub150 \uc815\ub9ac] VAE",description:"vae",section:"Posts",handler:()=>{window.location.href="/blog/2023/VAE/"}},{id:"post-paper-review-stargan",title:"[Paper Review] StarGAN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/StarGAN/"}},{id:"post-\uac1c\ub150-\uc815\ub9ac-wassersteingan",title:"[\uac1c\ub150 \uc815\ub9ac] WassersteinGAN",description:"Summary of mathematical concepts in wGAN",section:"Posts",handler:()=>{window.location.href="/blog/2023/wGAN/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%67%75%65%37%30%37@%6E%61%76%65%72.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HahnGyuTak","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/imtxxk","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>