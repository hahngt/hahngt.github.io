<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="promptstyler-논문-리뷰"><strong>PromptStyler 논문 리뷰</strong></h1> <blockquote> <p>제목 : <strong>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</strong></p> <p>저자 : <a href="https://jhcho99.github.io/" rel="external nofollow noopener" target="_blank">Junhyeong Cho</a>, <a href="https://scholar.google.com/citations?user=7PSBLtIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Gilhyun Nam</a>, <a href="https://cvlab.postech.ac.kr/~sungyeon/" rel="external nofollow noopener" target="_blank">Sungyeon Kim</a>, <a href="https://scholar.google.co.kr/citations?user=mDxJj2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hunmin Yang</a>, <a href="https://suhakwak.github.io/" rel="external nofollow noopener" target="_blank">Suha Kwak</a></p> <p>링크 : <a href="https://arxiv.org/abs/2307.15199" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://promptstyler.github.io/" rel="external nofollow noopener" target="_blank">Project Page</a></p> </blockquote> <p>Vision-Language 공간(space)에서 두 모달리티 간의 전이성(transferability)이 존재한다는 것이 최근 연구를 통해 발견되었다. <span style=" background-color: #F7DDBE"><b>이 논문은 이미지를 사용하지 않고, 오직 prompt만 사용하여 Style 변환을 수행하는 PromptStyler를 제안한다.</b></span> 이는 pseudo-words(의사 단어)를 Styler word vector로 표현한 후, Space의 분포 변화를 시뮬레이션하여 다양한 스타일을 생성할 수 있다. 저자들은 이를 통해 이미지 학습 없이 다양한 데이터셋에서 Style 변환을 이루었다고 말한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Domain Adaptation은 Train, Test 데이터 간의 분포 변화가 클 때(<em>distribution shifts</em>), target domain에 모델을 적응시켜 성능을 높인다. 하지만 완전히 새로운 domain에 대해서는 성능이 감소하기 때문에, Domain Generalization이라는 다양한 domain에 대해 모델을 일반화하는 연구가 진행되고 있지만 여러 한계에 부딫힌다.</p> <p>논문에서는 “<strong>source domain 데이터 없이 모델의 latent space에서 다양한 분포 변화를 시뮬레이션하여 Domain Generalization를 효과적으로 수행할 수 있을지?</strong>“에 대한 질문을 던진다.</p> <p>이 논문에서는 Large-scale Vision Language 모델에서 text가 joint vision-language space에서 관련 image를 대표할 수 있다는 점을 활용한다. 특히, cross-modal 전이 가능성을 보여준다.</p> <blockquote> <p>cross-modal 전이</p> <p>이 논문에서는 text feature로 Classifier를 train하고 image feature를 사용해 Classifier에서 inference하는 것을 말함.</p> <p>이 train process를 통해 source-free Domain Generalization을 수행할 수 있으며, image 없이 prompt(text)만을 통하여 다양한 분포 변화를 시뮬레이션 할 수 있다.</p> </blockquote> <p><img src="/assets/img/PromptStyler/figure1.png" alt="figure1" style="zoom:67%;"></p> <p>저자들은 <span style=" background-color: #F7DDBE"><b>learnable한 단어 vector를 통해 다양한 style을 합성할 수 있는 prompt 기반 Style 생성 기법인 PromptStyler</b></span>를 제안한다.</p> <p>OpenAI에서 개발한 Vision-Language 모델 CLIP을 사용하여 pseudoword $\mathit{S}_{\boldsymbol{\ast}}$에 대한 word vector(“a painting in the style of $\mathit{S}_{\boldsymbol{\ast}}$”)로 스타일을 포착할 수 있다고 한다.</p> <p><img src="/assets/img/PromptStyler/figure2.png" alt="figure2"></p> <p>PropmtStyler는 다음과 같은 특성이 있다.</p> <ol> <li> <strong>orthogonal style features</strong> : style feature들이 서로 영향을 주지 않으면서 독립적으로 구별될 수 있는 것 <ul> <li>style diversity(다양성)를 최대화</li> </ul> </li> <li> <strong>content consistency</strong> : 학습된 style이 정보를 왜곡하는 것을 방지 <ul> <li>style-content feature는 <strong>content prompt</strong>(“[class]”)로부터 얻은 content feature가 다른 feature들 보다 더 가까이 위치하도록 강제</li> <li>style-contents feature는 style-content prompt(“a $\mathit{S}_{\boldsymbol{\ast}}$ style of a [class]”)에서 얻어짐</li> </ul> </li> </ol> <p><strong>학습된 style word vector는 style-content feature를 합성하여 Classifier를 Train</strong>시킨다. 이 feature은 joint space에서 “알고있는” content 이미지를 “알려지지 않은” style로 시뮬레이션할 수 있다.</p> <p>Linear Classifier은 [style-content feature, content(“[class]”)] 쌍 데이터로 학습된다.</p> <blockquote> <p>[style-content feature, content(“[class]”)] == [input, label]</p> </blockquote> <p>inference 절차는 다음과 같다.</p> <ol> <li>Image Encoder가 Input 이미지에서 feature 추출</li> <li>추출된 feature를 학습된 Classifier에 입력</li> </ol> <blockquote> <p>Pretrained Vision-Language 모델에서 사용한 Text/Image Encoder를 사용.</p> <p>Text Encoder는 Train, Image Encoder는 Inference에 사용</p> </blockquote> <p>논문에서 제안한 모델은 CLIP에 비해 경량화 되었지만, 더 빠른 Inference 속도를 보여줬다고 말한다.</p> <h2 id="related-work"><strong>Related Work</strong></h2> <h4 id="domain-generalization">Domain Generalization</h4> <p>source 및 target domain간의 분포 변화로 인한 신경망의 성능 저하를 방지하기 위함이다.</p> <p>이에는 2가지 방법이 있다.</p> <ol> <li>multi-source DG <ul> <li>다양한 source로 모델이 다양한 domain의 특징을 학습</li> <li>새로운 domain에 대해서도 잘 일반화</li> </ul> </li> <li>single-source DG <ul> <li>하나의 source만을 사용하지만, 증강 기법 등을 통해 다양한 도메인을 생성</li> <li>multi-source DG의 효과를 기대</li> </ul> </li> </ol> <h4 id="source-free-domain-generalization">Source-free Domain Generalization</h4> <p>source 및 target domain없이 새로운 domain을 합성하여 모델의 일반화 능력을 향상시키는 방법으로, 논문에서 제시하는 새로운 기법이다.</p> <h4 id="joint-vision-language-space">Joint vision-language space</h4> <p>논문에서는 image-text 쌍으로 학습된 Vision-language 모델에서, Joint vision-language space를 활용한다. 이 space에서 prompt(“a painting in the style of $\mathit{S}_{\boldsymbol{\ast}}$”)를 사용하여 시각적 특징을 조작하고 다양한 분포 변화를 시뮬레이션(다양한 style)할 수 있음을 말한다.</p> <h2 id="method"><strong>Method</strong></h2> <p><img src="/assets/img/PromptStyler/figure3.png" alt="figure3"></p> <ul> <li>Large Vision-Language 모델인 CLIP 사용</li> <li>CLIP의 Image Encoder, Text Encoder를 사용하며 Framework에서 고정되어있음</li> </ul> <h3 id="1-prompt-driven-style-generation"><strong>1. Prompt-driven style generation</strong></h3> <p>Input prompt는 tokenizaiton process를 통해 여러 token으로 변환되며, 각 token은 word lookup process를 통해 word vector로 바뀐다.</p> <p>그 중, pseudo-word $\mathit{S}_{\boldsymbol{\ast}}$는 word lookup process에서 style word vector인 $s_i \in \mathbb{R}^D$로 변환된다.</p> <p>논문에서는 3가지 prompt가 사용된다고 한다.</p> <ul> <li> <p><strong>Style prompt $\mathcal{P}^{\text{style}}_i$</strong> : “a $\mathit{S}_i$ style of a”</p> </li> <li> <p><strong>Content prompt $\mathcal{P}^{\text{content}}_m$</strong> : “$[\text{class}]_m$”</p> </li> <li> <p><strong>Style-content prompt $\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m$</strong> : “a $\mathit{S}_i$ style fo a $[\text{class}]_m$”</p> </li> </ul> <blockquote> <p>$\mathit{S}_i$ : $i$번째 style word vector</p> <ul> <li>$K$개의 스타일을 학습하려면 $K$개의 style word vector인 $\lbrace s_i \rbrace_{i=1}^K$를 학습</li> </ul> <p>$[\text{class}]_m$ : $m$번째 class label</p> </blockquote> <p>저자들은 2가지 Loss 식을 제안한다.</p> <h4 id="style-diversity-loss"><strong>Style diversity Loss</strong></h4> <p>Joint vision-language space에서 Style diversity를 극대화하기 위해 저자들은 style word vector $\lbrace s_i \rbrace_{i=1}^K$를 순차적으로 학습한다.</p> <p>$i$번째 style vector $s_i$가 생성하는 feature를 $T(\mathcal{P}^{\text{style}}_i) \in \mathbb{R}^C$라 하고, 이전에 $1 \sim (i-1)$까지 $\lbrace s_j \rbrace_{j=1}^{i-1}$가 생성한 feature를 $\lbrace T(\mathcal{P}^{\text{style}}_j) \rbrace_{j=1}^{i-1}$라 하자.</p> <p>$T(\mathcal{P}^{\text{style}}_i)$는 이전에 생성되었던 feature들인 $\lbrace T(\mathcal{P}^{\text{style}}_j) \rbrace_{j=1}^{i-1}$에 직교하게 생성된다.</p> <p>즉 $i$번째 style vector를 학습하기 위한 <strong>Style diversity Loss</strong> $\mathcal{L}_{\text{style}}$는 다음과 같다.</p> \[\mathcal{L}_{\text{style}} = \frac{1}{i-1} \sum_{j=1}^{i-1}\left \vert\frac{T(\mathcal{P}^{\text{style}}_i)}{\Vert T(\mathcal{P}^{\text{style}}_i)\Vert_2} \bullet \frac{T(\mathcal{P}^{\text{style}}_j)}{\Vert T(\mathcal{P}^{\text{style}}_j)\Vert_2} \right \vert \tag{1}\] <p>위 식은 $i$번째 style feature와 기존 style feature의 cosine 유사성을 minimize하는 것을 목표로 한다.</p> <blockquote> <p>cosine 유사성이 0이 되면, 직교하게 생성되었다는 의미이다.</p> </blockquote> <h4 id="content-consistency-loss"><strong>Content consistency Loss</strong></h4> <p>위 style diversity loss만을 사용할 때 학습된 style인 $s_i$가 style-content feature $T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)$를 생성할 때, <u>content 정보를 많이 왜곡</u>한다고 한다.</p> <p>이는 style-content feature의 content 정보가 $i$번째 style vector $s_i$를 학습하는 동안 content feature $\mathcal{P}^{\text{content}}_m \in \mathbb{R}^C$와 일관성을 가지게 하여 해결할 수 있다.</p> <p>​ ⇨ $i$번째 style-content feature $T(\mathcal{P}^{\text{style}}_i)$는 해당 content feature $\mathcal{P}^{\text{content}}_m$와 높은 cosine 유사도를 가지게 한다.</p> <p>$i$번째 style vector $s_i$의 경우,</p> <p>($m$번째 class label을 가진 <strong>style-content feature</strong>) - ($n$번째 class label을 가진 <strong>content feature</strong>) 사이의 <b>cosine 유사도 점수인 $z_{imn}$</b>은 다음과 같다.</p> \[z_{imn} = \frac{T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)}{\Vert T(\mathcal{P}^{\text{style}}_i \circ \mathcal{P}^{\text{content}}_m)\Vert_2} \bullet \frac{T(\mathcal{P}^{\text{content}}_n)}{\Vert T(\mathcal{P}^{\text{content}}_n)\Vert_2} \tag{2}\] <p>위 $z_{imn}$을 사용하여 $s_i$를 학습하기 위한 content consistency Loss $\mathcal{L}_{\text{content}}$는 다음과 같다. ($N$은 class 수)</p> \[\mathcal{L}_{\text{content}} = -\frac{1}{N} \sum_{m=1}^N \log\left ( \frac{e^{z_{imm}}}{\sum_{n=1}^N e^{z_{imn}}} \right) \tag{3}\] <p>이 $\mathcal{L}_{\text{content}}$는 style-content feature가 content feature에 가깝게 위치하도록 유도하여 $s_i$가 content 정보를 보존하도록 한다.</p> <h4 id="total-prompt-loss"><strong>Total prompt Loss</strong></h4> <p>PromptStyler는 최종적으로 <strong>Style diversity Loss</strong>와 <strong>Content consistency Loss</strong>를 모두 사용하여 K개의 style word vector $\lbrace s_i \rbrace_{i=1}^K$를 순차적으로 학습한다.</p> \[\mathcal{L}_{\text{prompt}} = \mathcal{L}_{\text{style}} + \mathcal{L}_{\text{content}} \tag{4}\] <p>위 loss를 활용한 학습 <strong>Algorithm 1</strong>은 다음과 같다.</p> <p><img src="/assets/img/PromptStyler/algorithm1.png" alt="algorithm1" style="zoom:50%;"></p> <h3 id="2-training-a-linear-classifier-using-diverse-styles"><strong>2. Training a linear classifier using diverse styles</strong></h3> <p>앞에서 학습한 $s_i$를 이용하여 Linear Classifier를 학습하는 과정이다.</p> <ol> <li>$K$개의 style word vector $\lbrace s_i \rbrace_{i=1}^K$ 학습</li> <li>$KN$개의 style-content feature 생성 <ul> <li>학습된 $K$개의 style word vector와 $N$개의 class를 이용</li> <li> <u>Text encoder $T(\cdot)$</u> 사용</li> </ul> </li> <li>Linear classifier 학습 <ul> <li>$KN$개의 style-content feature과 class label을 사용</li> </ul> </li> </ol> <p>여기서 저자들은, Joint vision-language space를 활용하기 위해 <strong>ArcFace loss</strong>를 사용한다고 한다.</p> <blockquote> <p><strong><a href="https://arxiv.org/abs/1801.07698" rel="external nofollow noopener" target="_blank">ArcFace loss</a></strong>란?</p> <p>ArcFace는 얼굴 인식 작업을 위해 고안된 각도 기반 softmax loss function이다.</p> <p>Classifier의 input feature와 가중치 간의 cosine 유사도를 계산하고 class 간 추가적인 각도 margin penalty를 적용한다.</p> \[L_{ArcFace} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j=1, j\neq y_i}^n e^{s\cos(\theta_j)}}\] <p>각도 기반이기 때문에 Class 간 경계를 더 잘 형성하고 유클리드 거리 기반 손실 함수의 한계를 극복하기 위해 제안된 loss이다.</p> </blockquote> <h3 id="3-inference-using-the-trained-classifier"><strong>3. Inference using the trained classifier</strong></h3> <p>학습된 Classifier를 <u>Image Encoder $I(\cdot)$</u>과 함께 Inference하는 과정이다.</p> <ol> <li>vision-language space에 mapping된 <b>Image feature $I(\text{x}) \in \mathbb{R}^C$</b>를 추출 <ul> <li>$\textbf{x}$는 Input image</li> <li>Encoder $I(\cdot)$에서 이미지를 $\mathcal{l} _2$ 정규화</li> </ul> </li> <li>Classifer로 Class score를 생성 <ul> <li>위 image feature $I(\text{x})$를 사용</li> </ul> </li> </ol> <h2 id="experiments"><strong>Experiments</strong></h2> <h3 id="evaluation-datasets"><strong>Evaluation datasets</strong></h3> <p>Generalization 성능을 평가하기 위해, 데이터셋이 아닌 4가지 Domain Generalization benchmark를 사용한다.</p> <ul> <li>PACS (4개의 도메인과 7개의 클래스)</li> <li>VLCS (4개의 도메인과 5개의 클래스)</li> <li>OfficeHome (4개의 도메인과 65개의 클래스)</li> <li>DomainNet (6개의 도메인과 345개의 클래스)</li> </ul> <h3 id="implementation-details"><strong>Implementation details</strong></h3> <p>저자들은 RTX 3090 GPU에서 30분간 학습시키는 조건을 모두 동일하게 사용하였다고 한다.</p> <h4 id="architecture"><strong>Architecture</strong></h4> <p>pretrained Vision-Language 모델로 CLIP을 사용하였으며, Image Encoder $I(\cdot)$는 ResNet-50을, Text encoder $T(\cdot)$는 Tramsformer를 사용하였다.</p> <blockquote> <p>Image Encoder는 추가적인 비교를 위해 Vision Trasformer인 ViT-L, Vit-B도 사용</p> </blockquote> <h4 id="learning-style-word-vectors-s_i"> <strong>Learning style word vectors</strong> <b>$s_i$</b> </h4> <p>저자들은 $s_i$를 학습할 때, 다음과 같은 2가지 prompt learning 기법을 사용하였다고 한다.</p> <ol> <li> <a href="https://arxiv.org/abs/2109.01134" rel="external nofollow noopener" target="_blank">Learning to Prompt for Vision-Language Models</a> <ul> <li>거대 Vision-Language 모델인 CLIP을 전체 fine-tuning하는 것은 비효율적</li> <li>prompt를 도입하면 성능 향상에 효과적이지만 prompt engineering은 많은 시행착오가 필수적</li> <li> <strong><em>Context Optimization (CoOp)</em></strong>를 도입하여 <strong>learnable vector 가 있는 prompt 의 context words 를 모델링</strong> </li> </ul> </li> <li> <a href="https://arxiv.org/abs/2203.05557" rel="external nofollow noopener" target="_blank">Conditional Prompt Learning for Vision-Language Models</a> <ul> <li>학습용으로 label이 지정된 이미지 몇 개만 사용하는 CoOp는 학습되지 않은 class로 일반화 불가능</li> <li>conditional prompt learning를 도입하여 Input image에 따라 condition이 지정된 prompt를 만듬</li> <li>각 instance에 adaption되므로 class shift에 덜 민감하며, CoOp보다 domain generalization 성능이 향상됨</li> </ul> </li> </ol> <p>style word vectors <b>$s_i$</b>학습 조건은 다음과 같다.</p> <ul> <li>$\sigma = 0.02$인 zero-mean Gaussian 분포를 사용하여 $\lbrace s_i \rbrace_{i=1}^K$를 랜덤으로 초기화</li> <li>SGD optimizer : learning rate = 0.002, momentum = 0.9</li> <li>Total iteration = 100</li> </ul> <h4 id="training-a-linear-classifier"><strong>Training a linear classifier</strong></h4> <ul> <li>50 epochs</li> <li>SGD Optimizer : learning rate = 0.005, momentum = 0.9, batch size = 128</li> <li>ArcFace : scaling factor = 0, 각도 margin = 5</li> </ul> <h4 id="inference"><strong>Inference</strong></h4> <p>Input 이미지는 $224 \times 224$로 resize하고, 정규화를 진행하였다.</p> <h3 id="evaluations"><strong>Evaluations</strong></h3> <h4 id="main-result"><strong>Main Result</strong></h4> <p><img src="/assets/img/PromptStyler/table2.png" alt="table2" style="zoom:50%;"></p> <p>위 표를 보면 PromptStyler가 모든 benchmark에서 SOTA를 달성한 것을 알 수 있다.</p> <p>zero-shot CLIP을 제외한 기존 방법들은 source domain 데이터셋을 사용하였으며, zero-shot CLIP에서 사용한</p> <ol> <li>domain과 상관없는 prompt (“[class]”)</li> <li>domain별 prompt (“a photo of a [class]”)</li> </ol> <p>위 2가지 경우에도 PromptStyler가 더 좋은 성능을 보인다.</p> <p><strong>즉, latent space에서 prompt를 통해 다양한 분포 변화를 시뮬레이션함으로써 이미지를 사용하지 않고도 CLIP의 일반화 능력을 효과적으로 향상시킴을 알 수 있다.</strong></p> <h4 id="computational-evaluations-">**Computational evaluations. **</h4> <p>이 섹션에서는 parameter 수와 inference 속도를 비교한다. 비교 대상은 zero-shot CLIP과 PromptStyler이며, batch size를 1로 설정하고 단일 RTX 3090 GPU를 사용하였다.</p> <p><img src="/assets/img/PromptStyler/table3.png" alt="table3" style="zoom:50%;"></p> <p>PromptStyler는 inference할 때 Text Encoder를 사용하지않아 zero-shot CLIP에 보다 <strong>가볍고 빠르다</strong>는 것을 알 수 있다.</p> <h4 id="t-sne-visualization-results"><strong>t-SNE visualization results</strong></h4> <p>prompt loss에 사용된 $\mathcal{L}_{\text{style}}$과 $\mathcal{L}_{\text{content}}$의 성능을 VLCS benchmark를 통해 평가하고, t-SNE로 시각화한다.</p> <blockquote> <p><strong><a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="external nofollow noopener" target="_blank">Visualizing Data using t-SNE</a></strong></p> <p>t-SNE는 t-SNE는 2차원 또는 3차원 지도에 가지고 있는 데이터 포인트에 위치를 부여함으로서 이를 시각화할 수 있도록 해주는 방법론이다.</p> </blockquote> <p><img src="/assets/img/PromptStyler/figure4.png" alt="figure4" style="zoom:67%;"></p> <ul> <li> <p>(a) - $\mathcal{L}_{\text{style}}$ : 다른 class label(다른 모양들)과 유사한 feature를 공유(같은 색끼리 모여있음)</p> </li> <li>(b) - $\mathcal{L}_{\text{content}}$ : style-content feature의 다양성 하락</li> <li>(c) - $\mathcal{L}_{\text{style}} + \mathcal{L}_{\text{content}}$ : content 정보를 왜곡시키지 않으면서 다양한 스타일을 생성</li> </ul> <h4 id="text-to-image-synthesis-results"><strong>Text-to-Image synthesis results</strong></h4> <p><img src="/assets/img/PromptStyler/figure5.png" alt="figure5"></p> <p>위 사진은 “a <b>$\mathit{S}_{\boldsymbol{\ast}}$ </b>style of a <strong>cat</strong>“에서 추출된 style-content feature를 diffusers라는 라이브러리로 시각화한시킨 결과이다. 6개의 학습된 style word vector $s_i$를 사용하였다.</p> <h3 id="more-analyses"><strong>More analyses</strong></h3> <h4 id="loss">Loss</h4> <div class="image-container"> <img src="/assets/img/PromptStyler/table4.png" alt="table4" style="zoom:50%;"> <img src="/assets/img/PromptStyler/table5.png" alt="table5" style="zoom:50%;"> </div> <ul> <li>왼쪽 : Prompt Loss에 사용한 $\mathcal{L}_{\text{style}}$과 $\mathcal{L}_{\text{content}}$의 성능</li> <li>오른쪽 : Classifier에 사용한 ArcFace Loss의 성능</li> </ul> <h4 id="effect-of-the-number-of-styles"><strong>Effect of the number of styles</strong></h4> <p><img src="/assets/img/PromptStyler/figure6.png" alt="figure6"></p> <p>style 개수가 5개이상만 되어도 상당한 성능 향상을 보여준다.</p> <h4 id="effect-of-the-number-of-iterations-">**Effect of the number of iterations **</h4> <p><img src="/assets/img/PromptStyler/figure7.png" alt="figure7"></p> <p>iter가 20번이면 충분히 좋은 결과를 얻을 수 있음을 알 수 있다.</p> <h2 id="limitation"><strong>Limitation</strong></h2> <p>PromptStyler는 pretrained Vision-Language 모델의 joint vision-language space 성능에 의존한다. Terra Incognita 데이터셋의 경우, CLIP에서의 성능이 좋지 않기 때문에 PromptStyler에서도 성능이 감소하는 것을 확인하였다고 한다.</p> <h2 id="code"><strong>Code</strong></h2> <p><a href="https://colab.research.google.com/drive/1JDPvPufSxaj2f9T_VOY6tI4OhOtmWEFx?usp=sharing" rel="external nofollow noopener" target="_blank">Colab</a></p> </body></html>