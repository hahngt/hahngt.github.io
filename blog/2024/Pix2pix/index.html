<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="pix2pix-논문-리뷰"><strong>Pix2Pix 논문 리뷰</strong></h1> <p>Pix2pix는 Image-to-Image Translation을 수행하는 모델에 관한 연구로서, <strong>Conditional Generative Adversarial Network(CGAN)</strong>을 사용하여 하나의 structure을 이용한 이미지 간의 domain 변환을 수행하는 base model을 제공했다는 점에서 많은 인용 수를 자랑한다.</p> <p>이번 논문에서 제안하는 network는 <strong>input 이미지 → ouput 이미지로의 mapping</strong>을 학습할 뿐만 아니라 <strong>mapping train을 위한 loss function</strong>도 학습한다. 따라서 기존에는 서로 다른 loss식이 필요했던 문제에도 동일한 방식을 적용할 수 있다. 이번 논문은 더 이상 mapping function을 직접 설계하지 않으며, loss function 역시 직접 설계하지 않고도 합리적인 결과를 얻을 수 있음을 시사한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p><img src="/assets/img/post/pix2pix/figure1.png" alt="figure1"></p> <p>어떤 개념을 다양한 언어로 표현할 수 있는 것처럼, 이미지 역시 RGB, gradient field, edge map, sementic label map 등으로 표현할 수 있다. 위 figure 1처럼 이미지 간 translation은 충분한 train 데이터가 주어졌을 때 이미지에 내포된 domain 중 하나를 다른 domain으로 변환하는 작업으로 정의할 수 있다.</p> <p>Image-to-Image Translation 역시 loss function을 minimize하는 것을 목표로 하는 CNN을 사용하여 많은 연구가 이루어졌다. predict된 sample과 실제 데이터 사이의 <strong>Euclidean distance를 minimize하면 흐릿(blur)한 이미지를 얻는다.</strong> Euclidean distance는 그럴듯한(?) output을 모두 평균내어 계산하기 때문이다.</p> <p>이러한 문제를 해결하기 위해 이번 논문에서는 생성된 이미지가 fake인지 real인지 구별하는 <strong>GAN</strong>의 특성을 이용한다. blur 이미지는 fake로 분류될 것이고, GAN은 데이터에 맞는 loss function을 학습하기 때문에 다양한 loss가 필요한 task에 적용할 수 있기 때문이다.</p> <blockquote> <p>특히 특정 조건을 가진 이미지를 학습하기 때문에 Conditional GAN, 즉 cGAN을 사용한다. 원래의 GAN은 생성되는 sample이 어떤 label을 가지는지 조절할수 없었다. 이에 대한 가이드라인을 $D$와 $G$의 input으로 넣어줌으로써 원하는 label에 대한 output을 도출하게 만든 모델이 cGAN이다.</p> </blockquote> <h2 id="method"><strong>Method</strong></h2> <h3 id="31-objective"><strong>3.1 Objective</strong></h3> <p>cGAN은 observed image인 $x$와 noise vector $z$를 output인 $y$로의 mappping을 학습한다. (<strong>$G : ( x,z ) \to y$</strong>)</p> <p>cGAN의 Loss 식은 다음과 같다.</p> \[\underset{G}{\text{min}}\; \underset{D}{\text{max}}\;\mathrm{L}_{cGAN}(G, D) = \mathbb{E}_{x, y} \left[\log D(x, y) \right] + \mathbb{E}_{x, z} \left[1 - \log D(x, G(x, z)) \right] \tag {1}\] <p>또한 원래 GAN의 objective와 L2 distance를 결합하는 것이 좋다는 것을 발견하였고, blurring이 덜한 L1 distance를 사용한다.</p> \[\mathrm{L}_{L1}(G) = \mathbb{E}_{x,y,z} \left[\| y - G(x,y)\|_1 \right] \tag{3} \\\] <p>최종 Objective는 다음과 같다.</p> \[G^* = \text{arg}\;\underset{G}{\text{min}}\;\underset{D}{\text{max}}\; \mathrm{L}_{cGAN}(G, D)+ \lambda \mathrm{L}_{L1}(G) \tag{4}\] <blockquote> <p>pix2pix는 noise $z$를 사용하지 않는다. $G$가 이 noise를 무시하도록 학습하며, 이때문에 mapping할 때 stochastic하지 않고 deterministic해진다.</p> </blockquote> <h3 id="32-network-architecture"><strong>3.2 Network Architecture</strong></h3> <p>논문에서 제안하는 모델에서의 $G$와 $D$는 DCGAN(Deep Convolution GAN)을 베이스로 하였다. (Convolution, BatchNorm, ReLU 형식)</p> <p><a href="https://hahngyutak.github.io/posts/DCGAN/" rel="external nofollow noopener" target="_blank">DCGAN 논문 리뷰</a></p> <h4 id="321-generator-with-skip"><strong>3.2.1 Generator with skip</strong></h4> <p>image-to-image translation의 특징은 고해상도 input grid를 고해상도 output grid에 mapping한다는 것이다. 이러한 특징을 고려하여 이전 연구들에서는 <strong>Encoder-Decoder 네트워크 구조</strong>를 보통 사용하였다. 이 구조에서는 input이 여러 layer를 통과하며 downsampling된 후, upsampling된다. 이 과정에서 input-output간의 공유되는 정보에는 low-level 정보가 포함되어있지 않고 핵심 정보만이 남아있다. 즉 <strong>이미지의 detail에 대한 정보는 남아있지 않는 것</strong>이다.</p> <p><img src="/assets/img/post/pix2pix/figure3.png" alt="figure3"></p> <p>이러한 병목현상을 방지하기 위해 <strong>초반 layer에 존재하는 low-level 정보를 network를 통해 직접 전달</strong>해야하는데, 바로 <strong>U-Net</strong>이다. U-Net은 $i$번째 layer와 $n-i$번째 layer사이에 skip connection을 추가한다.</p> <details> <summary>U-Net</summary> ​ <img src="https://joungheekim.github.io/img/in-post/2020/2020-09-28/model_structure.gif" alt="unet_archi"> <a href="https://velog.io/@lighthouse97/UNet%EC%9D%98-%EC%9D%B4%ED%95%B4" rel="external nofollow noopener" target="_blank">U-net의 이해 </a> </details> <h4 id="322-markovian-discriminator-patchgan"><strong>3.2.2 Markovian discriminator (PatchGAN)</strong></h4> <p>뒤에 나올 Figure 4를 보면, <strong>L1 loss</strong>를 사용한 모델 sample에 많은 blur를 확인할 수 있다. 이는 high-frequency, 즉 이미지의 edge부분의 선명도 향상에는 도움이 되지 않지만 <strong>low-frequency, 즉 배경이나 텍스쳐 부분의 정확성을 강화</strong>한다.</p> <blockquote> <p>이미지에서의 High-frequency(고주파), Low-frequency(저주파)는 픽셀 변화 정도에 따라 나누어진다. High-frequency(고주파)는 픽셀값의 변화가 큰 부분, 즉 사물의 edge나 corner 등에 해당한다. Low-frequency(저주파)는 픽셀값의 변화가 작은 일반적인 배경이나 텍스쳐 등에 해당한다.</p> </blockquote> <p>논문에서는 <strong>$D$가 high-frequency 구조만을 modeling하도록 제한</strong>하기 위해 판별에 사용할 정보를 local image patches로 제한하며 이를 PatchGAN으로 부른다.</p> <blockquote> <p>기존 GAN은 이미지의 전체를 보고 $D$가 판단한다. 즉 전체적인 이미지만 진짜처럼 만들고 detail한 부분을 신경쓰지 않아도 된다는 것이다.</p> <p>앞에서 L1 loss가 low-frequency에 대한 정확성을 강화한다고하였듯이, low-frequency 부분에 대한 판단은 L1에게 맡기고 cGAN이 high-frequency에 대한 부분을 맡기 위함이다.</p> <p>이미지의 detail한 부분을 파악하기 위해서는 low-frequency 부분을 필요없기 때문에 $D$가 high-frequency에 대한 높은 정확성을 가지도록 PatchGAN을 사용한다.</p> </blockquote> <p><img src="/assets/img/post/pix2pix/patchGAN.png" alt="patchGAN"></p> <p><strong>PatchGAN의 $D$는 이미지를 $N \times N$개의 patch로 나눈 뒤, 각 patch를 real or fake로 분류</strong>한다. 모든 patch에 대한 Output의 평균을 구하여 최종 분류결과를 도출한다. PatchGAN은 파라미터의 크기가 적고, 빠르며 어떤 큰 이미지에도 적용할 수 있다는 장점을 가진다.</p> <h3 id="33-optimization-and-inference"><strong>3.3 Optimization and Inference</strong></h3> <p>2014년에 발표된 GAN 논문에서는 $D$를 $k$ step 업데이트 후, $G$를 1 step 업데이트한다. 또한 $\log \;(1-D(x, G(x,z)))$를 minimize한 대신, $\log \;(D(x,G(x, z)))$를 maximize하는 방향으로 $G$를 train한다.</p> <p>Pix2pix는 위 GAN의 optimizing 정책을 사용한다. <strong>mini-batch stochastic gradient descent(mini-batch SGD)</strong>와 <strong>Adam</strong>을 사용하며 <strong>learning rate는 0.0002</strong>, momentum parameter는 $\mathbf{\beta_1 = 0.5}$, $\mathbf{\beta_2 = 0.999}$이다.</p> <h2 id="experiments"><strong>Experiments</strong></h2> <h3 id="analysis-of-the-objective-function"><strong>Analysis of the objective function</strong></h3> <p>Eq. 4에서 어느 요소가 중요한지 알아보기 위해 논문에서는 각 항(L1, cGAN에 대한 loss)의 제거 실험을 진행하였다.</p> <p><img src="/assets/img/post/pix2pix/figure4.png" alt="figure4"></p> <p>그림 4에서 볼 수 있듯이, L1만 사용하면 흐릿한 결과를 얻으며 $\lambda = 0$으로 하여 cGAN만 사용할 때에는 훨씬 선명한 결과를 얻지만 특정 상황에서 시각적인 왜곡이 발생한다. $\lambda = 100$일때 두 항을 모두 사용하면 이러한 왜곡이 줄어든다.</p> <p><img src="/assets/img/post/pix2pix/table1.png" alt="table1" style="zoom:67%;"></p> <p>표 1을 봐도 L1 + cGAN의 성능이 가장 놓은 것을 알 수 있다.</p> <h3 id="analysis-of-the-generator-architecture"><strong>Analysis of the generator architecture</strong></h3> <p><img src="/assets/img/post/pix2pix/figure5.png" alt="figure5"></p> <p>U-Net 구조와 L1+cGAN를 함께 사용한 모델이 생성한 sample의 품질이 가장 높다는 것을 알 수 있다.</p> <h3 id="from-pixelgans-to-patchgans-to-imagegans"><strong>From PixelGANs to PatchGANs to ImageGANs</strong></h3> <p><img src="/assets/img/post/pix2pix/figure6.png" alt="figure6"></p> <p>Figure 6을 통해 PatchGAN의 Patch size에 따른 성능을 알 수 있다. 이미지를 많이 분할할 수록 high-frequency에 해당하는 부분, 즉 edge와 같은 detail적인 부분이 향상된다.</p> <p>여기서 $1 \times 1$에 해당하는 모델은 PixelGAN이라 하며 full image인 $286 \times 286$에 해당하는 모델은 ImageGAN이라 한다.</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <ul> <li>pix2pix는 다양한 image-to-image translation에 적용할 수 있는 일반적인 프레임워크를 제공한다.</li> <li>pix2pix는 다양한 image-to-image translation에서 높은 성능을 보여준다. 이는 GANs의 효과적인 적용을 통해 가능하게 되었으며, 이는 GANs의 강력함을 입증하는 사례로 작용되었다.</li> </ul> <p><img src="/assets/img/post/pix2pix/figure14.png" alt="figure14"></p> <p><img src="/assets/img/post/pix2pix/figure15.png" alt="figure15"></p> <p><img src="/assets/img/post/pix2pix/figure16.png" alt="figure16"></p> <blockquote> <p>사람이 그린 detail한 스케치로도 사실적인 sample을 생성할 수 있다.</p> </blockquote> <h2 id="code-review"><strong>Code Review</strong></h2> <p><a href="https://colab.research.google.com/drive/1pVfVmviZ3y8hAFA4ozvD8db8CRWnGD_0?usp=sharing" rel="external nofollow noopener" target="_blank">Pix2pix - Google Colab</a></p> </body></html>