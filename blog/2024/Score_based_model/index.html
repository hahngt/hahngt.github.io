<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="score-based-model-논문-리뷰"><strong>Score-based model 논문 리뷰</strong></h1> <p><a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">arxiv : Generative Modeling by Estimating Gradients of the Data Distribution</a></p> <p><a href="https://github.com/ermongroup/ncsn" rel="external nofollow noopener" target="_blank">github</a></p> <p>데이터의 score function, 즉 데이터 분포의 기울기를 직접 모델링하는 방식을 도입한 첫번째 논문이다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Generative AI는 최근 두 가지 방식이 주를 이루고 있지만, 각각의 한계점이 명확하게 존재하였다.</p> <ul> <li>Likelihood-based Model <ul> <li>log-likelihood를 Object function으로 사용</li> <li>정규화된 확률 모델을 만들기 위해 여러 방법을 사용해야한다. <ol> <li>특수한 구조를 사용해 데이터의 확률 분포를 직접 modeling : Autogressive models, flow models</li> <li>Surrogate loss (대리 손실 함수)를 사용해 원래의 목표 함수를 대신해서 최적화 : VAE</li> <li>Contrastive divergence를 사용하여 모델의 에너지 함수를 학습 : Energy-based</li> </ol> </li> </ul> </li> <li>Generative Adversarial Networks <ul> <li>경쟁적인 train 방식을 사용하여 모델과 데이터 간의 분포 차이를 minimize <ul> <li>분포 차이를 $f$-divergences라 하며, 그 종류로는 KL, JS 등이 있다.</li> </ul> </li> <li>위 Likelihood-based 모델의 한계를 보완하지만, Train 과정이 불안정</li> <li>GAN의 objective는 다른 GAN 모델을 평가할 수 없다.</li> </ul> </li> </ul> <p>논문에서는 log data density인 <strong>Score</strong>(Stein)을 추정하고 sampling하는 Generative 모델의 새로운 method를 탐구한다. 데이터로부터 log density 함수의 gradient를 학습하기 위해 <strong>score mathing</strong> 모델을 사용한다. 그런 다음, <strong>Langevin dynamics(랑주뱅 역학)</strong>을 사용하여 sample을 생성하며, 이는 초기 sample을 Score에 따라 점차 density가 높은 역영역으로 Transition하는 방식으로 작동한다.</p> <blockquote> <p><strong>Score(Stein)</strong></p> <p>이전 생성모델들은 log likelihood의 gradient인 $\nabla_\theta \log p_\theta(x)$을 통해 $\theta$를 업데이트하는 방식이었다.</p> <p>하지만, Score-Based 모델은 학습 데이터 $x$ 자체의 gradient을 구하는 <strong>Stein Score Function</strong>을 바탕으로 학습을 진행한다.</p> \[s_\theta(x) = \nabla_x \log p_\theta(x)\] <p>즉, Score function이란 데이터 $x$의 분포를 기준으로 likelihood가 높은 방향으로 gradient를 유도하는 알고리즘이다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/Gaussian%20plot.png" alt="Gaussian plot" style="zoom: 25%;"></p> <p>여기서 <strong>score</strong>는 각 지점에서 어떻게 움직여야 likelihood가 높은 지점으로 갈 수 있는지를 알려주는 역할을 한다. 따라서 위 분포에서는 <strong>화살표(score)</strong>를 따라가면 각각의 Gaussian의 mean vector에 도착하게 된다.</p> <p>Score-based model의 핵심아이디어는 확률분포함수를 사용하는 대신 이 score를 활용하는 것이다.</p> </blockquote> <blockquote> <p><strong>Langevin dynamics</strong></p> <p><strong>랑주뱅 동역학</strong>은 분자 시스템의 움직임의 수학적 모델링이다. 확률적 미분 방정식을 사용해 자유도를 생략하면서 단순화된 모델을 사용한다.</p> </blockquote> <h3 id="problem"><strong>Problem</strong></h3> <p>하지만, 이러한 접근 방식에는 다음과 같이 해결해야하는 과제 2가지가 있다.</p> <ol> <li>저차원 maniford에서의 데이터셋 분포는 score가 주변 space에서 정의되지 않으며 score matching이 일관된 추정치를 제공하지 않는다.</li> <li>low-density 영역의 데이터가 부족할 경우, score matching 정확도가 떨어지고 Langevin dynamics sampling의 mixing 속도가 감소한다.</li> </ol> <blockquote> <p>Langevin dynamics는 밀도가 낮은 영역에서 자주 초기화됨</p> </blockquote> <p>이 2가지 문제는 초기 sample을 high-density 영역으로 <strong>transition</strong>할 때 발생한다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/data%20scores.png" alt="data scores"></p> <p>위 사진처럼 좌측 하단 distribution에서 우측 상단 distribution으로 transition하며 추가적인 특징을 가진 이미지를 생성하려고 한다. 중간 대각선 영역은 low density 영역이며 score가 존재하지 않는다. 때문에 <strong>이 영역을 지나야 하지만, score가 존재하지 않기</strong> 때문에 어느 방향으로 가야할 지를 모르는 문제가 생긴다. 이를 모델이 <strong>저차원 manifold에서 “collapse”된다</strong>고 표현한다.</p> <h3 id="solution">Solution</h3> <p>위 문제를 해결하기 위해 <strong>데이터에 random Gaussian noise를 추가하여 collapse를 방지</strong>한다. Noise level이 클수록 데이터 분포의 low density 영역에서 sample을 생성하여 score matching을 개선한다.</p> <p>논문에서는 <strong>Annealed version of Langevin dynamics</strong>을 제안하는데, 이는 처음에는 높은 level의 nosie를 추가하고, Noise level을 점차 낮추며 추가하는 방법이다.</p> <h2 id="score-based-generative-modeling"><strong>Score-based Generative modeling</strong></h2> <p>데이터셋을 $p_{data}(x)$에서 sampling된 i.i.d.한 $\lbrace x \in \mathbb{R}^D \rbrace^{N}_{i=1}$으로 가정하자.</p> <blockquote> <p>i.i.d.란 independent and identically distribution의 약어로 독립 항등분포, 즉 독립적이고 같은 확률 분포를 가지는 것을 말한다.</p> </blockquote> <p>PDF인 $p(x)$의 score를 $\nabla_x \log p(x)$로 정의되며 score network $s_\theta : \mathbb{R}^D \rightarrow \mathbb{R}^D$는 $\theta$로 parameterize된 Neural network이다. 이 신경망은 $p_{data}(x)$의 score를 근사화하도록 훈련된다.</p> <p>이 Score-based generative modeling framwork에는 <strong>score matching</strong>과 <strong>Langevin dynamics</strong>라는 두가지 요소가 있다.</p> <h3 id="score-matching-for-score-estimation"><strong>Score matching for score estimation</strong></h3> <p>score matching을 사용하면, $p_{data}(x)$를 추정하는 모델 없이 score network $s_\theta$를 train하여 $\nabla_x \log p_{data}(x)$를 추정할 수 있다.</p> <blockquote> <p>즉, $\theta$라는 파라미터를 지닌 모델을 활용하여 $\nabla_x \log p_{\theta}(x)$를 $\nabla_x \log p_{data}(x)$가 되도록 train하는 것이 Score-based의 아이디어이다.</p> </blockquote> <p>즉 $s_\theta (x) = \nabla_x \log p_{\theta}(x)$가 $\nabla_x \log p_{data}(x)$와 같아지도록 하기 위해 Euclidean distance로 정의한 Objective은 다음을 minimize한다.</p> \[\frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right]\] <p>위 식을 <strong>Fisher Divergence</strong>라고 한다.</p> <p>위 식은 부분적분을 통해 다음과 같이 유도할 수 있다. (우리가 모르는 $p_{\text{data}}(x)를 없애기 위해) (<strong>이과정이 score matching이다.</strong>)</p> \[\mathbb{E}_{p_{data}} \left [\text{tr}(\nabla_xs_\theta(x)) + \frac{1}{2}\left|\left| s_\theta(x) \right|\right|^2_2 \right] \tag{1}\] <details> <summary><b>Eq. 1</b>유도 과정</summary> $$ \begin{align} \frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right] &amp;= \frac{1}{2} \int p(x)(\nabla_x \log p(x) - s_\theta(x))^2 \text{d}x \nonumber\\ &amp;= \frac{1}{2} \int p(x)(\nabla_x \log p(x))^2\text{d}x + \frac{1}{2} \int p(x)s_\theta(x)^2 \text{d}x - \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x \nonumber \end{align} $$ 여기서 마지막 항 $- \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x$는 다음과 같이 유도할 수 있다. $$ \begin{align} - \int p(x) \nabla_x \log p(x)s_\theta(x) \text{d}x &amp;= - \int p(x) \frac{\nabla_xp(x)}{p(x)}s_\theta(x)\text{d}x \nonumber\\ &amp;= -\int \nabla_x p(x)s_\theta(x) \text{d}x \nonumber\\ &amp;= -p(x)s_\theta(x) |^\infty_{x=-\infty} + \int p(x)\nabla_xs_\theta(x)\text{d}x \nonumber\\ &amp;= \int p(x)\nabla_x s_\theta(x)\text{d}x \nonumber \end{align} $$ 3번째 줄에서 boundary term은 $\pm\infty$에 의해 0이 된다. 즉 마지막 항을 $\int p(x)\nabla_x s_\theta(x)\text{d}x$으로 유도함으로써 $\frac{1}{2}\mathbb{E}_{p_{data}} \left [\left|\left| s_\theta(x) - \nabla \log p_{\text{data}}(x) \right|\right|^2_2 \right]$는 다음과 같이 유도할 수 있다. $$ = \underbrace{\frac{1}{2} \int p(x)(\nabla_x \log p(x))^2\text{d}x }_{1}+ \underbrace{\frac{1}{2} \int p(x)s_\theta(x)^2 \text{d}x}_2 + \underbrace{\int p(x)\nabla_x s\_\theta(x)\text{d}x}_3 $$ 여기서 1번 항을 $\theta$에 대해 independent하기 때문에 상수로 바꾸고 2, 3번 항을 각각 $\mathbb{E}$로 묶으면 다음과 같은 식이 된다. $$ C + \frac{1}{2}\mathbb{E}_{p(x)} [s_\theta(x)^2] + \mathbb{E}_{p(x)}[\nabla_xs_\theta(x)] $$ </details> <p><br></p> <p>$\nabla_x s_\theta(x)$는 $s_\theta(x)$의 Jacobian matrix를 나타낸다.</p> <blockquote> <p>Jacobian matrix란, 다변수 함수의 도함수를 나타내는 행렬이다. 다변수 함수의 입력 변수가 여러 개일 때, 각 입력 변수에 대한 편미분 값을 행렬로 정리한 것을 말한다.</p> </blockquote> <p>이 Jacobian matrix는 Deep network와 고차원 데이터에서 계산이 어렵기 때문에 다음과 같은 Score matching method 2가지를 설명한다.</p> <h4 id="denoising-score-matching"><strong>Denoising score matching</strong></h4> <p>Denoising score matching은 $\text{tr}(\nabla_x s_\theta(x))$의 계산을 우회하는 변형된 score matching이다. 데이터 $x$에 nosie distribution $q_\sigma (\widetilde{x} | x)$를 추가하고, 이를 제거하는 방식으로 진행한다.</p> <p>noise가 추가된 데이터 distribution $q_\sigma (\widetilde{x}) \triangleq \int q_\sigma (\widetilde{x} | x)p_{data}dx$의 score를 추정하며, Objective는 다음과 같다.</p> \[\frac{1}{2} \mathbb{E}_{q_\sigma(\widetilde{x}|x)p_{data}(x)}\left [\left|\left| s_\theta(\widetilde{x}) - \nabla_{\widetilde{x}}\log q_\sigma(\widetilde{x}|x) \right|\right|^2_2 \right] \tag{2}\] <p><strong>최적의 score 모델</strong>은 위 식 <strong>Eq. (2)</strong>를 minimize하여 $s_{\theta^*}(x) = \nabla_x \log q_\sigma (x)$를 만족하는 $s_{\theta^*}(x)$이다.</p> <h4 id="sliced-score-matching"><strong>Sliced score matching</strong></h4> <p>Sliced score matching은 random projection을 사용하여 $\text{tr}(\nabla_xs_\theta(x))$의 근사치를 구한다.</p> <p>Data score와 모델 분포의 vector field가 scalar 필드가 되도록 score를 random한 방향으로 projection한 다음 scalar 필드를 비교하여 모델 분포가 데이터 분포에서 얼마나 떨어져 있는지 확인한다.</p> <p>radom projection 방향을 $\textbf{v}$로, $\textbf{v}$의 분포를 $p_{\textbf{v}}$로 할때, random projection에 대한 <strong>Fisher Divergence</strong>는 다음과 같다.</p> \[\frac{1}{2} \mathbb{E}_{p_{\text{data}}}[(\textbf{v}^\top\nabla_x \log p_{\text{data}}(x) - \textbf{v}_\top \nabla_x \log p_\theta(x))^2]\] <blockquote> <p>$s_\theta(x) = \nabla_x \log p_\theta(x)$이므로 아래 식과 동일</p> \[\frac{1}{2} \mathbb{E}_{p_{\text{data}}}[(\textbf{v}^\top\nabla_x \log p_{\text{data}}(x) - \textbf{v}_\top s_\theta(x))^2]\] </blockquote> <p>위 식 역시 <strong>Eq. 1</strong>을 유도한 trick을 사용하여 다음과 같이 나타낼 수 있다.</p> \[\mathbb{E}_{p_\textbf{v}}\mathbb{E}_{p_\text{data}} \left[\textbf{v}^\top \nabla_xs_\theta(x)\textbf{v} + \frac{1}{2}\left|\left| s_\theta(x) \right| \right| ^2_2 \right] \tag{3}\] <h3 id="sampling-with-langevin-dynamics"><strong>Sampling with Langevin dynamics</strong></h3> <p>Langevin dynamics는 score function $\nabla_x \log p(x)$를 사용하여 sampling한다.</p> <p>$\textbf{z}_t \sim \mathcal{N}(0, \mathit{I})$일때 다음과 같다.</p> \[\widetilde{x}_t = \widetilde{x}_{t-1} + \frac{\epsilon}{2} \nabla_x \log p(\widetilde{x}_{t-1}) + \sqrt{\epsilon}\;\textbf{z}_t \tag{4}\] <p>이 sampling 과정에서 $\nabla_x \log p(x)$가 필요하기 때문에, $s_\theta(x) \approx \nabla_x \log p(x)$가 되도록 train 한 다음, 이 $s_\theta$를 사용하여 sampling을 진행한다.</p> <h2 id="challenges-of-score-based-generative-modeling"><strong>Challenges of score-based generative modeling</strong></h2> <p>이 section에서는 Score-based 방식의 2가지 장애물을 다룬다.</p> <h3 id="1-the-manifold-hypothesis"><strong>1. The manifold hypothesis</strong></h3> <p>위에서 언급했듯이, 데이터가 저차원 manifold에 있는 경우, score가 well-define되지 않고, model이 collapse될 수 있다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/figure1.png" alt="figure1" style="zoom:50%;"></p> <p><strong>Figure 1</strong>을 보자.</p> <p>왼쪽 그래프는 <strong>원본 CIFAR-10</strong>로 train한 Sliced score matching의 loss로 불규칙적인 변화를 보인다.</p> <p>이를 방지하기 위해 Gaussian noise를 추가하는데, 오른쪽 그래프를 보면 loss값들이 수렴하는 것을 볼 수 있다.</p> <h3 id="2-low-data-density-regions"><strong>2. Low data density regions</strong></h3> <p>low density 영역의 데이터 부족은 score matching을 통한 score estimate와 Langevin dynamics을 이용한 MCMC samping에 어려움을 야기한다.</p> <h4 id="1-inaccurate-score-estimation-with-score-matching"><strong>1. Inaccurate score estimation with score matching</strong></h4> <p>low density 영역에서는 <strong>Eq. 2</strong>를 minimize하는 score matching을 통해 score function이 잘 추정되지 않는다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/figure2.png" alt="figure2" style="zoom:67%;"></p> <p>위 사진에서 실제 데이터 score (왼쪽)와 추정된 score (오른쪽)의 중앙을 보면 score가 제대로 구해지지 않았다는 것을 볼 수 있다. 해당 영역에서의 score estimation은 매우 부정확해지며, Score estimation이 부정확하면 결국 Langevin dynamics을 통한 sampling 과정도 부정확해지게 될 것이다.</p> <h4 id="2-slow-mixing-of-langevin-dynamics"><strong>2. Slow mixing of Langevin dynamics</strong></h4> <p>다른 두 데이터 분포를 mix할 때 그 비율을 알 수 없다는 문제도 발생한다.</p> <p>$p_{\text{data}}$가 다음과 같이 $p_1$과 $p_2$ 분포의 mixture로 구성된 데이터 분포라고 가정하자.</p> \[p_{\text{data}}(x) = \pi p_1(x) + (1-\pi)p_2(x)\] <p>$\nabla_x \log p_{\text{data}}(x)$를 구하기 위해서 각 항에 $\log$를 씌우면, 각각 $\log \pi$와 $\log (1-\pi)$는 상수가 되어버리므로, 결국 다음과 같이 된다.</p> \[\nabla_x \log p_{\text{data}}(x) = \nabla_x \log p_1(x) + \nabla_x \log p_2(x)\] <p>즉, mixture 비율(즉 가중치)인 $\pi$에 상관없이 score가 추정된다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/figure3.png" alt="figure3" style="zoom:67%;"></p> <p>위 그림에서 (a)는 원래 데이터 분포이며, 두 분포가 다른 비율로 mixture된 것을 알 수 있다.</p> <p>(b)는 Langevin dynamics으로 sampling된 결과이며, 두 분포가 균일하게 samping된 것을 볼 수 있다.</p> <p>(c)는 annealed된 Langevin dynamics으로 sampling된 결과이며, mixture 비율을 반영한 것이다.</p> <h2 id="noise-conditional-score-networks--learning-and-inference"><strong>Noise Conditional Score Networks : learning and inference</strong></h2> <p>논문에서는 Gaussian Noise를 추가한 경우, Score-based 모델링의 성능이 높아진다는 것을 발견하였다. 그 이유는 크게 다음과 같은 2가지가 있다.</p> <ol> <li>Gaussian noise의 support가 하나의 whole space이기 때문에 저차원 maniford에 제한받지 않게 되어 score matching이 수월하게 define된다.</li> <li>Gaussian nosie를 추가함으로써 low density 영역을 채워주기 때문에 score estimation을 개선할 수 있다.</li> </ol> <h3 id="1-noise-conditional-score-networks"><strong>1. Noise Conditional Score Networks</strong></h3> <p>$\lbrace\sigma_i\rbrace^L_{i=1}$를 $\frac{\sigma_1}{\sigma_2} = \cdots = \frac{\sigma_{L-1}}{\sigma_L} &gt; 1$를 만족하는 수열이라고 하자. 이 각각의 $\sigma$가 바로 level에 따른 nosie가 된다.</p> <blockquote> <p>$\sigma_1$의 경우 매우 큰 nosie이며 $\sigma_L$에 가까워질 수록 데이터에 영향을 거의 미치지 않는 작은 수준의 nosie이다.</p> </blockquote> \[q_\sigma(x) \triangleq \int p_{data}(\textbf{t})\mathcal{N}(x | t, \sigma^2\textit{I})d\textbf{t}\] <p>$q_\sigma(x)$를 위와 같이 <strong>timestep $\textbf{t}$에 따라 nosie가 부여된 데이터의 분포</strong>라고 정의하면, 이 데이터 분포를 이용하여 score를 추정하는 network는 다음과 같다.</p> \[\forall \sigma \in \lbrace\sigma_i\rbrace^L_{i=1} : s_\theta (x, \sigma) \approx \nabla_x \log q_\sigma(x)\] <p>이때 $x \in \mathbb{R}^D$, $s_\theta (x, \sigma) \in \mathbb{R}^D$이다.</p> <p>위와 같은 $s_\theta (x, \sigma)$를 <strong><em>Noise Conditional Score Network(NCSN)</em></strong>이라 부른다.</p> <p>NCSN에서 사용한 아키텍쳐의 특징으로는 <strong>U-Net</strong>과 <strong>Instance Normalization</strong>이 있다.</p> <h3 id="2-learning-ncsns-via-score-matching"><strong>2. Learning NCSNs via score matching</strong></h3> <p>위 언급한 두 score matching 기법 모드 NCSN를 train할 수 있지만, noise있는 데이터를 다루고 속도가 약간 더 빠른 <strong>Denoising score matching을 사용</strong>하였다고 한다.</p> <p>Noise 분포 $q_\sigma(\widetilde{x} \;| \;x) = \mathcal{N}(\widetilde{x}\; | x, \sigma^2 \textit{I})$ 를 사용하면 score function은 다음과 같다.</p> \[\nabla_{\widetilde{x}} \log q_\sigma(\widetilde{x} \;| \;x) = -\frac{\widetilde{x}-x}{\sigma^2}\] <details> <summary> Why?? </summary> <blockquote> Gaussian 분포 $p(x)$는 $p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$로 정의되며, 이에 대한 score function은 위 $p(x)$에 $\log$를 씌우고 $x$에 대해 편미분을 진행한 $\nabla_x \log p(x)$이며 다음과 같다. $$ \begin{align} \nabla_x \log p(x) &amp;= \nabla_x \left (\log\frac{1}{\sqrt{2\pi\sigma^2}} + -\frac{(x-\mu)^2}{2\sigma^2} \log e \right) \nonumber \\ &amp;= -\frac{x-\mu}{\sigma^2} \nonumber \end{align} $$ 여기에 우리는 Noise가 추가된 조건부 Gaussian 분포 $p(\widetilde{x} | x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\widetilde{x}-x)^2}{2\sigma^2}}$를 정의한다. 이렇게 정의된 조건부 Gaussian 분포의 평균은 $x$가 되는데, 그 이유는 분포가 주어진 조건부 $x$를 중심으로 sampling이 이루어지기 때문이다. 즉 $p(\widetilde{x} | x) = \mathcal{N}(\widetilde{x} | x, \sigma^2 \textit{I})$로 정의할 수 있으며, 위와 같이 처리하면 $ -\frac{\widetilde{x}-x}{\sigma^2}$를 얻을 수 있다. </blockquote> </details> <p><br></p> <p>noise인 $\sigma$가 주어졌을 때, denoising score matching의 Objective(<a href="#denoising-score-matching">Eq. 2</a>)는 다음과 같다.</p> \[\mathscr{l}(\theta; \sigma) \triangleq \frac{1}{2} \mathbb{E}_{p_{\text{data}}(x)} \mathbb{E}_{\widetilde{x} \sim \mathcal{N}(x, \sigma^2, \textit{I})} \left[\left|\left|s_\theta(\widetilde{x}, \sigma) + \frac{\widetilde{x} - x}{\sigma^2} \right|\right|^2_2 \right] \tag{5}\] <p>여기에 $\sigma \in \lbrace\sigma_i\rbrace^L_{i=1}$을 결합하여 timestep에 대해 정의하면 <strong>Unified objective</strong>를 얻을 수 있다.</p> \[\mathcal{L}(\theta ; \lbrace\sigma_i\rbrace^L_{i=1}) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \mathscr{l}(\theta, \sigma_i) \tag{6}\] <p>여기서 $\lambda(\sigma)$는 $\sigma$에 의한 계수 함수(coefficient function)이다.</p> <p>이상적으로 모든 $i$에 대해 $\lambda(\sigma_i) \mathscr{l}(\theta, \sigma_i)$가 동일하기 위해 경험적으로 $\lambda(\sigma) = \sigma^2$을 얻어낼 수 있었다고 한다.</p> <h3 id="3-ncsn-inference-via-annealed-langevin-dynamics"><strong>3. NCSN inference via annealed Langevin dynamics</strong></h3> <p>NCSN $s_\theta(x, \sigma)$를 train 한 후, 이번 section에서는 sampling 과정인 <strong>Annealed Langevin dynamics</strong>인 <strong>Algorithm 1</strong>을 제안한다.</p> <p><img src="/assets/img/post/Score-based%20Generative%20Model/Algorithm1.png" alt="Algorithm1" style="zoom:67%;"></p> <p>고정된 분포 $x_0$에서 시작하여 $q_{\sigma_{i}}(x)$와 step size $\alpha_i$를 통해 $q_{\sigma_{i+1}}(x)$를 만든다. step size $a_i$는 위 3번째 줄 식을 통해 점차 감소시키며, 마지막에 $q_{\sigma_{L}}(x)$로부터 만들어진 sample은 $p_{\text{data}}(x)$와 비슷해진다.</p> <blockquote> <p>이때 noise level은 거의 없는 수준이다. $\rightarrow \sigma_L \approx 0$</p> </blockquote> <p><strong>Gaussian Noise가 첨가된 분포인 $\lbrace q_{\sigma_i} \rbrace ^L_{i=1}$은</strong> whole space에 걸쳐 있기 때문에 <strong>density가 높다</strong>고 할 수 있다. <strong>score estimation과 Langevin dynamics는 high-density 영역에서 매우 잘 수행</strong>되므로 <span style=" background-color: #F7DDBE">처음에 언급한 문제 2가지를 해결</span>할 수 있다고 논문에서 말한다.</p> <p>$a_i$를 조정하는 다양한 방법이 있지만, 논문에서는 $\alpha_i \propto \sigma_i^2$를 채택하였다고 한다. 그 이유는 <strong>Langevin dynamics에서 signal-to-noise 비율을 고정</strong>하는 것에서 비롯되었다. ($\frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z}$)</p> \[\mathbb{E} \left[ \left|\left| \frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z} \right|\right|^2_2 \right] \approx \mathbb{E} \left[ \frac{\alpha_i \left|\left|s_\theta(x, \sigma_i) \right|\right|^2_2}{4} \right] \propto \frac{1}{4} \mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right]\] <p>위 관계를 기억하자. 앞에서 우리는 $\left|\left| s_\theta(x, \sigma) \right|\right|_2 \propto \frac{1}{\sigma}$일때 Score network 학습이 optimal 해진다는 것을 찾았다.</p> <p>(그렇지 않은 경우에는 $\mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right] \propto 1$)</p> <p>그러므로 위 관계는 다음과 같이 되어 $\sigma_i$와 무관해진다.</p> \[\left|\left| \frac{\alpha_i s_\theta(x, \sigma_i)}{2\sqrt{\alpha_i}\;z} \right|\right|^2_2 \propto \frac{1}{4} \mathbb{E} \left[\left|\left| \sigma_i s_\theta(x, \sigma_i) \right|\right|^2_2 \right] \propto \frac{1}{4}\] <p>위 <strong>Algorithm 1</strong>의 효율성을 입증하기 위해 실험을 진행하였으며, 그 결과를 section <a href="#2-slow-mixing-of-langevin-dynamics">3.2</a>에서 설명하였듯이 Annealed Langevin dynamics는 두 mode간의 가중치를 반영하여 복구한 것을 볼 수 있다.</p> <blockquote> <p>Sampling은 Annealed Langevin dynamics로 진행한다면, <strong>score estimator 역시 noise가 추가된 sample로 학습을 해야 정확한 score가 추정</strong>된다.</p> <p>때문에, sample에 다양한 크기의 nosie($\mathcal{N}(0, \sigma_i^2\textit{I})$)를 추가하여 학습시키는데, 이때 추가하는 noise의 크기는 갈수록 증가한다. ($\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L$, <strong>sampling의 정반대</strong>)</p> <p>sample은 데이터 $x \sim p(x)$에 noise $z \sim \mathcal{N}(0, \textit{I})$를 더한 $x + \sigma_i z$로 만들며 NCSN 모델 $s_\theta (x, i)$를 학습시킨다.</p> \[p_{\sigma_i}(x) = \int p(y)\mathcal{N}(x;y, \sigma_i^2,\textit{I})\text{d}y\] <p>여느 Conditional 모델처럼 $\sigma$(더해지는 nosie level)가 입력으로 들어가기 때문에 $s_\theta(x, i) : \mathbb{R}^{D+1} \rightarrow \mathbb{R}^D$가 된다.</p> <p>$s_\theta(x, i)$의 Objective는 다음과 같다. (Denoising score matching 기반이며 noise인 $\sigma$가 추가됨)</p> \[\sum_{i=1}^L \lambda(i) \mathbb{E}_{p_{\sigma_i}(x)}\left[|| \nabla_x \log p_{\sigma_i}(x) - s_\theta(x, i)||^2_2\right]\] <p>이러한 train과정을 거치면 Annealed Langevin dynamics를 통해 sampling을 진행하면 된다.</p> </blockquote> <h2 id="experiments"><strong>Experiments</strong></h2> <h4 id="set-up">Set up</h4> <ul> <li>사용 데이터셋 : MNIST, CelebA, CIFAR-10</li> <li>[0, 1] 범위로 scaling</li> <li>Noise $\sigma \in \lbrace\sigma_i\rbrace^L_{i=1}$가 $1 \sim 0.01$이 되도록 $L = 10$으로 설정 ($\sigma_L = 0.01$은 육안으로 거의 구분 불가)</li> <li>$T = 100$, $\epsilon = 2 \times 10^{-5}$</li> <li>Architecture 특징 : U-net, Conditional Instance normalization, Dilated convolutions</li> </ul> <p>Yang song은 Score-based model을 학습할 때 다음과 같은 사항을 추천했다.</p> <ul> <li>$\sigma_1 &lt; \cdots &lt; \sigma_L$은 기하수열로 구성</li> <li>$\sigma_1$은 충분히 작은 값에서 시작하여야 하며 $\sigma_L$은 데이터의 maximum pairwise distance 수준으로 결정</li> <li>$L$은 주로 수백 ~ 수천의 크기로 설정</li> </ul> <figure> <img src="https://yang-song.net/assets/img/post/score/celeba_large.gif" alt="adf"> <img src="https://yang-song.net/assets/img/post/score/cifar10_large.gif" alt="dfd"> <figcaption>NCSN 모델이 CelebA와 CIFAR-10 데이터셋으로 학습시켜 sampling하는 결과를 보여준다.</figcaption> </figure> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>이번 논문에서는 Score-based Generative model의 기본 원리에 대해 탐구해보다. 이 모델은 기존의 GAN, VAE와 같은 방식이 아닌 새로운 생성 모델 프레임워크로서, score라는 개념을 중심으로 새로운 패러다임을 제시한다. CIFAR-10에서 inception score SOTA를 달성하였으며, SNGAN에 필적하는 FID score를 얻었다고 한다.</p> <h2 id="code"><strong>Code</strong></h2> </body></html>