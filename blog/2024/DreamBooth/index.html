<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="dreambooth-논문-리뷰"><strong>DreamBooth 논문 리뷰</strong></h1> <p>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation : <a href="https://arxiv.org/abs/2208.12242" rel="external nofollow noopener" target="_blank">arxiv</a></p> <p>구글과 보스턴 연구진이 2022년에 발표한 논문으로, Diffusion 논문에 대한 새로운 Fine-tuning 기법을 제시하였다. 특정 피사체에 대해 적은 데이터만으로도 대상의 사실적인 이미지를 생성할 수 있다. 대상의 핵심 특징을 보존하여 합성하고, 기존의 모델의 큰 오염 없이 새로운 피사체에 대해 이미지를 생성할 수 있다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>이 연구는 특정 대상을 새로운 관점에서 자연스럽게 재구성하는 것에 집중한다. 최근 많이 연구되는 text-to-image 모델은 대규모 image-caption 데이터셋으로 semetic하고 prior하게 학습하여 다양한 고품질 이미지를 생성하는 것이 장점이다.</p> <blockquote> <p>prior하다는 것은 “개”라는 단어를 다양한 상황과 포즈에서 나타날 수 있는 개의 여러 instance와 연결시키는 것</p> </blockquote> <p><img src="/assets/img/post/DreamBooth/figure1.png" alt="figure1"></p> <p><span style=" background-color: #F7DDBE">이 연구에서는 text-to-image diffusion 모델의 “<b>개인화(personalization)</b>“에 집중한다.</span> 모델의 language-vision dictionary를 확장하여 사용자가 특정 subject를 자신이 원하는 새로운 단어와 결합하여 생성할 수 있도록 한다. 사용자가 생성하고자 하는 특정 대상에 새로운 단어를 연결하여 대상의 key feature를 high fidelity로 유지하면서 다양한 context 이미지를 합성할 수 있다.</p> <p>본 논문에서는 특정 subject의 3~5장 이미지를 사용하여 대상을 rare token identifier로 나타내고 이를 모델의 ouput domain에 삽입하고, subject와 연결된 unique identifier를 사용하여 이미지를 합성한다.</p> <blockquote> <p>이를 subject-driven generation이라 하며, 본 논문에서 이러한 새로운 문제를 정의하였다.</p> </blockquote> <p><img src="/assets/img/post/DreamBooth/figure2.png" alt="figure2" style="zoom:50%;"></p> <p>위 이미지는 DALL-E2, Imagen과 함께 본 논문에서 제안한 기법을 비교한다.</p> <p>Input images에 있는 시계의 key feature(숫자 3이 특이한 글씨체로 쓰여진 시계)를 유지했는지 (fidelity), 다양한 context 이미지를 생성하였는지(New contexts)를 보여준다.</p> <p>이 논문에서는 위와 같이 pretrain된 diffusion 기반의 text-to-image 모델에 대해 새로운 fine-tuning 기법을 제안한다.</p> <h2 id="method"><strong>Method</strong></h2> <p>이 논문에서는 text 설명 없이s ubject에 대한 이미지 3~5장 만 주여지면, text prompt에 따라 detail fidelity가 높고 다양한 context로 이미지를 생성하는 것을 목표로 한다.</p> <h3 id="1-text-to-image-diffusion-models"><strong>1. Text-to-Image Diffusion Models</strong></h3> <p>text encoder $\Gamma$와 text prompt $\textbf{P}$로 만들어진 conditioning vector $c = \Gamma (\textbf{P})$가 있다. 이 $c$와 초기 $\epsilon \sim \mathcal{N}(0, \textit{I})$가 주어졌을 때, $\mathbf{x}_{\text{gen}} = \hat{ \mathbf{x}}_\theta (\epsilon, c)$를 생성한다.</p> <p>또한 이 $\hat{ \mathbf{x}}_\theta$는 다음과 같이 noise 이미지 $\mathbf{z}_t := \alpha_t\mathbf{x} + \sigma_t\mathbf{\epsilon}$의 noise를 제거하도록 학습한다.</p> \[\mathbb{E}_{\mathbf{x},c, \epsilon, t} \left[w_t || \hat{\mathbf{x}_\theta}(\alpha_t\mathbf{x} + \sigma_t\epsilon, c) - \mathbf{x}||^2_2\right] \tag{1}\] <h3 id="2-personalization-of-text-to-image-models"><strong>2. Personalization of Text-to-Image Models</strong></h3> <p>첫번째 task는 subject의 instance를 모델의 output domain에 삽입하는 것이다. 몇장 안되는 subject의 이미지로 생성 모델을 fine-tuning 할 때 가장 핵심적인 문제는 target distribution을 잘 학습하지 못하는 것이다. (그밖에도 overfitting, mode-collapse가 있음)</p> <p>이를 해결하기 위해 Eq. 1의 diffusion loss를 fine-tuning하여 새로운 정보를 domain에 삽입하는 데 효과가 좋다는 것을 발견하였다고 한다.</p> <h4 id="designing-prompts-for-few-shot-personalization"><strong>Designing Prompts for Few-Shot Personalization</strong></h4> <p>목표는 diffusion 모델의 <strong>dictionary(사전)에 새로운 (unique identifier, subject) 쌍을 이식</strong>하는 것이다. subject의 모든 input image에 “a [identifier] [class noun]”라는 label을 지정해준다.</p> <blockquote> <p>[identifier]는 subject에 연결된 고유값이며, [class noun]는 dog, cat과 같은 class를 말한다.</p> <p>[class noun]의 경우 classifier를 사용하거나, 사용자가 직접 붙여준다.</p> </blockquote> <p>즉, 논문에서는 모델의 특정 class의 prior를 subject의 unique identifier embedding과 연결하여 다양한 context에서 subject를 생성할 수 있도록 한다.</p> <h4 id="rare-token-identifiers"><strong>Rare-token Identifiers</strong></h4> <p>먼저 영어에서 각 문자를 tokenize하여 rare한 token을 찾는다. 이 token을 text space로 변환하여 identifier가 강력한 prior를 가질 확률을 최소화한다. 이를 <strong>rare-token lookup</strong>이라 하며, 이를 수행하여 rare-token identifier $f(\hat{\textbf{V}})$ sequence를 얻는다. 그런 다음, $f(\hat{\textbf{V}})$를 de-tokenize하여 text $\hat{\textbf{V}}$를 얻는다.</p> <blockquote> <p>$f$는 문자 sequences를 token에 mapping하는 tokenizer이고, $\hat{\textbf{V} }$는 $f(\hat{\textbf{V}})$에서 decoding된 text를 의미한다. 논문에서 token은 3자 이하의 unicode로 구성된 문자일 때 성능이 가장 좋다고 언급한다.</p> </blockquote> <h3 id="3-class-specific-prior-preservation-loss"><strong>3. Class-specific Prior Preservation Loss</strong></h3> <p>논문에서는 subject의 fidelity를 극대화 하기 위해서는 model의 모든 layer를 fine-tuning 해야함을 경험적으로 알 수 있었다고 한다. 여기에는 text-embedding을 condition으로 하는 fine-tuning layer가 포함되며, 이로 인하여 <strong>language drift</strong>문제가 발생한다고 한다.</p> <blockquote> <p><strong>language drift</strong></p> <p>language drift란 language model fine-tuning에서 관찰되는 문제로, 특정 task에 맞게 fine-tuning된 모델이 task에 대한 성능을 올리기 위해 점차 언어에 대한 syntactic and semantic knowledge을 잃어버리는 현상이다.</p> <p>논문에서는 위와 같이 언어 모델에서 나타나는 유사한 문제가 diffusion 모델에 영향을 미치는 것을 자기네들이 처음 발견하였다고 한다.</p> </blockquote> <p>또 다른 문제는 <strong>output의 다양성이 감소</strong>할 수 있다는 것이다. text-to-image diffusion 모델은 다양성이 높지만, 적은 image 데이터셋으로 fine-tuning 할 때 variability(가변성)이 줄어들 위험이 있다.</p> <p><strong>위 언급한 2가지 문제를 해결하기 위해 Class-specific Prior Preservation Loss를 제안한다.</strong> 자체적으로 생성된 sample로 모델을 가두어 여러번의 fine-tuning이 시작되어도 prior를 유지하도록 하는 것이다.</p> <p>randon initial noise $\mathbf{z}_{t_1} \sim \mathcal{N}(0, \textbf{I})$과 conditioning vector $\textbf{c}_{pr} := \Gamma(f(\text{“a [class noun]”}))$로 고정된 pretrained diffusion model에서 $\mathbf{x}_{pr} = \hat{\mathbf{x}}(\mathbf{z}_{t_1}, \mathbf{c}_{pr})$을 생성한다.</p> <p>해당 loss는 다음과 같다.</p> \[\mathbb{E}_{\mathbf{x},c, \epsilon, \epsilon', t} \left[w_t || \hat{\mathbf{x}_\theta}(\alpha_t\mathbf{x} + \sigma_t\epsilon, c) - \mathbf{x}||^2_2 + \lambda w_{t'}|| \hat{\mathbf{x}_\theta}(\alpha_{t'}\mathbf{x}_{\text{pr}} + \sigma_{t'}\epsilon', c_{\text{pr}}) - \mathbf{x}_\text{pr}||^2_2 \right] \tag{2}\] <p>여기서 추가된 2번째 term은 생성된 sample로 모델을 supervise하는 prior-preservation term이며, $\lambda$는 이 term의 가중치를 조절한다.</p> <p><img src="/assets/img/post/DreamBooth/figure3.png" alt="figure3" style="zoom:50%;"></p> <p>위 사진은 class에서 생성된 sample과 Prior Preservation Loss로 모델을 fine-tuning하는 과정을 보여준다.</p> </body></html>