<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="wassersteinganwgan-논문-리뷰"><strong>WassersteinGAN(wGAN) 논문 리뷰</strong></h1> <p><a href="https://arxiv.org/abs/1701.07875" rel="external nofollow noopener" target="_blank">[논문 링크]</a></p> <p>수학적인, 특히 확률론에 관련된 내용과 글이 많아서 가독성이 떨어질 수 있다.</p> <p>필자가 예전에 wGAN에서 필요한 수학적 개념들을 정리한 글이 있으니 <a href="https://hahngyutak.github.io/posts/wGAN/" rel="external nofollow noopener" target="_blank">참고</a></p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>probability distribution, 즉 확률 분포를 학습한다는 것은 확률 밀도(density)를 학습한다는 말이다. 이는 $(P_\theta)_{\theta \in \mathbb{R}_d}$ 의 parametric family를 정의하고, 데이터에서 likehood를 maximize한 밀도를 찾는 방식으로 이루어진다.</p> <blockquote> <p>parametric family란, 일련의 확률 밀도 함수 또는 확률 분포를 의미한다. 이러한 분포는 parameter set에 의해 결정되며, 이러한 파라미터를 변경함으로써 다양한 분포가 생성될 수 있다.</p> <p>예를 들어, 정규분포는 <strong>평균</strong>과 <strong>표준편차</strong>라는 2개의 파라미터를 가지는 분포이다. 이를 변경함으로써 다양한 형태의 정규분포를 만들 수 있으며, 가능한 모든 정규분포의 집합을 ‘정규분포의 parametric family’라 할 수 있다.</p> </blockquote> <p>데이터 예시인 $\lbrace x_{i} \rbrace ^m_{i=1}$가 있을 때, 다음과 같은 문제를 해결할 수 있다.</p> \[\underset{\theta \in \mathbb{R}^d}{\text{max}} \frac{1}{m} \sum_{i=1}^m\log P_\theta(x^{(i)})\] <p>데이터 분포 $\mathbb{P}_r$이 밀도를 가지고, $\mathbb{P}_\theta$가 parametrize된 밀도 $P_\theta$의 분포인 경우, 이는 KL divergence $KL(\mathbb{P}_r || \mathbb{P}_\theta)$를 minimize하는 것과 같다.</p> <p>위를 설명하려면, 모델의 density(밀도) $P_\theta$가 있어야하지만, 저차원 <strong>maniford</strong>에서 <strong>support</strong>하는 분포를 다루는 상황에서는 density가 존재하지 않을 수도 있다.</p> <details> <summary><b>Maniford란</b></summary> <blockquote> maniford란 고차원 공간에서 데이터를 설명하는 데 사용되는 저차원 구조이며, 고차원 데이터에서 복잡성을 줄이고, 데이터의 핵심 특징을 추출하는 데 사용된다. 예를 들어, 지구 표면의 작은 부분은 2차원 평면처럼 보이기 때문에 지구 표면은 3차원 공간의 2차원 maniford라고 할 수 있다. 이처럼, maniford는 종종 데이터의 '구조' 또는 '패턴'을 찾는 데 사용되며, maniford 학습은 고차원 데이터에서 maniford를 찾아내는 과정이다. </blockquote> </details> <p><br></p> <details> <summary><b>support</b></summary> <blockquote> 확률론에서 support란, 어떠한 확률 분포나 확률 밀도에서 양수인 값을 가지는 집합을 의미하며, <b>확률 변수의 가능한 결과들의 집합</b>을 의미한다. </blockquote> </details> <p><br></p> <p>이 경우, 모델 maniford와 실제 데이터 분포의 support가 교집합을 거의 가지지 않으며, 이는 <strong>KL distance가 정의되지 않거나 infinite</strong>하다는 것을 의미한다.</p> <p>이 문제를 해결하기 위해 일반적으로 모델 분포에 noise를 추가한다. (실제로 대부분의 Generate 모델에는 Gaussian noise가 포함되어있음.) 하지만, 이러한 잡음이 sample의 품질을 저하시킨다.</p> <p>확률 밀도 $\mathbb{P}_r$을 직접 추정하는 대신, fixed 분포 $p(z)$를 가지는 random variable $Z$를 정의하고, 이를 매개변수 함수 $g_\theta : \mathcal{Z} \rightarrow \mathcal{X}$를 통해 변환하여 특정 분포 $\mathbb{P}_\theta$를 따르는 sample을 직접 생성할 수 있다.</p> <p>이 방법은 VAE와 GAN이 사용하며, GAN은 $x$를 결정하는 latent variable $z$를 input으로 하고, $G$와 $D$의 관계를 학습하며 $G$의 분포를 $P(x)$에 가까워지도록 유도한다. 하지만, <strong>Mode collapsing</strong>이 빈번하게 일어난다는 문제가 있다.</p> <p>이 논문에서는 <strong>모델 분포와 실제 분포간의 거리를 측정</strong>하는 다양한 방법, 즉 두 분포 사이의 <strong>거리 또는 발산</strong>($\rho (\mathbb{P}_\theta \mathbb{P}_r)$)을 정의하는 다양한 방법에 집중한다.</p> <p>매개변수 $\theta$를 최적화하기 위해서는 모델 분포 $\mathbb{P}_\theta$를 $\theta \rightarrow \mathbb{P}_\theta$라는 mapping이 <strong>continuous</strong>하도록 해야한다.</p> <blockquote> <p><strong>continuous</strong>한다는 것, 즉 Continuity이란 parameter $\theta_t$가 $\theta$로 converge(수렴)할 때 분포 $P_{\theta_t}$ 역시 $P_\theta$로 수렴한다는 것을 의미</p> </blockquote> <p>분포 $P_{\theta_t}$의 converge은 거리를 계산하는 방식에 따라 달라진다. 거리가 약할 수록 분포는 converge하기 쉽고 $\theta$space에서 $\mathbb{P}_\theta$space의 continuous mapping을 정의하기 쉬워진다.</p> <blockquote> <p>왜 연속적이어야 하는가에 대해서는 optimizing algorithm을 생각하면 된다. gradient를 활용하는 경사 하강법은 함수가 연속적이지 않으면 gradient는 증발하거나 무한대가 되기 때문이다.</p> </blockquote> <h2 id="different-distances"><strong>Different Distances</strong></h2> <p>여기서는 Introduction에서 언급한 다양한 distance에 대해 소개한다.</p> <p>논문에서 사용하는 표기는 다음과 같다.</p> <ul> <li>$\mathcal{X}$ : <a href="https://ko.wikipedia.org/wiki/%EC%BD%A4%ED%8C%A9%ED%8A%B8_%EA%B3%B5%EA%B0%84" rel="external nofollow noopener" target="_blank">compact metric set</a> (ex- 이미지 공간 $[0,1]^d$)</li> <li> <p>$\Sigma$ : $\mathcal{X}$의 모든 <a href="https://ko.wikipedia.org/wiki/%EB%B3%B4%EB%A0%90_%EC%A7%91%ED%95%A9" rel="external nofollow noopener" target="_blank">Borel subset</a></p> </li> <li>$\text{Prob}(\mathcal{X})$ : $\mathcal{X}$에 정의된 확률 측정값의 공간</li> </ul> <blockquote> <p>참 어렵다,,</p> </blockquote> <p>이제 두 분포 $\mathbb{P}_r, \mathbb{P}_g \in \text{Prob}(\mathcal{X})$사이의 다양한 distance와 divergence를 정의할 수 있다.</p> <h4 id="total-variation-tv-distance">Total Variation (TV) distance</h4> \[\delta(\mathbb{P}_r, \mathbb{P}_g) = \underset{A \in \Sigma}{\text{sup}} \left |\mathbb{P}_r(A) - \mathbb{P}_g(A) \right|\] <p>두 확률 분포의 측정값의 차의 상한, 즉 벌어질 수 있는 가장 큰 값을 뜻한다.</p> <h4 id="kullback-leibler-kl-divergence">Kullback-Leibler (KL) divergence</h4> \[KL(\mathbb{P}_r \parallel \mathbb{P}_g) = \int \log \left(\frac{P_r(x)}{P_g(x)}P_r(x)d\mu(x) \right)\] <p>여기서 $\mathbb{P}_r$와 $\mathbb{P}_g$는 $\mathcal{X}$에 정의된 측정값 $\mu$에 대해 <strong>연속적</strong>이며, 즉 밀도를 가진다고 가정할 수 있다.</p> <p>KL divergence는 $\mathbb{P}_g(x) = 0$이고 $\mathbb{P}_r &gt; 0$일때 무한할 수 있으며 비대칭적이다. ($KL(\mathbb{P}_r \parallel \mathbb{P}_g) \neq KL(\mathbb{P}_g \parallel \mathbb{P}_r)$)</p> <h4 id="jensen-shannon-js-divergence">Jensen-Shannon (JS) divergence</h4> \[JS(\mathbb{P}_r, \mathbb{P}_g) = KL(\mathbb{P}_r \parallel \mathbb{P}_m) + KL(\mathbb{P}_g \parallel \mathbb{P}_m)\] <p>여기서 $\mathbb{P}_m = \frac{\mathbb{P}_r + \mathbb{P}_g}{2}$이다.</p> <p>JS divergence는 두 분포에 대한 평균에 대한 KL divergence로 정의되기 때문에 대칭성을 가진다.</p> <h4 id="earth-mover-em-distance-or-wasserstein-distance">Earth-Mover <strong>(EM) distance</strong> or <strong>Wasserstein distance</strong> </h4> \[W(\mathbb{P}_r, \mathbb{P}_g) = \underset{\gamma \in \prod(\mathbb{P}_r, \mathbb{P}_g)}{\text{inf}} \mathbb{E}_{(x,y)\sim\gamma}\left[\parallel x-y \parallel \right] \tag{1}\] <p>$\prod(\mathbb{P}_r, \mathbb{P}_g)$는 최댓값이 각각 $\mathbb{P}_r$과 $\mathbb{P}_g$인 모든 joint distribution $\gamma(x,y)$의 집합을 나타낸다.</p> <blockquote> <p>직관적으로 $\gamma(x,y)$는 분포 $\mathbb{P}_r$을 분포 $\mathbb{P}_g$로 변환하기 위해 $x$에서 $y$로 얼마나 많은 “질량”을 운반해야 하는지를 나타낸다. 그러면 EM distance는 최적의 운반 계획의 “비용”이라고 할 수 있다.</p> <p>이에 관한 설명은 해당 <a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490" rel="external nofollow noopener" target="_blank">링크</a>에 매우 잘 설명되어 있다. (wGAN을 이해하는데 큰 도움이 될것이다.)</p> </blockquote> <p>논문에서는 한가지 예시를 통해 EM distance의 타당성을 제시하고 있다. 아래 참고</p> <details> <summary><b>Example 1</b></summary> <br>임의의 확률분포 $\mathbb{P}_0$와 $\mathbb{P}_\theta$를 정의하고 그 사이의 distance와 divergence를 구한다. <br> <br><blockquote> $Z \sim U[0,1]$은 $Z$가 0과 1사이의 균일한 분포를 따른다는 것이다. $\mathbb{P}_0$를 $(0, Z) \in \mathbb{R}^2$라고 하자. 이는 $x$는 항상 0이고, $y$축 위 임의의 점 $Z$를 의미한다. $g_\theta (z) = (\theta, z)$는 입력 $z$이며 실수 파라미터인 $\theta$에 의해 결정되는 $(\theta, z)$를 출력한다. <br> 이때, 각 distance는 다음과 같다. </blockquote> <ul> <li>EM distance : $W(\mathbb{P}_0, \mathbb{P}_\theta) = |\theta|$</li> <li>JS divergence : $JS(\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} \log 2 &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> <li>KL divergence : $KL(\mathbb{P}_0 \parallel \mathbb{P}_\theta) = \begin{cases} +\infty &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> <li>TV distance : $\delta (\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} 1 &amp; \text{ if }\; \theta \neq 0 \\ 0 &amp; \text{ if } \; \theta = 0 \end{cases}$</li> </ul> $\theta_t \rightarrow 0$일때, EM distance를 제외한 다른 distance, divergence는 $\theta$가 0일때 불연속적인것을 확인할 수 있다. 때문에 다른 distance와 divergence를 사용한 loss function은 사용할 수 없다. <br> <br> 아래 사진은 위에 대해 각각 EM distance(좌)와 JS divergence(우)의 경우를 보여준다. <img src="/assets/img/post/wGAN/figure1.png" alt="figure1"> </details> <p><br></p> <p>위 <strong>Example 1</strong>을 통해 EM distance가 JS divergence보다 week하다는 것을 알 수 있다. 그러므로 간단한 가정 하에서 $W(P_r,P_\theta)$가 $\theta$에 대해 연속인 loss function인지 의문을 제기할수 있으며, 아래 <strong>Theorem 1</strong>을 통해 말하듯이 연속이다.</p> <details> <summary><b>Theorem 1</b></summary> $P_r$을 $\mathcal{X}$에 대한 고정 분포이며, $Z$을 다른 space $\mathcal{Z}$에 대한 radom 변수라고 가정하자. <br> $g : \mathcal{Z} \times \mathbb{R}^d$라는 함수라 할때, $g_\theta(z) = (z, \theta)$라 하자. $\mathbb{P}_\theta$는 $g_\theta(Z) $의 분포이다. <ol> <li>$g$가 $\theta$에 대해 연속이라면, $W(\mathbb{P}_r, \mathbb{P}_\theta$)도 연속이다.</li> <li>$g$가 local Lipschitz 조건과 regularity assumption 1을 만족한다면, $W(\mathbb{P}_r, \mathbb{P}_\theta)$는 항상 연속이며 미분가능하다.</li> <li>위의 명제 1, 2는 $JS(\mathbb{P}_r, \mathbb{P}_\theta)$과 모든 KL divergence에 대해서는 성립하지 않는다.</li> </ol> <br><blockquote> <b>local Lipschitz</b>란 함수의 변화율이 특정한 범위 내에서 제한되어 있음을 말한다. $K$-Lipschitz는 Lipschitz 상수 $K$를 가지며, 다음과 같은 제한이 있다. $$ |f(x_1) - f(x_2)| \leq K|x_1-x_2| $$ 즉, f(x₁)와 f(x₂)를 연결하면 그 gradient는 항상 $K$보다 작은 절대값을 가지며, 이를 함수의 Lipschitz 상수라고 한다. 사인 함수의 경우 도함수의 절대값은 항상 1로 제한되므로 Lipschitz 상수는 1이다. 직관적으로 Lipschitz 연속성은 gradient를 제한하며 딥러닝에서 gradient explosions을 완화하는데 사용된다. </blockquote> <blockquote> <b>regularity assumption 1</b><br> 유한한 차원 vetor space 사이의 locally Lipschitz를 $g:\mathcal{Z} \times \mathbb{R}_d \rightarrow \mathcal{X}$라 한다. 아래와 같은 local Lipschitz 상수 $L(\theta,z)$가 있는 경우 $\mathcal{Z}$에 대한 특정 확률 분포 $p$에 대해 $g$가 가정 1을 만족한다고 한다. $$ \mathbb{E}_{z \sim p} \left [L(\theta, z) \right] &lt; + \infty $$ </blockquote> </details> <p><br></p> <p>Neural Network에서 EM distance를 minimize하여 train하는 것이 이론적으로 합리적임을 <strong>다음</strong>을 통해 말한다.</p> <details> <summary><b>Corollary 1</b></summary> $g_\theta$는 $θ$로 parameterize된 feedfoward Neural Network이고, $p(z)$는 $\mathbb{E}\_{z \sim p(z)} [\parallel z \parallel] &lt; \infty$ (Gaussian, uniform 등)이 되도록 하는 $z$에 대한 사전분포라고 가정하자. 그러면 위 assumption 1이 충족되므로 $W(\mathbb{P}_r,\mathbb{P}_θ)$는 모든 곳에서 연속적이고 거의 모든 곳에서 미분 가능하다. </details> <p><br></p> <p>이 모든 것은 EM distance가 적어도 JS divergence보다 훨씬 더 합리적인 cost function이라는 것을 보여주며 위상의 상대적 강도는 다음과 같다.</p> \[\underset{strong \;\;\;\rightarrow \;\;\;week}{\text{KL} &gt; \text{(JS, TV)} &gt; \text{EM}}\] <h2 id="wasserstein-gan"><strong>Wasserstein GAN</strong></h2> <p>EM distance인 <strong>Eq. 1</strong>의 극한은 매우 intractable하다. <strong>Eq. 1</strong>에서의 $\mathbb{P}_r$과 $\mathbb{P}_g$의 joint distribution을 구해야 하지만, $\mathbb{P}_r$는 우리가 알고자 하는 대상이기 때문이다.</p> <p>그래서 Kantorovich-Rubinstein duality를 이용해서 다음과 같이 바꿀 수 있다.</p> \[W(\mathbb{P}_r, \mathbb{P}_\theta) = \underset{\parallel f\parallel _L \leq1}{\text{sup}}\; \mathbb{E}_{x \sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)] \tag{2}\] <p>$\parallel f\parallel _L \leq1$에서 1 대신 $K$를 대입하면, $K \cdot W(\mathbb{P}_r, \mathbb{P}_\theta)$가 된다. 이는 임의의 상수 $K$에 대해 $K$-Lipschitz를 고려하는 것이다.</p> <blockquote> <p><strong>supremum</strong>, 즉 $\text{sup}$은 주어진 집합에서 가장 큰 <strong>상한</strong>을 의미한다. 이는 주어진 집합의 모든 원소보다 크거나 같은 가장 작은 숫자를 찾는 것을 의미하며 즉, 어떤 집합의 모든 원소를 넘지 않으면서 그 집합의 상한인 수를 의미한다.</p> <p>이는 ‘최대값(maximum)’과는 다른 개념이다. 최대값은 집합의 원소 중에서 가장 큰 값을 의미하지만, supremum은 집합의 원소가 아니어도 된다.</p> <p>예를 들어, 0과 1 사이의 모든 실수를 포함하는 집합의 최대값은 정의되지 않지만, supremum은 1이다.</p> </blockquote> <p>그러므로, paramerize된 함수의 가족 $\lbrace f_w \rbrace_{w \in \mathcal{W}} $이 $K$-Lipschitz 인 경우 다음과 같은 문제를 해결할 수 있다.</p> <p>즉, parameterize된 함수 $f_w$로 바꾸고, $\mathbb{P}_\theta$를 $g_\theta$에 관한 식으로 바꾸면 다음과 같은 식을 구할 수 있다.</p> \[\underset{w\in \mathcal{W}}{\text{max}}\; \mathbb{E}_{x \sim \mathbb{P}_r} [f_w(x)] - \mathbb{E}_{z\sim p(z)}[f_w(g_\theta(z))] \tag{3}\] <p><strong>Eq. 3</strong>은 GAN의 loss식과 유사해 보이며, 이 식에서 존재하는 $\mathbb{P}_r$에 대한 의문점이 생길 수 있다. 실제로는 학습된 $D$가 그 역할을 해주며, gradient를 update할 때 $\theta$에 대해 미분하기 때문에 앞 항은 사라진다. (wGAN에서는 $D$를 <strong>critic</strong>으로 대체한다. 추후 설명)</p> <p>논문에 있는 <strong>Theorem 3</strong>은 위 식 <strong>Eq. 3</strong>을 미분한다.</p> \[\bigtriangledown _\theta W(\mathbb{P}_r, \mathbb{P}_\theta) = -\mathbb{E}_{z\sim p(z)}[\bigtriangledown _\theta f(g_\theta(z))]\] <p>우리는 <strong>Eq. 2</strong>의 maximize 문제를 해결하기 위한 함수 $f$를 찾아야한다. 이를 위해 가중치 $w$로 parameterize한 신경망을 사용하며 GAN에서의 <strong>Discriminator</strong> 역할을 하게 된다. WassersteinGAN에서는 이를 <strong>critic</strong>이라 명명한다.</p> <blockquote> <p>Eq. 3은 critic의 loss 식이자, EM distance를 의미하며 $w$는 그 critic($f$)의 파라미터이다.</p> </blockquote> <p>$f_w$가 compact space $\mathcal{W}$으로 제한하기 위해서 gradient update 후에 $w$를 $[-0.01, 0.01]$ 사이로 조정하며, 이를 <strong>Weight clipping</strong>이라고 한다.</p> <blockquote> <p><strong>Weight clipping</strong>은 Lipschitz 조건을 적용하기 위해 넣었지만, 그렇게 좋지 못한 방법이라고 논문에서 말한다. 이 논문에서는 단순하고 이미 성능이 좋다고 알려진 정도의 clipping을 적용하였으며, Neural Network에서 Lipschitz 조건을 적용하는 문제는 추후 연구를 위해 남겨 놓았다고 말한다.</p> </blockquote> <p>WassersteinGAN의 <strong>Generator</strong> loss fuction은 <strong>Theorem 3</strong>에 나와있듯이, Eq. 3을 $\theta$에 대해 미분하여 앞의 항을 사라지게 하여 얻을 수 있다.</p> <p><img src="/assets/img/post/wGAN/wGANloss.webp" alt="wGANloss" style="zoom:80%;"></p> <p>위 그림은 wGAN의 아키텍처를 표현한 그림(<a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490" rel="external nofollow noopener" target="_blank">출처</a>)이며 <strong>Algorithm 1</strong>에 설명되어있다.</p> <p><img src="/assets/img/post/wGAN/Algorithm1.png" alt="Algorithm1" style="zoom:67%;"></p> <p>위 알고리즘을 천천히 뜯어보자.</p> <ol> <li>$n_{\text{critic}}$ 횟수만큼 critic을 update한다. <ul> <li>$\mathbb{P}_r$과 $p(z)$를 각각 mini batch 만큼 sampling한다.</li> <li>critic loss function을 이용해서 파라미터 $w$를 update한다. (Adam 대신 RMSProp)</li> <li>clip($w, -c, c$)부분은 위에서 언급한 Weight clipping을 수행한 것이다.</li> </ul> </li> <li>critic update가 끝나면 $G$를 loss function에 맞게 update한다.</li> </ol> <p>GAN의 $D$는 이미지가 real인지 fake인지 sigmoid 확률값을 사용해 판단하며 gradient 정보는 사용되지 않는다.</p> <p>하지만 wGAN의 <strong>critic</strong>은 EM distance의 식을 그대로 사용하기 때문에, gradient를 제공하는 linear function으로 수렴한다. 즉, output이 scalar값으로 이미지의 진위 여부를 점수처럼 표시한다.</p> <p><img src="/assets/img/post/wGAN/figure2.png" alt="figure2"></p> <p>위 사진을 보면 <span style="color: #EE4444">GAN</span>의 $D$는 Gradient vanishing 문제가 생겼으며, <span style="color: #7CddD8">WGAN</span>은 안정적인 linear한 형태를 띄고 있음을 알 수 있다.</p> <p><span style=" background-color: #F7DDBE"><b>EM distance가 연속적이고 미분 가능하다는 것은 critic을 optimal 할때까지 계속 train 할 수 있다는 의미이며, 기존 GAN이 가지고 있던 mode collapse문제와 $G$와 $D$간의 balance 붕괴 문제가 해결되는 것이다.</b></span></p> <h2 id="empirical-results"><strong>Empirical Results</strong></h2> <p>저자들은 WassersteinGAN 알고리즘을 사용하여 이미지 생성 실험을 진행하였을 때, 2가지 이점을 얘기한다.</p> <ul> <li>$G$의 수렴과 sample의 품질에 영향을 미치는 loss metric</li> <li>Optimization process의 안정화</li> </ul> <h3 id="experimental-procedure"><strong>Experimental Procedure</strong></h3> <p>Train에 사용한 데이터는 LSUN-Bedroom dataset이며 WGAN과 비교 대상은 DCGAN의 아키텍쳐를 따른 GAN이다.</p> <h3 id="meaningful-loss-metric"><strong>Meaningful loss metric</strong></h3> <p>첫 번째 실험은 이 추정치가 생성된 샘플의 품질과 얼마나 잘 상관관계를 가지는지를 보여준다.</p> <p>DCGAN 아키텍처 외에도, $G$ 또는 $G$와 critic 모두를 512개의 hidden unit을 가진 4-layer ReLU-MLP(Multi-Layer Perceptron)로 교체하는 실험도 진행하였다.</p> <p><img src="/assets/img/post/wGAN/figure3.png" alt="figure3"></p> <ul> <li>좌측 상단 : $G$만 4-layer ReLU-MLP로 교체한 결과 <ul> <li>학습이 진행될수록 sample 품질은 향상되고 loss는 감소</li> </ul> </li> <li>우측 상단 : 표준 DCGAN <ul> <li>loss는 빠르게 감소하고 sample 품질도 그에따라 증가</li> </ul> </li> </ul> <p>상단 2개의 plot은 모두 critic에 sigmoid가 없는 DCGAN이므로 비교할수 없다.</p> <ul> <li>아래쪽 : $G$와 $D$모두 4-layer ReLU-MLP로 교체 <ul> <li>학습률이 상당히 높은 MLP라서 학습 <strong>실패</strong> </li> <li>loss와 sample이 모두 일정하다.</li> </ul> </li> </ul> <p><img src="/assets/img/post/wGAN/figure4.png" alt="figure4"></p> <p>위 plot은 같은 모델로 실험을 진행하되, 각 iter 마다 JS Divergence를 계산한 그래프이다. sample 품질이 좋아져도 JS Divergence는 증가하거나 상수 값을 유지하는 것을 알 수 있다.</p> <p>실제로 JS Divergence의 max인 $\log 2 ≈ 0.69$에 매우 근접하는 경우가 많다. 즉, JS Divergence가 포화되고 $D$의 loss가 0가 되며 생성된 sample이 어떤 경우에는 meaningful한(오른쪽 상단 플롯) 반면 어떤 경우에는 무의미한 이미지가 되기도 한다.</p> <p>또한, 기존에는 Optimizer로 <strong>Adam</strong>을 사용했는데, 이와 같은 momentum기반 optimizer나 높은 train 속도를 사용하는 경우 WGAN의 train이 불안정해진다고 한다. <strong>critic</strong>의 loss는 fix되어있지 않기 때문에, momentum 기반 대신, non-fixed 문제에도 잘 작동하는 <strong>RMSPorp</strong>를 사용하였다고 한다.</p> <h4 id="improved-stability"><strong>Improved stability</strong></h4> <p>WassersteinGAN은 최적의 <strong>critic</strong>으로 train할 수 있다는 장점이 있다. 기존의 GAN은 $D$와 $G$의 balance를 맞추며 학습해야했다. 하지만 wGAN은 <strong>critic</strong>이 완전히 train되면, $G$에 loss를 제공하기만 하면 되며 $G$의 gradient 품질은 <strong>critic</strong>의 성능에 비례한다.</p> <p>DCGAN의 $G$를 사용하되, 여러 변화를 주어서 실험하여 $D$와 <strong>critic</strong>을 비교하였다. (즉, WGAN과 GAN을 비교함 셈)</p> <p><span style=" background-color: #F7DDBE"><b>논문에서는 실험할 때 WGAN에서는 mode collapse가 발견된적이 없다고 한다.</b></span></p> <blockquote> <p>즉, WGAN은 GAN의 학습 과정을 안정화시키는 데 큰 기여를 했다. EM distance를 사용하여 optimal한 지점에 도달할 수 있게 되었다. 이러한 변형은 mode collapse 를 해결함으로써 다양한 sample을 생성할 수 있게 하여 기존의 GAN이 가지고 있던 문제들을 해결하였다고 한다.</p> </blockquote> <h2 id="code-review"><strong>Code Review</strong></h2> <p><a href="https://colab.research.google.com/drive/1Uotwtr0CELvtnovj5NP-yGod6VyG9DRR?usp=sharing" rel="external nofollow noopener" target="_blank">wGAN 구현 Colab 이동하기</a></p> </body></html>