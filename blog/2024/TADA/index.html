<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="tada--timestep-awara-data-augmentation-for-diffusion-models-논문-리뷰"><strong>TADA : Timestep-Awara Data Augmentation for Diffusion models 논문 리뷰</strong></h1> <p><a href="https://openreview.net/forum?id=U6Mb3CRuj8" rel="external nofollow noopener" target="_blank">논문 링크</a></p> <p>데이터 증강(Data Augmentation)은 주어진 원본 데이터를 확장하여 데이터셋의 다양성을 증가시키는 기법이다. 이 방법은 특히 학습 데이터가 부족한 경우, 모델의 일반화 능력을 향상시키기 위해 사용한다.</p> <p>이번 논문에서는 Diffusion model의 distribution 변화가 특정 timestep에서 발생한다는 것을 발견하고 timestep에 따라 유연하게 강도를 조정하는 Data augmentation 전략을 제안한다.</p> <p>이러한 전략이 다양한 diffusion model에 도움되기를 바란다고 저자들은 말한다.</p> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Diffusion 모델의 reverse process에서 데이터 증강이 어떤 영향을 미치는지 살펴보았을 때, 성능 저하에 영향을 미치는 timestep을 파악하고, 특정 timestep이 sampling 과정의 변형에 기여하여 의도하지 않은 sample 생성이 이루어진다는 것을 파악하였다.</p> <p>위에서 파악한 것을 바탕으로 저자들은 timestep에 따라 증강 강도를 유연하게 조정하는 Timestep-Aware Data Augmentation (TADA) 전략을 제안한다.</p> <p>위 전략은 $T(x_t, w_t)$로 표시되는데, timestep $t$(noise level이 되기도 함)에 따라 증강 강도 $w_t$를 조절한다. 즉, input 데이터에 큰 noise가 포함된 경우 증강 강도가 강한 train sample($w_t$)로 model을 train한다. vulnerable한 timestep 동안에는 0에 가까운 $w_t$를 적용하다가 다시 약한 noise가 포함된 data가 input이 들어오면 다시 강도를 높인다.</p> <h2 id="method"><strong>Method</strong></h2> <h3 id="preliminaries"><strong>Preliminaries</strong></h3> <p>TADA를 살펴보기 전 필요한 여러 개념들을 살펴보자</p> <h4 id="diffusion-models"><strong>Diffusion models</strong></h4> <p>$n$개의 데이터 point ${x_0^1, \cdots, x_0^n}$가 분포 $q(x_0)$에서 sampling되었다고 가정하자. Diffusion model은 분포 $q(x_0)$에 가장 근접한 모델 $p_θ(x_0)$를 만드는 것이 목표이다.</p> <p>Diffusion model을 training하는 것에는 2가지 process가 있다.</p> <ul> <li>foward process : Gaussian noise $z \sim \mathcal{N}(0,\textit{I})$를 timestep $t$에 따라 추가하여 noise data $x_t$를 만드는 과정</li> <li>reverse process : foward process에서 추가된 noise를 제거(denoising)하여 원래의 데이터를 복구하는 과정</li> </ul> <p>Foward process에서는 $x_0$를 이용해 $x_t = \alpha_t x_0 + \sigma^2 z$를 계산한다. 모델 $\widehat{\epsilon}_\theta (x_t, t)$는 weighted MSE를 최소화하여 timestep $t$에서 추가된 noise $z$를 예측하도록 train된다.</p> <p>Reverse process에서는 $p_\theta(x_{t-1}|x_t)$를 통해 $x_t$로 $x_{t-1}$을 계산한다. 이 과정은 $x_T \sim \mathcal{N}(0, \textit{I})$, 즉 완전한 Gaussian noise인 상태에서 시작한다. $\widehat{x}_\theta (x_t, t) = \frac{x_t - \sigma_t^2 \widehat{\epsilon}<em>\theta (x_t, t)}{\alpha_t}$일 때, reverse process $x</em>{t-1}$는 다음과 같다.</p> \[\widehat{x}_{t-1} = \alpha_{t-1} \widehat{x}_\theta(x_t, t) + \sigma_{t-1}^2z \tag{1}\] <p>$\alpha_t$와 $\sigma_t^2$, Objective, sampline method 등에는 다양한 방법이 있지만, 이 논문에서는 DDPM의 향상된 버전인 Improved DDPM에서 제안한 방법을 채택하였다.</p> <h4 id="signal-to-nosie-ratiosnr"><strong>Signal-to-nosie ratio(SNR)</strong></h4> <p><a href="https://arxiv.org/abs/2107.00630" rel="external nofollow noopener" target="_blank">Variational Diffusion Models</a>에서는 signal-to-noise ratio(SNR)이라는 개념을 도입했으며, 이는 각 timestep $t$에서 noise level을 측정하는 개념이다. SNR은 $t$에 따라 점점 감소하는 함수이다. ($\text{SNR}(t) = \frac{\alpha_t^2}{\sigma_t^2}$)</p> <h4 id="data-augmentation"><strong>Data augmentation</strong></h4> <p>이 논문에서는 데이터 증강을 $T(x_t, w)$로 표시하며, $w \in [0,1]$은 증강의 강도를 제어하는 정규화된 hyper parameter이다.</p> <h3 id="analyzing-the-effect-of-data-augmentation-on-learned-reverse-process"><strong>Analyzing the effect of data augmentation on learned reverse process</strong></h3> <p>데이터 증강이 Diffusion model에 미치는 영향을 파악하고 취약한 $t$를 파악하기 위해 증강을 통해 학습된 model과 일반적인 model의 reverse process를 비교한다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure%201.png" alt="Figure 1"></p> <p>위 이미지에서 <strong>(a)</strong>는 horizontal-flip 으로만 train된 baseline 모델 ($\widehat{ε}_{\text{base}}$)과 증강 데이터로 train된 모델인 ($\widehat{ε}_{\text{aug}}$)의 두 Diffusion model의 reverse process를 비교한 실험의 결과이다. <strong>(a)</strong>그래프를 보면 rough(초기)와 fine(마지막) 시점에서는 두 모델의 차이(<span style="color: #F7DDBE">노란선</span>)가 거의 없으며, sensitive한 timestep에서는 차이가 벌어짐을 알 수 있다.</p> <p><strong>(b)</strong>는 <strong>(a)</strong>의 sensitive timestep에서 $\widehat{ε}_{\text{aug}}$와 $\widehat{ε}_{\text{base}}$ 사이의 reverse process를 $\widehat{ε}_{\text{base}} \rightarrow \widehat{ε}_{\text{aug}}$ 와 $\widehat{ε}_{\text{aug}} \rightarrow \widehat{ε}_{\text{base}}$로 바꿔서 두 가지 sampling process를 생성한 결과이다.</p> <ul> <li>처음 두 행은 sample이 처음에는 데이터 분포를 따르지만 결국 증강된 데이터 분포에 포함되는 $\widehat{ε}_{\text{base}} \rightarrow \widehat{ε}_{\text{aug}}$ 의 경우를 보여준다. 즉, sample이 $\widehat{ε}_{\text{base}}$에 의한 데이터 분포를 따르더라도 $\widehat{ε}_{\text{aug}}$에 의해 변경되어 증강된 것과 같은 결과물로 다시 나타난다.</li> <li>마지막 두 행은 처음에 $\widehat{ε}_{\text{aug}}$의 궤적을 따르던 샘플이 sensitive timestep 동안 $\widehat{ε}_{\text{base}}$에 의해 조정되어 궁극적으로 데이터 분포와 일치하게 된 경우를 보여준다.</li> </ul> <blockquote> <p>Diffusion model의 sensitive timestep에서 sampling trajectory을 변경하여 최종 sample이 원래 데이터 분포를 따르는지 아니면 증강된 분포를 따르는지를 결정할 수 있음을 알 수 있다.</p> </blockquote> <h3 id="timestep-aware-data-augmentation-for-diffusion-models"><strong>Timestep-Aware Data Augmentation for Diffusion models</strong></h3> <p>위 실험을 바탕으로 논문에서는 rough와 fine timestep에서는 강력한 증강(큰 $w$)을 적용하고, sensitive timestep에서는 강도를 낮추는 전략을 제안한다.</p> <ul> <li> <strong>sensitive timestep($t \in [t_{rough}, t_{fine}]$)</strong>: 약한 강도의 증강이 train 데이터에 적용되어 생성된 sample $p(x)$에 가깝게 유지</li> <li> <strong>rough($t \in (t_{rough},T]$) 및 fine($t \in [0,t_{fine})$) timestep</strong>: 강력한 증강을 통해 과적합을 방지하고 확산 모델의 일반화 기능을 개선</li> </ul> <p>증강 강도 $w_t$는 다음과 같이 구할 수 있다.</p> \[w_t = k(r_t - r_{rough})(r_t - r_{fine}) + \delta \tag{2}\] <p>여기서 $r_t = \log (\textbf{SNR}(t))$이며, $\delta$는 sensitive timestep에서 최대 증강 강도를 나타낸다. $w_t$가 너무 낮으면 안되기 때문에 $\delta=0.1$로 설정하였다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure%202.png" alt="Figure 2"></p> <blockquote> <p>위 그림은 <strong>Eq. 2</strong>에 대한 그래프이다.</p> </blockquote> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure5.png" alt="Figure5" style="zoom:50%;"></p> <p>$k$는 clipping이 발생하기 전에 $w_t$가 0이 되도록 한다. 이는 자동으로 계산되는데, 위 그래프는 $k$를 조절하였을 때 $ r_{rough}$와 $ r_{fine}$에서 증강 강도가 어떻게 변화하는지 나타내는 그래프이다.</p> <p>Diffusion 모델의 noise level이 해상도에 따라 달라지므로 SNR을 조정할 필요를 느끼고 다른 논문에서 제안한 식을 적용하였다. 이미지의 해상도를 $d$, timestep을 $t$라 할 때</p> \[\text{SNR}_{calibrated}(t) = \frac{\text{SNR}(t)}{(d/64)^2} \tag{3}\] <p>이 식을 <strong>Eq. 2</strong>의 $r_t$에 적용시키면 해상도에 따른 조정이 가능해진다고 한다.</p> <h2 id="experiment"><strong>Experiment</strong></h2> <p>TADA를 이전 연구에서 사용된 두 가지 증강 방법과 비교하는 실험을 한다.</p> <ol> <li> <strong>50% 수평 뒤집기(h-flip)</strong>: 확산 모델에서 일반적으로 사용</li> <li> <strong>증강 정규화 방법(AR)</strong>:&lt;Elucidating the Design Space of Diffusion-Based Generative Models&gt;(2023)에서 사용한 기법</li> </ol> <h3 id="benefit-of-tada"><strong>Benefit of TADA</strong></h3> <p>TADA는 sensitive timestep 동안 증강 강도를 조정하기 때문에 <strong>distribution-shifted sample을 생성하지 않는다</strong>는 것을 보여주며, <strong>overfitting 문제를 완화</strong>하여 작은 데이터 세트에서 Diffusion 모델의 성능을 향상시킨다는 것을 보여준다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/table1.png" alt="table1"></p> <p>위 <strong>표 1</strong>를 보면, iter가 100,000회인 FID 및 KID 결과를 나타낸다. h-filp 기법보다 성능이 좋은 것을 알 수 있으며, TADA라는 비교적 간단한 기법으로 AR과 비슷한 결과를 낸다는 것을 알 수 있다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure3.png" alt="Figure3" style="zoom:50%;"></p> <p>위 사진은 TADA가 분포 내에서 sample을 생성하는지 시각화한 것으로, 모든 timestep에서 증강을 적용한 Naive augmentation에 비해 TADA가 훨씬 선명한 얼굴을 생성한다는 것을 알 수 있다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure4.png" alt="Figure4" style="zoom:50%;"></p> <p>위 그래프는 overfitting에 미치는 영향을 보여주는 그래프로, FID값을 나타내는 그래프이다.</p> <p>TADA(보라색)가 h-flip(파랑색)보다 overfitting을 완화하는 데 더 효과적임 알 수 있다.</p> <h3 id="generalization"><strong>Generalization</strong></h3> <p>TADA가 Diffusion 모델 학습에서 다양한 설계에 적용할 수 있음을 알아보기 위해 높은 해상도, Transfer learning, 다양한 Noise scheduling, 다양한 모델 크기로 실험을 진행하였다.</p> <h4 id="high-resolution"><strong>High resolution</strong></h4> <p><img src="/assets/img/post/TADA%20for%20Diffusion/table2.png" alt="table2" style="zoom:50%;"></p> <p><strong>표 2</strong>역시 <strong>표 1</strong>과 동일</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/Figure6.png" alt="Figure6"></p> <p>높은 해상도인 256*256 데이터에 TADA를 적용한 결과이다.</p> <h4 id="transfer-learning"><strong>Transfer Learning</strong></h4> <p><img src="/assets/img/post/TADA%20for%20Diffusion/table3.png" alt="table3" style="zoom:50%;"></p> <p>위 <strong>표 3</strong>은 AHFQ-v2의 세 가지 class에서 TADA가 AR을 능가하고 있음을 보여준다.</p> <h4 id="noise-scheduling-sampling-step-model-size"><strong>Noise scheduling, Sampling step, Model size</strong></h4> <p><img src="/assets/img/post/TADA%20for%20Diffusion/table4.png" alt="table4" style="zoom:50%;"></p> <p>위 <strong>표 4</strong>는 3가지 관점에서 TADA와 h-flip을 평가한 결과이다.</p> <ul> <li>TADA는 <strong>linear</strong>과 <strong>cosine</strong> <strong>noise schedule</strong> 모두에서 h-flip보다 우수한 성능을 보여준다.</li> <li> <strong>{50, 100, 500, 1000} sampling</strong> 단계에서의 TADA와 h-flip의 성능 추이를 보면, TADA는 h-flip보다 지속적으로 우수한 성능을 보인다.</li> <li>TADA는 <strong>대규모 모델</strong>에서는 뚜렷한 이점을 보였지만 <strong>소규모 모델</strong>에서는 효과가 떨어진다. <ul> <li>이러한 결과는 데이터 증강이 대규모 모델의 성능을 향상시키는 데 중요한 역할을 하기 때문에 예상되는 결과라고 논문에서 말한다.</li> </ul> </li> </ul> <h3 id="ablation-study"><strong>Ablation Study</strong></h3> <p>이 section은 TADA에서 각 구성 요소의 영향을 분석하기 위해 세 가지 ablation studies를 수행한다.</p> <p><img src="/assets/img/post/TADA%20for%20Diffusion/table5.png" alt="table5"></p> <h4 id="augmentation-range"><strong>Augmentation Range</strong></h4> <p><strong>표 5a</strong>에서는 특정 timestep에서만 $w_t$를 적용하고 다른 범위에서는 $w_t = 0$으로 설정하여, $rough$, $sensitive$, $fine$에 대한 FID score를 측정한다.</p> <p>baseline(h-flip)을 ‘none’으로 고려할 때, 각 timestep의 기여도는 sampling 단계에 따라 다르다.</p> <ul> <li>50 sampling step에서는 sensitive timestep이 가장 영향력이 크다.</li> <li>250 sampling step에서는 fine timestep이 가장 영향력이 크다.</li> </ul> <p>sampling step에 관계없이 TADA가 다양한 범위에서 baseline model에 비해 명확한 개선을 보이는 것을 알 수 있다.</p> <h4 id="m-variation"><strong>$M$ Variation</strong></h4> <p><strong>표 5b</strong>에서는 다양한 $M$ 값에 대해 TADA를 테스트하고, 250 sampling step에서의 FID score를 측정한다. $M = 0$ (h-flip)의 baseline model과 비교할 때,</p> <ul> <li>$M$이 적당히 작을 때(ex: $M=2$) 증강을 적용하는 것이 유익하다는 것을 알 수 있다.</li> <li>$M$이 너무 크면 train data 분포가 원본 train data로부터 너무 멀어져 학습을 방해할 수 있다.</li> </ul> <p>이를 바탕으로, 논문에서는 모든 setting 에 대해 $M = 2$로 설정한다.</p> <h4 id="κ-variation"><strong>$κ$ Variation</strong></h4> <p>TADA에서 $κ$의 영향을 평가하기 위해, <strong>linear</strong>과 <strong>cosine</strong> <strong>noise schedule</strong>에 대해 $κ$의 값을 조절하여 실험을 진행하였다.</p> <p><strong>표 5c</strong>는 $κ$를 변화시키며 얻은 FID score를 보여주는데, 직접 설정해준 $κ$가 default보다 성능이 우수한 것을 알 수 있다.</p> <blockquote> <p>default는 $κ$가 자동으로 계산되는 것임</p> </blockquote> <p>default가 합리적이지만, 더 나은 hyper-parameter tuning으로 TADA의 성능을 향상시킬 수 있음을 시사한다.</p> <h2 id="discussion--conclusion"><strong>Discussion &amp; Conclusion</strong></h2> <p>Generative model을 위한 다른 데이터 증강 방법과 마찬가지로, TADA는 데이터 부족 문제를 완전히 해결할 수 없으며, 더 많은 데이터를 수집하는 것보다 덜 효과적이다. 또한, 논문에서 비교 실험을 위해 채택한 증강기법(h-flip, AR)은 GAN에서 일반적으로 사용되는 기법이므로 Diffusion model에 최적이 아닐 수 있다.</p> <p>하지만, Diffusion model에서 데이터 증강과 분포 변화 사이의 관계를 처음으로 심층적으로 조사하였으며, 특정 timestep에서는 유의미한 영향을 미친다는 것을 알아내었다.</p> <p>이를 바탕으로 시간 간격에 따른 데이터 증강 강도를 조절하는 방법인 TADA를 제안하였으며 간단하게 구현하더라도 Dataset, model 구성, Sampling step에 걸쳐 Diffusion model을 일관되게 향상시킬 수 있음을 보여주었다.</p> <p>TADA는 데이터가 제한된 환경에서 Diffusion model의 성능을 효과적으로 개선하여 overfitting 문제를 해결하고 Transfer learning에서도 성능을 개선하였다.</p> </body></html>